{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:using textgen params:\n",
      "\n",
      ":{'min_length': 5, 'max_length': 384, 'no_repeat_ngram_size': 7, 'encoder_no_repeat_ngram_size': 7, 'repetition_penalty': 3.7, 'num_beams': 12, 'length_penalty': 0.5, 'early_stopping': True, 'do_sample': False}\n",
      "INFO:root:removed -221 chars via cleaning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677b82df845544bb8bb8611febf3f315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import fitz\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from cleantext import clean\n",
    "from datasets import load_dataset\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "\n",
    "# Assuming you have a TextAnalysis module\n",
    "import TextAnalysis as TA\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def remove_text_in_brackets(text):\n",
    "  \"\"\"Removes text enclosed in parentheses or square brackets.\"\"\"\n",
    "  pattern = r'\\([^)]*\\)|\\[[^\\]]*\\]'\n",
    "  return re.sub(pattern, '', text)\n",
    "\n",
    "def remove_lines_starting_with_number_or_symbol(text):\n",
    "  \"\"\"Removes lines starting with a number or symbol.\"\"\"\n",
    "  pattern = r'^[\\d\\W].*$'\n",
    "  return re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "\n",
    "def remove_lines_with_one_word(text):\n",
    "  \"\"\"Removes lines containing only one word.\"\"\"\n",
    "  lines = text.split('\\n')\n",
    "  pattern = r'^\\s*\\w+\\s*$'\n",
    "  filtered_lines = [line for line in lines if not re.match(pattern, line)]\n",
    "  return '\\n'.join(filtered_lines)\n",
    "\n",
    "def remove_empty_lines(text):\n",
    "  \"\"\"Removes empty lines.\"\"\"\n",
    "  lines = text.split('\\n')\n",
    "  non_empty_lines = [line for line in lines if line.strip() != '']\n",
    "  return '\\n'.join(non_empty_lines)\n",
    "\n",
    "def find_and_remove_references(text, keywords):\n",
    "  \"\"\"Finds and removes text after the first occurrence of any of the given keywords.\"\"\"\n",
    "  earliest_position = -1\n",
    "  for keyword in keywords:\n",
    "    position = text.find(keyword)\n",
    "    if position != -1:\n",
    "      earliest_position = position if earliest_position == -1 else min(position, earliest_position)\n",
    "  if earliest_position != -1:\n",
    "    text = text[:earliest_position]\n",
    "  return text\n",
    "\n",
    "def process_text(text):\n",
    "  \"\"\"Applies a series of cleaning steps to the text.\"\"\"\n",
    "  keywords = [\"REFERENCES\", \"ACKNOWLEDGEMENTS\", \"References\", \"Acknowledgements\"]\n",
    "\n",
    "  text = find_and_remove_references(text, keywords)\n",
    "  text = find_and_remove_references(text, keywords)\n",
    "  text = remove_text_in_brackets(text)\n",
    "  text = remove_lines_starting_with_number_or_symbol(text)\n",
    "  text = remove_lines_with_one_word(text)\n",
    "  text = remove_empty_lines(text)\n",
    "\n",
    "  return text\n",
    "\n",
    "def extract_and_process_text(pdf_path, output_path):\n",
    "  \"\"\"Extracts text from a PDF file, cleans it, and saves it to a text file.\"\"\"\n",
    "  pdf_doc = fitz.open(pdf_path)\n",
    "  text = ''\n",
    "  for page in pdf_doc:\n",
    "    text += page.get_text()\n",
    "\n",
    "  lines = text.split('\\n')\n",
    "  filtered_lines = [line for line in lines if len(line.strip()) > 1]\n",
    "  filtered_text = '\\n'.join(filtered_lines)\n",
    "  cleaned_text = process_text(filtered_text)\n",
    "\n",
    "  with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(cleaned_text)\n",
    "  return output_path\n",
    "\n",
    "def process_pdf(pdf_filename):\n",
    "  \"\"\"Processes a single PDF file, extracts and cleans text, and returns the text filename.\"\"\"\n",
    "  text_filename = pdf_filename.replace(\".pdf\", \".txt\")\n",
    "  cleaned_text = extract_and_process_text(pdf_filename, text_filename)\n",
    "  return cleaned_text\n",
    "\n",
    "# --- PDF Processing ---\n",
    "\n",
    "# Specify the directory containing your PDF files\n",
    "content_directory = \"C:\\\\Users\\\\admin\\\\Desktop\\\\Intern\\\\ReviewerExtractorFork\\\\ReviewerExtractor\\\\LLM\\\\content\"\n",
    "\n",
    "# Find all PDF files in the directory\n",
    "pdf_filenames = [os.path.join(content_directory, file) for file in os.listdir(content_directory) if file.endswith('.pdf')]\n",
    "\n",
    "# Process the PDF files in parallel\n",
    "text_filenames = []\n",
    "num_workers = 100\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "  results = list(executor.map(process_pdf, pdf_filenames))\n",
    "\n",
    "text_filenames.extend(result for result in results if result is not None)\n",
    "\n",
    "# --- Summarization Model ---\n",
    "\n",
    "hf_tag = \"pszemraj/led-large-book-summary\"\n",
    "_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./model\").to(_device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./model\")\n",
    "token_batch_length = 3072\n",
    "batch_stride = 50\n",
    "max_len_ratio = 8\n",
    "\n",
    "settings = {\n",
    "    'min_length': 5,\n",
    "    'max_length': int(token_batch_length // max_len_ratio),\n",
    "    'no_repeat_ngram_size': 7,\n",
    "    'encoder_no_repeat_ngram_size': 7,\n",
    "    'repetition_penalty': 3.7,\n",
    "    'num_beams': 12,\n",
    "    'length_penalty': 0.5,\n",
    "    'early_stopping': True,\n",
    "    'do_sample': False\n",
    "}\n",
    "\n",
    "logging.info(f\"using textgen params:\\n\\n:{settings}\")\n",
    "\n",
    "def summarize(ids, mask, **kwargs):\n",
    "    ids = ids[None, :]\n",
    "    mask = mask[None, :]\n",
    "\n",
    "    input_ids = ids.to(_device)\n",
    "    attention_mask = mask.to(_device)\n",
    "    global_attention_mask = torch.zeros_like(attention_mask)\n",
    "    global_attention_mask[:, 0] = 1\n",
    "\n",
    "    summary_pred_ids = model.generate(\n",
    "        ids,\n",
    "        attention_mask=mask,\n",
    "        global_attention_mask=global_attention_mask,\n",
    "        return_dict_in_generate=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    summary = tokenizer.batch_decode(\n",
    "        summary_pred_ids.sequences,\n",
    "        skip_special_tokens=True,\n",
    "        remove_invalid_values=True,\n",
    "    )\n",
    "\n",
    "    return summary\n",
    "\n",
    "def summarize_via_tokenbatches(input_text, **kwargs):\n",
    "    encoded_input = tokenizer(\n",
    "        input_text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=token_batch_length,\n",
    "        stride=batch_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    in_id_arr, att_arr = encoded_input.input_ids.to(_device), encoded_input.attention_mask.to(_device)\n",
    "    gen_summaries = []\n",
    "\n",
    "    with tqdm(total=len(in_id_arr)) as pbar:\n",
    "        for _id, _mask in zip(in_id_arr, att_arr):\n",
    "            result = summarize(ids=_id, mask=_mask, **kwargs)\n",
    "            _sum = {\"input_tokens\": _id, \"summary\": result}\n",
    "            gen_summaries.append(_sum)\n",
    "            pbar.update()\n",
    "\n",
    "    return gen_summaries\n",
    "\n",
    "# --- Summarization ---\n",
    "\n",
    "directory = './content'\n",
    "text_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.txt')]\n",
    "\n",
    "# Summarize each text file and store in a dictionary\n",
    "summaries = {} \n",
    "for file_path in text_files:\n",
    "  with open(file_path, 'r', errors='ignore') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "  long_text = clean(raw_text)\n",
    "  logging.info(f\"removed {len(long_text) - len(raw_text)} chars via cleaning\")\n",
    "\n",
    "  _summaries = summarize_via_tokenbatches(long_text, **settings)\n",
    "  summaries[file_path] = _summaries  # Store summaries by filename\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# --- Combine Summaries for Given Author ---\n",
    "\n",
    "# Assuming you have a DataFrame named 'datf'\n",
    "def combine_summaries(datf):\n",
    "  for index, row in datf.iterrows():\n",
    "    identifiers = row['arxiv-id'].split(',')\n",
    "    concat_summ = \"\"\n",
    "\n",
    "    for identifier in identifiers:\n",
    "      if identifier.strip() == 'None':\n",
    "        concat_summ = 'None'\n",
    "        break\n",
    "\n",
    "      sanitized_identifier = identifier.strip().replace('/', '_')\n",
    "      filename = f'SUM_{sanitized_identifier}.txt'  # Assuming summaries are saved as SUM_*.txt\n",
    "\n",
    "      if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "          concat_summ += file.read()\n",
    "      else:\n",
    "        concat_summ = 'File not found'\n",
    "\n",
    "    datf.at[index, 'Summaries'] = concat_summ\n",
    "\n",
    "# --- N-grams ---\n",
    "\n",
    "def sum_n_grams(df, directorypath):\n",
    "    top10Dict = {'Summaries Top 10 Words': [],\n",
    "                 'Summaries Top 10 Bigrams': [],\n",
    "                 'Summaries Top 10 Trigrams': []}\n",
    "\n",
    "    for i in df.values:\n",
    "        summaries = i[15]  # Assuming 'Summaries' is the column name for your combined summaries\n",
    "\n",
    "        # Now use functions from TextAnalysis\n",
    "        top10words = TA.topwords(summaries, directorypath)\n",
    "        top10bigrams = TA.topbigrams(summaries, directorypath)\n",
    "        top10trigrams = TA.toptrigrams(summaries, directorypath)\n",
    "\n",
    "        top10Dict['Summaries Top 10 Words'].append(top10words)\n",
    "        top10Dict['Summaries Top 10 Bigrams'].append(top10bigrams)\n",
    "        top10Dict['Summaries Top 10 Trigrams'].append(top10trigrams)\n",
    "\n",
    "    sum_top10Df = df\n",
    "    sum_top10Df['Summaries Top 10 Words'] = top10Dict['Summaries Top 10 Words']\n",
    "    sum_top10Df['Summaries Top 10 Bigrams'] = top10Dict['Summaries Top 10 Bigrams']\n",
    "    sum_top10Df['Summaries Top 10 Trigrams'] = top10Dict['Summaries Top 10 Trigrams']\n",
    "\n",
    "    sum_top10Df = sum_top10Df[['Input Author', 'Input Institution', 'First Author', 'Bibcode', 'Title', 'Publication Date',\n",
    "             'Keywords', 'Affiliations', 'Abstract', 'Top 10 Words', 'Top 10 Bigrams', 'Top 10 Trigrams', 'Data Type',\n",
    "            'arxiv-id', 'Summaries', 'Summaries Top 10 Words', 'Summaries Top 10 Bigrams', 'Summaries Top 10 Trigrams']]\n",
    "\n",
    "    return sum_top10Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'./content\\\\2410.11851v1.txt': [{'input_tokens': tensor([11900,  7037,    13,  ..., 20992,     9, 15935], device='cuda:0'),\n",
       "   'summary': ['In this paper, the authors commemorate the half-century anniversary of Stephen hawking\\'s discovery of the \"Hawking temperature, an equation that predicts that black holes have an absolute temperature proportional to their mass, by calculating the \"hooting temperature. This discovery is significant because it revolutionized the field of theoretical physics and black hole physics for the first time. The authors explain how they will derive the hawking temperature using a combination of general relativity, quantum mechanics, and thermodynamics. They will also use dimensional analysis to derive the value of the hawking temperature from the equations used to calculate it. The authors do not yet know the meaning of the hawking temperature, but they plan to explore its possible consequences in the following sections']},\n",
       "  {'input_tokens': tensor([   4,    7, 6925,  ...,    1,    1,    1], device='cuda:0'),\n",
       "   'summary': ['The black hole emits mainly photons, but also light, and this \"hawking\" radiation can also be broken down into neutrinos, electrons, and positrons. It does not evaporate however. Instead, it undergoes evaporation, a process that gradually reduces its mass. This is similar to evaporation of a pond with water on a summer day; in this analogy, the acceleration of the water is gradually reducing the mass of the puddle. Similarly, the acceleration of a black hole is gradually reduced as it absorbs more material. The amount of material absorbed determines the mass of the black hole. The time taken for an isolated black hole of initial mass mbh to reach zero is determined by the evaporation time, which is expressed in seconds. The number of times a black hole loses its initial mass is subtracted from the absolute value of its initiality, i.e., its initial temperature. Brief introduction to dimensionless dimensionless dimensionless units of dimension. dimensionless units of dimension are given in SI units per unit of ten thousandths of a kilogram or metre, kilogram. In other words, a dimension is a dimension that can be measured in standard fundamental units. The seven principles of dimensionless dimensionless dimensionality are listed in table a1 below. The rules of dimensional analysis, i.e., the rules of relation between dimensionless quantities, are defined as follows: i.e., if a quantity is too small to be directly related to a dimension, then it would not be significant; if it were large enough; if it were big enough; if it were too large, then it would become significant; if it were too great, then it would have negligible importance. The author notes that many of the great discoveries of science and engineering have been made using dimensionless dimensionless solutions to problems involving large numbers']}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # --- Main Execution ---\n",
    "\n",
    "# # Load your DataFrame\n",
    "# datf = pd.read_csv('output.csv')  # Replace 'output.csv' with your actual file\n",
    "\n",
    "# # Combine summaries\n",
    "# combine_summaries(datf)\n",
    "\n",
    "# # Calculate N-grams and update DataFrame\n",
    "# path_stop = '/content/'\n",
    "# stop_file = 'stopwords.txt'\n",
    "# stop_dir = path_stop + stop_file\n",
    "# final_df = sum_n_grams(datf, stop_dir)\n",
    "\n",
    "# # Save the updated DataFrame\n",
    "# final_df.to_csv('Results.csv', index=False)\n",
    "summaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
