{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Meta Llama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Precision and Load the Tokenizer and Model from Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/nobackup/ielhaime/models/\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/nobackup/ielhaime/models/\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Text Gen Pipeline and Set Size of Reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = pipeline(\n",
    "    \"text_generation\",\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    max_new_tokens=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(document):\n",
    "    response = text_generator(document)\n",
    "    gen_text = response[0]['generated_text']\n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2410.11851v1.txt...\n",
      "Processing 2410.17135v1.txt...\n",
      "Processing 2410.17142v1.txt...\n",
      "Processing 2410.17163v1.txt...\n",
      "Processing 2410.17169v1.txt...\n",
      "Processing 2410.17176v1.txt...\n",
      "Processing 2410.17178v1.txt...\n",
      "Processing 2410.17187v1.txt...\n",
      "Processing 2410.17189v1.txt...\n",
      "Processing 2410.17232v1.txt...\n",
      "Processing 2410.17252v1.txt...\n",
      "Summaries saved to summaries.csv\n",
      "\n",
      "Summary of processed files:\n",
      "Total files processed: 11\n",
      "\n",
      "First few entries:\n",
      "          file_name  summary\n",
      "0  2410.11851v1.txt  Summary\n",
      "1  2410.17135v1.txt  Summary\n",
      "2  2410.17142v1.txt  Summary\n",
      "3  2410.17163v1.txt  Summary\n",
      "4  2410.17169v1.txt  Summary\n",
      "Successfully saved DataFrame to summaries.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def get_summary(document):\n",
    "    \"\"\"Generate a summary for a given document text.\"\"\"\n",
    "    response = text_generator(document)\n",
    "    gen_text = response[0]['generated_text']\n",
    "    return gen_text\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    \"\"\"Read and return the contents of a text file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        # Try a different encoding if UTF-8 fails\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_directory(directory_path='content', output_file='summaries.csv'):\n",
    "    \"\"\"\n",
    "    Process all text files in the specified directory and save summaries to CSV using pandas.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing text files\n",
    "        output_file (str): Name of the output CSV file\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the summaries, or None if no files were processed\n",
    "    \"\"\"\n",
    "    # Create content directory if it doesn't exist\n",
    "    Path(directory_path).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get all .txt files in the directory\n",
    "    txt_files = [f for f in os.listdir(directory_path) if f.endswith('.txt')]\n",
    "    \n",
    "    if not txt_files:\n",
    "        print(f\"No text files found in {directory_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Process each file and store results\n",
    "    summaries = []\n",
    "    for file_name in txt_files:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        print(f\"Processing {file_name}...\")\n",
    "        \n",
    "        # Read the file content\n",
    "        content = read_text_file(file_path)\n",
    "        if content is None:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Generate summary\n",
    "            summary = get_summary(content)\n",
    "            summaries.append({\n",
    "                'file_name': file_name,\n",
    "                'summary': summary\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame from summaries\n",
    "    if summaries:\n",
    "        try:\n",
    "            # Convert list of dictionaries to DataFrame\n",
    "            df = pd.DataFrame(summaries)\n",
    "            \n",
    "            # Save to CSV\n",
    "            df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "            print(f\"Summaries saved to {output_file}\")\n",
    "            \n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating DataFrame or saving to CSV: {str(e)}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No summaries were generated\")\n",
    "        return None\n",
    "\n",
    "def save_dataframe(df, filename='summaries.csv'):\n",
    "    \"\"\"\n",
    "    Save the DataFrame as a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): DataFrame to save\n",
    "        filename (str): Name of the output CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print(f\"Successfully saved DataFrame to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving DataFrame to CSV: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Process the directory and get the DataFrame\n",
    "    df = process_directory()\n",
    "    \n",
    "    if df is not None:\n",
    "        # Display summary information\n",
    "        print(\"\\nSummary of processed files:\")\n",
    "        print(f\"Total files processed: {len(df)}\")\n",
    "        print(\"\\nFirst few entries:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Save DataFrame to CSV\n",
    "        save_dataframe(df, 'summaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
