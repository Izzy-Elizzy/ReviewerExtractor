Coniferest: a complete active anomaly detection
M. V. Kornilov1,2⋆, V. S. Korolev3, K. L. Malanchev4,5, A. D. Lavrukhina1,6,
E. Russeil7, T. A. Semenikhin1,6, E. Gangler8, E. E. O. Ishida8,
M. V. Pruzhinskaya8, A. A. Volnova7, S. Sreejith9
Universitetsky pr. 13, Moscow, 119234, Russia
Basmannaya Ulitsa, Moscow, 105066, Russia
Carnegie Mellon University, Pittsburgh, PA 15213, USA
Green Street, Urbana, IL 61801, USA
bld. 52, Moscow 119234, Russia
Profsoyuznaya Street, Moscow, 117997, Russia
UK.
Abstract. We present coniferest, an open source generic purpose ac-
tive anomaly detection framework written in Python. The package design
and implemented algorithms are described. Currently, static outlier de-
tection analysis is supported via the Isolation forest algorithm. Moreover,
Active Anomaly Discovery  and Pineforest algorithms are avail-
able to tackle active anomaly detection problems. The algorithms and
package performance are evaluated on a series of synthetic datasets. We
also describe a few success cases which resulted from applying the pack-
age to real astronomical data in active anomaly detection tasks within
the SNAD project.
Keywords: Machine learning · Active learning · Anomaly detection.
The ultimate goal of every machine learning  algorithm is to extract in-
formation present from large data sets, and in the process, optimise the allo-
cation of human resources devote to it. Ultimately, improving human life. In
arXiv:2410.17142v1    22 Oct 2024
M. V. Kornilov et al.
the context of anomaly detection  strategies this translates into enabling
discoveries which would never happen otherwise. Depending on the considered
field, it means that we can discover hidden or upcoming hardware failures, rare
software bugs, malicious or fraud activity in software systems, or even new and
completely unforeseen astrophysical objects. All these cases have implications
either on our understanding of the Universe as a whole, or on crucial aspects of
daily life.
In AD tasks, it is usually assumed that the dataset includes rare objects
hypothesis that all elements in the data set were generate by the same underlying
mechanism. For example, it is often assumed that anomalies are a minority
consisting of a few instances, and that they have attribute values that are very
different from those of nominal instances in . In this context, such objects can
be caught by one of the many outlier detection algorithms available. However, in
the context of scientific research, both assumptions are not fully correct. First,
being rare is not enough attribute to consider an astrophysical source valuable
or scientifically interesting. When outliers are inspected by a human expert,
it often happens that most of them are statistically rare but not useful data
samples – since the final expert request is not to find samples in low density
parts of the feature space but to solve the real-world problems .
Second, sometimes having attribute values slightly above a certain threshold is
enough to be considered a scientifically interesting anomaly. For instance, if we
imagine a white dwarf with a mass greater than the Chandrasekhar limit, it
becomes clear that the mass only needs to be significantly greater within the
measurement error, rather than an order of magnitude greater.
However, identifying such cases is far from a trivial task. Given the volume
and complexity of modern data sets, the expert is usually able to inspect a very
small subset of the objects identified as anomalous by traditional algorithms.
Indeed, inspecting the hardware or checking software behaviour are extremely
time consuming tasks, not to mention the specific case of astronomy, where
additional measurements may be required in order to make a final decision.
Thus, algorithms able to fully, and optimally, exploit past expert decisions when
dealing with new and unlabeled data are required.
Active learning  algorithms constitute a set of learning strategies that
employ expert feedback to fine tune the learning model . In the context of
AD, one possible approach is to sequentially show to the expert the object with
highest anomaly score and using the feedback to update the hyperparameters of
the learning model . The ill-posed nature of the AD problem coupled with the
AL strategy results in personalized models which are trained to identify, within
a large data set, the specific type of anomaly that is interesting to the expert.
While performing research in this field, the SNAD10 team noticed still an-
other practical issue preventing the wide spread use of AL strategies in as-
Coniferest: a complete active anomaly detection framework
tronomy: the lack of AL algorithms in popular scientific software, like scikit-
learn11. Recent efforts in this direction are very sparse and do not completely
address these issues12,13. This motivated us to develop our own software frame-
work, as well as completely new algorithms which were specifically designed
to the application of active AD strategies in astronomy catalog data. This re-
sulted in a fully automatized environment and optimize algorithms which have
fulfilled our expectations throughout the last few years. We believe this tool
is now ready to benefit others, so we decided to provide the community with
coniferest, a ML framework created using our team deep expertise in the field
of active anomaly detection. The package accepts as input any rectangular data
matrix holding one line per object and one column per feature14. Despite our
main interest being on astronomical application, the entire framework is suitable
for a large range of scientific applications.
This paper is organized as follows. In Section 2 we present all currently
implemented algorithms employed in the package. In Section 3 we highlight
package design considerations and overall usage pattern. In Section 4 we provide
package evaluation and conclusions are given in Section 5.
Isolation forest
Isolation forest  is an outlier detection algorithm described in  and later
in . It is not an active anomaly detection method, however it is the base for
other tree-based algorithms implemented within the package. Each member of
the tree ensemble is a particular variant of extremely randomized trees called an
iTree  in the original paper .
Considering a binary tree, each node represents a split point and a feature , resulting into the feature space partitioning into non-overlapping
multidimensional rectangles. An arbitrary value can be assigned to every rect-
angle, for instance a class, for a classification problem, or a real value, for a
regression problem. Note that it is computationally inexpensive to find the cor-
responding rectangle for any input point in the feature space. Its assigned value
is then used as the tree prediction for that particular object. Depending on the
problem, the split value and the feature are evaluated for every node in some
optimal way to create the optimal feature space partitioning, providing good
predictions.
An ensemble of trees — called as forest — consists of a number of weak
predictors  each solving the same problem, being it a classification, a
regression problem, or an outlier isolation problem. Then, the predictions of
but documentation is sparse and it is not adapted to astronomical data.
M. V. Kornilov et al.
every tree are averaged, summed, or taken into account in another manner over
the whole forest, depending on the considered method. Exactly as Leo Tolstoy
writes, Happy families are all alike; every unhappy family is unhappy in its
own way , all trees in the forest provide a slightly wrong prediction in its
own way, but using the ensemble averaging allows us to improve prediction
quality dramatically. Usually, every tree needs to be additionally randomized by
subsampling the dataset or choosing a subset of considered features, otherwise
all trees would be wrong in a similar way and the ensemble averaging would not
help. Decision trees for classification via the random forest method make splits
in some local-optimal manner, trying to separate samples belonging to different
classes as much as possible. In case of AD, isolation trees choose completely
random order for both, feature and the split point. At every node a random
feature and a random split point, from the domain covered by the data, are
chosen. Empty  splits are prohibited, i.e. every single feature
space rectangle encloses at least one sample from the initial dataset. The single-
point rectangle cannot be split further and corresponds to a leaf of the tree. Then
it is fairly obvious that splitting single point into the separate region are more
likely when the point is distant from the bulk of the data in the feature space.
Therefore, the regions of feature space populated by outliers appear earlier, near
the root of the tree. It means that average leaf depths are smaller for the outliers
than those for the nominal data.
For every particular point in the feature space it is possible to evaluate the
leaf depth in every tree of the forest. Then, the leaf depths are averaged and used
as a measure of abnormality. Namely, the following anomality score s is used
within coniferest package, to be consistent with those used in scikit-learn:
s = −2−
c .
dataset size N following:
c ≡2  −1) ≈2
γ −1 + ln + 1
the constant c is essentially an average leaf depth for an isolation tree built for
uniformly distributed data. Moreover, ¯d is an ensemble-averaged leaf depth
for a given data point, x:
i=1
th tree covering point x, d being the leaf l depth, and N corresponding to the
number of samples from the dataset in leaf l. A popular optimization is employed
in our isolation forest implementation: anomaly scores for the nominal data are
Coniferest: a complete active anomaly detection framework
evaluated only approximately, since they are not as important as anomaly scores
for outliers. The tree depth is limited to log2, which allows to speedup the
process of building trees. The term c)) in Equation 3 a consequence of
such optimization.
We notice that s →−1 as ¯d →0, and s →+1 as ¯d →+∞,
which results in outliers having negative scores.
Active Anomaly Discovery
Active Anomaly Discovery  is a generic active learning technique proposed
by . Basically, AAD is not limited by forest algorithms such as Isolation
forest. It is also possible to apply AAD on top of any other outlier detection
algorithm. However, we briefly describe here only the forest flavour of AAD
approach, since it is the one currently implemented in the coniferest package.
To understand some motivation of this technique, let us note that our final
goal is to mimic the decisions provided by a human expert. If the expert could
label the entire dataset, we would have reformulated the problem as a binary
classification one. Then, the Random forest classifier or even Extremely ran-
domized trees could have been applied to solve the problem. The class  would have been assigned to every leaf of the classifier.
The above algorithm seems very similar to our previous description of IF,
except that a leaf depth, instead of a class, is assigned to every leaf of the forest.
Indeed, both IF and Extremely randomized trees, perform a partitioning of the
feature space. The only difference is in the values assigned to the leafs of each
tree. The key idea of AAD is to take IF as a starting point and then iteratively
adapt the leaf values to make the forest similar to a hypothetical Extremely
randomized tree classifier, trained to recognise 2 classes ,
given that the anomaly class is composed by objects that fulfill the expectations
of the expert. Considering that IF is a good enough starting point, we can expect
that the classifier state is potentially reachable using partially labeled data.
There are two possible sources of partially labeled data. First, we can op-
tionally provide the algorithm with the labels known in advance, e.g. given well
established catalogs or literature reports. Second, the human expert can be asked
to input decisions while the active learning loop is running.
For AAD, we have the following equation for the score, which replaces Equa-
tion :
s =
i=1
wi,liφ ) + c))) ,
formation function15 and wi,li are weights which can be adjusted, allowing
Equation  to better predict decisions reported by the human expert. There
φ = 1, or φ = d are also considered .
M. V. Kornilov et al.
are as many different weight parameters as there are leafs in the forest. For sim-
plicity, we also use notation wj further each j corresponds to a leaf in the forest.
Additionally, all weights were normalized, so ∥w∥2 = 1.
Considering A as a known anomaly subset of the data and N as a known
nominal subset, the optimization problem for estimating weights w is defined
as:
w = arg min
i∈A
ReLU  −qτ) +
i∈N
ReLU ) + 1
inals by the model. The second term is a loss due to known nominals wrongly
predicted as anomalies and the third term accounts for regularization. The con-
stant Ca allows us to assign a relative importance of to both kind of errors, our
current default choice being Ca = 1. The threshold score qτ is the limiting score
necessary to identify the 1 −τ percentile of the dataset. Our current default
choice is τ = 0.97. This means that all known anomalies should occupy the top
regularization term is either an uniform vector or the weights from the previous
iteration, while ReLU denotes the rectified linear function:
ReLU ≡
to high number of unknown parameters to be determined. Currently, we use
trust-krylov method from scipy optimization framework, which uses the New-
ton GLTR trust-region algorithm .
At each iteration, Equation  is solved and the scores for the entire dataset
are reevaluated using Equation  and the new weights w. Thus, resulting in
a new unknown object associated to the highest anomaly score and a new τ
score quantile. The most anomalous unknown object is shown to the human
expert. Depending on the expert decision, this object is added to either A or
N, and the next iteration occurs. We emphasize that it is crucial to perform the
model inference as fast as possible, since estimations for the entire dataset are
reevaluated at every iteration.
The total number of iterations, called budget, is an input chosen by the
expert, and should be determined taking into account the available time, as
well as other resources, necessary to properly judge the candidates presented by
the algorithm. Once the budget is exhausted, we can calculate one of the most
important metrics used to evaluate method effectiveness: tje number of expert-
approved anomalies within the budget. In case of simulated data, we can also
use convenient metrics, e.g. confusion matrices. However, this is not possible in a
Coniferest: a complete active anomaly detection framework
real data scenario since the user would not know how many interesting anomalies
are included in the entire data set.
Pineforest16 presents another approach of refining IF by incorporating feedback
from experts. Unlike the previous method, Pineforest does not directly modify
existing trees. Instead, it selectively discards trees that inadequately represent
the underlying data distribution.
In order to determine the suitability of each tree, a scoring mechanism is
devised based on the labeled data. We assign scores to the data points as follows:
yi =
if xi is labeled as an anomaly,
if xi is unlabeled,
if xi is labeled as a regular point.
For any given tree ti, we calculate its score as:
s =
j=1
yj · d),
where d) denotes the depth of the sample xj in tree ti.
Using these scores, we adapt the forest to the labeled data by discarding trees
with lower scores and retaining those with higher ones. This iterative process
involves building an initial set of trees, filtering out a certain percentage of them
based on their scores , and subsequently rebuilding the
forest with the remaining trees. This procedure can be repeated multiple times
for further refinement.
The scoring mechanism is designed to prioritize trees that accurately capture
regular points deep within the forest, while potentially isolating anomalies at
shallower depths. Following the acquisition of new data, the learning process can
be reiterated to incorporate the updated information, thus continually adapting
to evolving data distributions.
coniferest package
The coniferest package is an open source software, and we follow all best prac-
tices currently adopted for developing such tools. The source codes are available
at GitHub: https://github.com/snad-space/coniferest according the terms
of MIT license.
The package is mainly written in Python, since this language is de-facto
standard within contemporary academic machine learning communities. How-
ever, the performance-critical parts, such as tree search algorithms, required the
M. V. Kornilov et al.
use of native binary code. We choose to use Cython to write the performance-
critical code, which is latter compiled to native binary code. Cython is a famous
solution for such cases, for instance scikit-learn is heavily related on Cython.
It is also popular in the machine learning community, which simplifies possible
contributions from the community. Using Cython allows us to support parallel
and fast IF inference, which is required by the AAD algorithm. In alternative to
Cython, performance-critical code could be written in C++ or Rust.
Using native compiled binary code produced by Cython, C++, or Rust com-
plicates the package installation, particularly for non-experienced users. In or-
der to circumvent this issue, we provide pre-built python packages in the Wheel
format, distributed via the PYthon Package index, for all major platforms, in-
cluding Windows , MacOS , and Linux . It means that
pip install coniferest easily does the job for most users. The pre-built pack-
ages are compiled in the GitHub Actions environment automatically, which sim-
plifies package preparation and makes it more reproducible, reliable and secure
for the end user. GitHub Actions are also used to build the code and run unit
tests on different Python versions and operation systems. Employing unit testing
helps finding regressions and dramatically improves overall code quality.
There are only a few dependencies for coniferest. Obviously, numpy is on the
list, and we also currently re-use scikit-learn for building trees. The latter is
a temporarily solution, because currently we have to use internal scikit-learn
interface which is subject to change without notice. Using internal interfaces
of other libraries dramatically complicates development and distribution of the
package, since it requires carefully handling dependencies versions. Addition-
ally, we would like to support parallel  tree building in future
versions of our package, which may come in hand for Pineforest algorithm.
We emphasize that, while IF algorithm is implemented within scikit-learn
package, we had to re-implement it to enhance its performance. While training is
typically the most time-consuming aspect of most machine learning algorithms,
for IF and active learning strategies based upon it, scoring is the more demanding
process. This is because scoring, which is essential for anomaly detection, requires
the use of the entire dataset, whereas training often only uses a small subset of
the original data.
In order to provide useful documentation for the package, we use Python doc-
string, to describe functions and classes, and Markdown, for long read tutorials.
Both are rendered automatically by ReadTheDocs service and available online
at https://coniferest.snad.space.
Even though active anomaly detection usually assumes some active session
and the final product are filtered subsets of the input data, we also provide
support for serializing the trained models to portable ONNX format. This is
useful for re-using the trained models for automatic anomaly detection pipelines.
Under such circumstances, the model cannot be update further, but it can be
already satisfactory pre-trained by the expert to produce a reasonable stream of
data.
Coniferest: a complete active anomaly detection framework
Models and the Session
There are three basic kinds of entities in coniferest packages: datasets, mod-
els, and sessions. The datasets are used solely for testing and demonstrating
purposes. They are not required in production, since each user can analyze their
own data.
The models are scikit-learn-style classes, implementing every supported
algorithm: Isolation forest , Active Anomaly Discovery  and Pinefor-
est. These classes mimic the well-known scikit-learn interface with a few ex-
ceptions, because scikit-learn has poor support for partially-labeled data, and
consequently there was no good interface to follow. However, for the scikit-learn
user it will not be difficult to get acquainted with the coniferest interface.
The sessions are objects to support iterative training of the models, as it
is done in active anomaly detection. The idea behind the session is to create
glue layer between the human expert and the retraining of the model. Unlike
scikit-learn model class, the Session class constructor takes data and meta-
data at the object construction time. The data are ordinary two-dimensional
array of the features. The metadata are one-dimensional array of auxiliary data,
only required to help the human expert in distinguishing the objects inside main
dataset. In the simplest case the sample metadata are their row index, however,
a more description choice is also possible which would help identifying each
sample.
The session constructor also requires the existing model object to be created,
which can either be pre-trained or not. Currently, both AAD and Pineforest
models are supported.
The end user can modify the session behaviour by providing callbacks to
the object constructor. The callbacks are used to input the expert decision, to
visualize the learning process, to snapshot current model and to save results,
among other tasks. For instance, in our workflow, we supply the expert with a
SNAD viewer  URL, pointing to a portal containing extensive information
about the object under consideration. The code then expects a decision  to be input interactively.
An alternative to callbacks could have been to use an abstract class and
inheritance to tune desired session behaviour. However, we found that it could
be difficult for a normal data scientist to grasp the object-oriented concept. So,
we decided to use the functional approach.
Supplied datasets
In order to make the package self-consistent some toy datasets are also supplied
within the code. It is a modern good custom in the machine learning community
to provide the dataset within the code package in order to allow the user to
start exploring the framework immediately. Currently, we provide the following
datasets.
In documentation and tutorial one can find references to an astronomical
dataset ztf_m31. It is a sky field around the M31 galaxy from Zwicky Tran-
M. V. Kornilov et al.
sient Facility17   survey with features extracted as described in
For those who don’t like astronomy, we adopted datasets collected by  for
their work in neural network anomaly detection . The follow-
ing datasets are available: donors , census , fraud , celeba , backdoor , campaign ,
thyroid . For further detailed description, please
see .
There is also a debugging dataset called non_anomalous_outliers, this
dataset is generated by request and consists of nominal data and a required
fraction of outliers.
Isolation forest prediction
CPU wall time, seconds
sklearn 1.4.2
sklearn 1.3.2
sklearn 1.2.2
sklearn 1.1.3
coniferest 0.0.13
sklearn 1.4.2
coniferest 0.0.13
Intel Xeon Gold 6148 
Apple M3 Pro
Fig. 1.
Performance comparison between scikit-learn  and
coniferest. A dataset containing ≈106 samples, each having 2 features is consid-
ered. The dataset anomaly score evaluation has been measured for different versions
of scikit-learn. Additionally, coniferest in single-thread mode is considered. Note,
that scikit-learn is always single-threaded.
As we noted before, it is important to be able to quickly evaluate the model
for AAD and Pineforest. Unfortunately, at the time of the creation of coniferest,
IF prediction within scikit-learn was single-threaded and not quite optimized,
even in single-threaded mode. While scikit-learn performance has been suffi-
ciently improved later, it is still impossible to make multi-threading predictions.
There is two ways to make multi-threading prediction for IF: process each tree
Coniferest: a complete active anomaly detection framework
in a separate thread or process each data sample in a separate thread. We use
the latter approach. Performance comparison is shown in Figure 1.
Success cases
In , coniferest package has been employed within ZWAD pipeline  to per-
form anomaly detection for ZTF DR17 dataset. The human expert was happy
to find anything astrophysically exciting. For instance, a unknown binary mi-
crolensing event AT 2021uey has been found, an optical counterpart for radio
source NVSS J080730+755017, and lots of variable stars and fast transients,
such as red dwarf flares.
The same ZWAD pipeline has been used in . Instead of general anomaly
detection problem, the problem was stated as semi-supervised classification one.
This allowed us to discover 104 supernovae 
in ZTF DR3 dataset. The human expert was serching only for supernovae, and
answered to the active anomaly algorithm accordingly. About 2000 candidates
were inspected, and labeled, leading to the discovery of 100 supernovae. Similarly,
red dwarf flares were of interest in . ZTF DR8 dataset was used to find red
dwarf flare events and about 130 flares were found within 1200 candidates.
Note that instead of inspecting millions of light curves in ZTF dataset only
tiny subsets were scrutinized while looking for supernovae and red dwarf flares.
From astrophysical intuition we understand that it is unlikely to one would find
would mean that the entire Galaxy is consisted of supernovae which is obviously
not true.
We introduced the coniferest package, a multi-algorithm and open-source soft-
ware designed for anomaly detection, developed by the SNAD team. The soft-
ware, written in Python, supports a range of algorithms, including Isolation
Forest  for static anomaly detection, Active Anomaly Discovery  and
Pineforest for active anomaly detection. The framework’s integration of Cython
for performance-critical tasks ensures efficient parallel processing, enhancing the
speed and scalability of anomaly detection tasks. Coniferest’s design aligns with
modern machine learning practices, offering a user-friendly interface similar to
scikit-learn, and supports the serialization of trained models in ONNX format
for seamless integration into automated pipelines.
The successful application of coniferest algorithms for the Zwicky Tran-
sient Facility data highlights its effectiveness in identifying astrophysically signif-
icant events such as binary microlensing, optical counterparts for radio sources,
rare variable stars, and supernovae. This framework not only simplifies the im-
plementation of anomaly detection algorithms, but also bridges the gap between
data scientists and domain experts, fostering more effective and efficient iden-
tification of anomalies in large datasets. As we continue to develop and refine
M. V. Kornilov et al.
coniferest, we anticipate its broader adoption across different fields, facilitat-
ing more accurate and timely detection of anomalies in diverse datasets, which
will be particularly beneficial for the V. Rubin Observatory Legacy Survey of
Space and Time18.
Acknowledgments. M. Kornilov, A. Lavrukhina, A. Volnova and T. Semenikhin
acknowledges support from a Russian Science Foundation grant 24-22-00233, https:
LLC. for K. Malanchev.
Disclosure of Interests. The authors declare no conflicts of interest.