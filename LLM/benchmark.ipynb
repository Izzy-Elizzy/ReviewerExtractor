{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = \"\"\"On its 50th anniversary, this work celebrates Hawking's revolutionary finding that cosmic black holes release heat energy - a phenomenon now bearing his name. Through mathematical size and unit analysis, researchers establish the Hawking heat level formula, which has become a cornerstone in modern space science. Their analysis reveals how space holes possess a measurable warmth level (TH) that changes in opposite proportion to their size (MBH). Beyond the math framework, the study explores the broader implications of this discovery, including how these cosmic voids gradually fade away and their internal disorder measurement.\n",
    "\n",
    "The investigation starts with an overview of unit analysis - a method for studying measurable quantities in different scales. Starting from the basic idea that a stationary space hole's properties are determined by its mass, the researchers use mathematical relationships to calculate the heat output. Central to their calculations is the space-mass attraction constant Gm.\n",
    "\n",
    "Examining the implications of Hawking's insight, the study explains the importance of space holes having a measurable temperature. The researchers explore the role of internal disorder in this context, noting how it relates to the size of the hole's outer boundary. In its final sections, the work addresses both the challenges in detecting this heat output and the promising field of simulated gravity - a discipline focused on recreating space-like conditions through laboratory setups.\n",
    "\n",
    "The text covers various specialized ideas, ranging from unit analysis and heat radiation to disorder measurement and simulated gravity systems. Mathematical formulas support a detailed calculation of the heat output levels. Written with precision and clarity, the content remains approachable for those with a background in space science and matter studies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"The paper commemorates the 50th anniversary of Stephen Hawking's groundbreaking discovery of black holes emitting thermal radiation, known as Hawking radiation. The authors use dimensional analysis to derive the Hawking temperature, a fundamental concept in modern astrophysics. They demonstrate that black holes have an absolute temperature, TH, which depends inversely on their mass, MBH. The authors also explore the physical implications of Hawking's discovery, including the evaporation of black holes and their entropy.\n",
    "\n",
    "The paper begins by introducing the concept of dimensional analysis, a tool used to study physical quantities with different dimensions. The authors then derive the Hawking temperature using dimensional analysis, starting from the assumption that a static black hole is characterized by its mass. They introduce the standard gravitational parameter, Gm, which is essential to the derivation.\n",
    "\n",
    "The authors then discuss the physical meaning of Hawking's discovery, highlighting the implications of black holes having an absolute temperature. They also explore the concept of entropy, which is proportional to the area of the event horizon. The paper concludes by discussing the challenges of detecting Hawking radiation and the potential applications of analogue gravity, a field that aims to create physical systems that mimic the behavior of gravitational phenomena.\n",
    "\n",
    "Throughout the paper, the authors use a range of technical terms, including dimensional analysis, Hawking radiation, entropy, and analogue gravity. They also provide a detailed derivation of the Hawking temperature, using mathematical equations and formulas. The paper is written in a clear and concise style, making it accessible to readers with a background in astrophysics and physics.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class ScientificMetricsEvaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize scientific metrics\"\"\"\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
    "            ['rouge1', 'rouge2', 'rougeL', 'rougeLsum'],\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        self.bert_scorer = BERTScorer(\n",
    "            model_type=\"adsabs/astroBERT\",\n",
    "            num_layers=9,\n",
    "            batch_size=32,\n",
    "            nthreads=4,\n",
    "            all_layers=False,\n",
    "            idf=False,\n",
    "            lang='en',\n",
    "            rescale_with_baseline=False\n",
    "        )\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"adsabs/astroBERT\")\n",
    "        self.max_length = 512\n",
    "        self.overlap = 50  # Number of tokens to overlap between chunks\n",
    "\n",
    "    def _preprocess_text_rouge(self, text: str) -> str:\n",
    "        \"\"\"Preprocessing for ROUGE - keeps whitespace\"\"\"\n",
    "        text = re.sub(r'[^\\w\\s,.!?-]', '', text)\n",
    "        return ' '.join(text.split()).strip()\n",
    "\n",
    "    def _preprocess_text_bert(self, text: str) -> str:\n",
    "        \"\"\"Minimal preprocessing for BERT - only removes potentially problematic characters\"\"\"\n",
    "        return re.sub(r'[^\\w,.!?-]', '', text)\n",
    "\n",
    "    def _chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Chunk text into overlapping sequences that fit within BERT's maximum sequence length\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to be chunked\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: List of text chunks\n",
    "        \"\"\"\n",
    "        # Tokenize the entire text\n",
    "        tokens = self.tokenizer.encode(text, add_special_tokens=False)\n",
    "        \n",
    "        # Initialize chunks\n",
    "        chunks = []\n",
    "        \n",
    "        # Create overlapping chunks\n",
    "        for i in range(0, len(tokens), self.max_length - self.overlap):\n",
    "            # Take a chunk of max_length, starting from i\n",
    "            chunk_tokens = tokens[i:i + self.max_length]\n",
    "            \n",
    "            # Add special tokens\n",
    "            chunk_tokens = [self.tokenizer.cls_token_id] + chunk_tokens + [self.tokenizer.sep_token_id]\n",
    "            \n",
    "            # Decode the chunk\n",
    "            chunk = self.tokenizer.decode(chunk_tokens)\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def calculate_rouge(self, reference: str, candidate: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate ROUGE scores\"\"\"\n",
    "        reference = self._preprocess_text_rouge(reference)\n",
    "        candidate = self._preprocess_text_rouge(candidate)\n",
    "        \n",
    "        scores = self.rouge_scorer.score(reference, candidate)\n",
    "        \n",
    "        return {\n",
    "            'rouge1': scores['rouge1'].fmeasure,\n",
    "            'rouge2': scores['rouge2'].fmeasure,\n",
    "            'rougeL': scores['rougeL'].fmeasure,\n",
    "            'rougeLsum': scores['rougeLsum'].fmeasure\n",
    "        }\n",
    "\n",
    "    def calculate_bertscore(self, reference: str, candidate: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate BERTScore using chunked texts\"\"\"\n",
    "        reference = self._preprocess_text_bert(reference)\n",
    "        candidate = self._preprocess_text_bert(candidate)\n",
    "        \n",
    "        # Chunk both reference and candidate\n",
    "        reference_chunks = self._chunk_text(reference)\n",
    "        candidate_chunks = self._chunk_text(candidate)\n",
    "        \n",
    "        # Calculate BERTScore for each chunk pair and average\n",
    "        chunk_scores = []\n",
    "        for ref_chunk in reference_chunks:\n",
    "            for cand_chunk in candidate_chunks:\n",
    "                P, R, F1 = self.bert_scorer.score([cand_chunk], [ref_chunk])\n",
    "                chunk_scores.append((float(P[0]), float(R[0]), float(F1[0])))\n",
    "        \n",
    "        # Average the chunk scores\n",
    "        if chunk_scores:\n",
    "            avg_precision = np.mean([score[0] for score in chunk_scores])\n",
    "            avg_recall = np.mean([score[1] for score in chunk_scores])\n",
    "            avg_f1 = np.mean([score[2] for score in chunk_scores])\n",
    "        else:\n",
    "            avg_precision = avg_recall = avg_f1 = 0.0\n",
    "        \n",
    "        return {\n",
    "            'precision': avg_precision,\n",
    "            'recall': avg_recall,\n",
    "            'f1': avg_f1\n",
    "        }\n",
    "\n",
    "    def calculate_ngram_novelty(self, reference: str, candidate: str) -> float:\n",
    "        \"\"\"Calculate the proportion of novel n-grams in the candidate\"\"\"\n",
    "        reference = self._preprocess_text_rouge(reference)\n",
    "        candidate = self._preprocess_text_rouge(candidate)\n",
    "        \n",
    "        def get_ngrams(text, n):\n",
    "            words = text.split()\n",
    "            return set(' '.join(words[i:i+n]) for i in range(len(words)-n+1))\n",
    "        \n",
    "        # Calculate for different n-gram sizes\n",
    "        novelty_scores = []\n",
    "        for n in [1, 2, 3]:\n",
    "            ref_ngrams = get_ngrams(reference, n)\n",
    "            cand_ngrams = get_ngrams(candidate, n)\n",
    "            novel_ngrams = cand_ngrams - ref_ngrams\n",
    "            if cand_ngrams:\n",
    "                novelty_scores.append(len(novel_ngrams) / len(cand_ngrams))\n",
    "        \n",
    "        return np.mean(novelty_scores) if novelty_scores else 0.0\n",
    "\n",
    "    def evaluate_summary(self, reference: str, candidate: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate all metrics and combine them with weights optimized for abstractive summaries\"\"\"\n",
    "        rouge_scores = self.calculate_rouge(reference, candidate)\n",
    "        bert_scores = self.calculate_bertscore(reference, candidate)\n",
    "        novelty_score = self.calculate_ngram_novelty(reference, candidate)\n",
    "        \n",
    "        final_scores = {\n",
    "            'rouge1': rouge_scores['rouge1'],\n",
    "            'rouge2': rouge_scores['rouge2'],\n",
    "            'rougeL': rouge_scores['rougeL'],\n",
    "            'rougeLsum': rouge_scores['rougeLsum'],\n",
    "            'bertscore_precision': bert_scores['precision'],\n",
    "            'bertscore_recall': bert_scores['recall'],\n",
    "            'bertscore_f1': bert_scores['f1'],\n",
    "            'ngram_novelty': novelty_score\n",
    "        }\n",
    "        \n",
    "        # New weighting scheme that rewards:\n",
    "        # 1. High semantic similarity (BERTScore) \n",
    "        # 2. High n-gram novelty (high term novelty and similiarity means that it understands the text sementically and is able to use terms interchangebly)\n",
    "        weights = {\n",
    "            'bertscore_f1': 0.5,      # Semantic preservation\n",
    "            'ngram_novelty': 0.5,     # Vocabulary diversity        \n",
    "        }\n",
    "        \n",
    "        final_scores['abstractive_score'] = sum(\n",
    "            final_scores[metric] * weight\n",
    "            for metric, weight in weights.items()\n",
    "        )\n",
    "        \n",
    "        return final_scores\n",
    "\n",
    "def evaluate_scientific_summary(reference_text: str, summary_text: str) -> Dict[str, float]:\n",
    "    \"\"\"Convenience function for evaluating scientific summaries\"\"\"\n",
    "    evaluator = ScientificMetricsEvaluator()\n",
    "    return evaluator.evaluate_summary(reference_text, summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.43478260869565216, 'rouge2': 0.11764705882352941, 'rougeL': 0.32136105860113423, 'rougeLsum': 0.32136105860113423, 'bertscore_precision': 0.7116235196590424, 'bertscore_recall': 0.7540913820266724, 'bertscore_f1': 0.7305146753787994, 'ngram_novelty': 0.8522407782276202, 'abstractive_score': 0.6852017828779127}\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_scientific_summary(paper, summary)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
