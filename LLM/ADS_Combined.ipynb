{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline here: ADS_Search -> Download Clean Papers -> Summarize -> Concat Together -> N-Gram -> Output CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "import io\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"Applies a series of cleaning steps to the text.\"\"\"\n",
    "    keywords = [\"REFERENCES\", \"ACKNOWLEDGEMENTS\", \"References\", \"Acknowledgements\"]\n",
    "    \n",
    "    text = find_and_remove_references(text, keywords)\n",
    "    text = remove_text_in_brackets(text)\n",
    "    text = remove_lines_starting_with_number_or_symbol(text)\n",
    "    text = remove_lines_with_one_word(text)\n",
    "    text = remove_empty_lines(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def find_and_remove_references(text, keywords):\n",
    "    \"\"\"Finds and removes text after the first occurrence of any of the given keywords.\"\"\"\n",
    "    earliest_position = float('inf')\n",
    "    for keyword in keywords:\n",
    "        position = text.find(keyword)\n",
    "        if position != -1:\n",
    "            earliest_position = min(position, earliest_position)\n",
    "    \n",
    "    if earliest_position != float('inf'):\n",
    "        text = text[:earliest_position]\n",
    "    return text\n",
    "\n",
    "def remove_text_in_brackets(text):\n",
    "    \"\"\"Removes text enclosed in parentheses or square brackets.\"\"\"\n",
    "    pattern = r'\\([^)]*\\)|\\[[^\\]]*\\]'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def remove_lines_starting_with_number_or_symbol(text):\n",
    "    \"\"\"Removes lines starting with a number or symbol.\"\"\"\n",
    "    pattern = r'^[\\d\\W].*$'\n",
    "    return re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "\n",
    "def remove_lines_with_one_word(text):\n",
    "    \"\"\"Removes lines containing only one word.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    pattern = r'^\\s*\\w+\\s*$'\n",
    "    filtered_lines = [line for line in lines if not re.match(pattern, line)]\n",
    "    return '\\n'.join(filtered_lines)\n",
    "\n",
    "def remove_empty_lines(text):\n",
    "    \"\"\"Removes empty lines.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    non_empty_lines = [line for line in lines if line.strip() != '']\n",
    "    return '\\n'.join(non_empty_lines)\n",
    "\n",
    "def process_arxiv_paper(arxiv_id, save_path='content'):\n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    # Format the PDF URL\n",
    "    url = f'https://arxiv.org/pdf/{arxiv_id}.pdf'\n",
    "    \n",
    "    try:\n",
    "        # Download PDF and convert to text\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Create a PDF reader object from the downloaded content\n",
    "        pdf_file = io.BytesIO(response.content)\n",
    "        pdf_reader = PdfReader(pdf_file)\n",
    "        \n",
    "        # Extract text from all pages\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "        # Apply text processing steps\n",
    "        text = process_text(text)\n",
    "        \n",
    "        # Clean the text (remove special characters but keep whitespace)\n",
    "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        \n",
    "        # Save as text file\n",
    "        txt_filename = os.path.join(save_path, f'{arxiv_id}.txt')\n",
    "        with open(txt_filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned_text)\n",
    "            \n",
    "        print(f\"Successfully downloaded, converted, and cleaned: {txt_filename}\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading PDF: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")\n",
    "\n",
    "arxiv_id = '0910.2715'\n",
    "process_arxiv_paper(arxiv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded, converted, and cleaned: content\\0910.2715.txt\n"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# import os\n",
    "# from PyPDF2 import PdfReader\n",
    "# import io\n",
    "# import re\n",
    "# from pathlib import Path\n",
    "\n",
    "# def process_text(text):\n",
    "#     \"\"\"Applies a series of cleaning steps to the text.\"\"\"\n",
    "#     keywords = [\"REFERENCES\", \"ACKNOWLEDGEMENTS\", \"References\", \"Acknowledgements\"]\n",
    "    \n",
    "#     text = find_and_remove_references(text, keywords)\n",
    "#     text = remove_text_in_brackets(text)\n",
    "#     text = remove_lines_starting_with_number_or_symbol(text)\n",
    "#     text = remove_lines_with_one_word(text)\n",
    "#     text = remove_empty_lines(text)\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# def find_and_remove_references(text, keywords):\n",
    "#     \"\"\"Finds and removes text after the first occurrence of any of the given keywords.\"\"\"\n",
    "#     earliest_position = float('inf')\n",
    "#     for keyword in keywords:\n",
    "#         position = text.find(keyword)\n",
    "#         if position != -1:\n",
    "#             earliest_position = min(position, earliest_position)\n",
    "    \n",
    "#     if earliest_position != float('inf'):\n",
    "#         text = text[:earliest_position]\n",
    "#     return text\n",
    "\n",
    "# def remove_text_in_brackets(text):\n",
    "#     \"\"\"Removes text enclosed in parentheses or square brackets.\"\"\"\n",
    "#     pattern = r'\\([^)]*\\)|\\[[^\\]]*\\]'\n",
    "#     return re.sub(pattern, '', text)\n",
    "\n",
    "# def remove_lines_starting_with_number_or_symbol(text):\n",
    "#     \"\"\"Removes lines starting with a number or symbol.\"\"\"\n",
    "#     pattern = r'^[\\d\\W].*$'\n",
    "#     return re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "\n",
    "# def remove_lines_with_one_word(text):\n",
    "#     \"\"\"Removes lines containing only one word.\"\"\"\n",
    "#     lines = text.split('\\n')\n",
    "#     pattern = r'^\\s*\\w+\\s*$'\n",
    "#     filtered_lines = [line for line in lines if not re.match(pattern, line)]\n",
    "#     return '\\n'.join(filtered_lines)\n",
    "\n",
    "# def remove_empty_lines(text):\n",
    "#     \"\"\"Removes empty lines.\"\"\"\n",
    "#     lines = text.split('\\n')\n",
    "#     non_empty_lines = [line for line in lines if line.strip() != '']\n",
    "#     return '\\n'.join(non_empty_lines)\n",
    "\n",
    "# def process_arxiv_paper(arxiv_id, save_path='content'):\n",
    "#     # Create directory if it doesn't exist\n",
    "#     if not os.path.exists(save_path):\n",
    "#         os.makedirs(save_path)\n",
    "    \n",
    "#     # Format the PDF URL\n",
    "#     url = f'https://arxiv.org/pdf/{arxiv_id}.pdf'\n",
    "    \n",
    "#     try:\n",
    "#         # Download PDF and convert to text\n",
    "#         response = requests.get(url)\n",
    "#         response.raise_for_status()\n",
    "        \n",
    "#         # Create a PDF reader object from the downloaded content\n",
    "#         pdf_file = io.BytesIO(response.content)\n",
    "#         pdf_reader = PdfReader(pdf_file)\n",
    "        \n",
    "#         # Extract text from all pages\n",
    "#         text = \"\"\n",
    "#         for page in pdf_reader.pages:\n",
    "#             text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "#         # Apply text processing steps\n",
    "#         text = process_text(text)\n",
    "        \n",
    "#         # Clean the text (remove special characters and whitespace)\n",
    "#         cleaned_text = re.sub(r'[^a-zA-Z0-9]', '', text)\n",
    "        \n",
    "#         # Save as text file\n",
    "#         txt_filename = os.path.join(save_path, f'{arxiv_id}.txt')\n",
    "#         with open(txt_filename, 'w', encoding='utf-8') as f:\n",
    "#             f.write(cleaned_text)\n",
    "            \n",
    "#         print(f\"Successfully downloaded, converted, and cleaned: {txt_filename}\")\n",
    "        \n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Error downloading PDF: {e}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing file: {e}\")\n",
    "\n",
    "# arxiv_id = '0910.2715'\n",
    "# process_arxiv_paper(arxiv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import os\n",
    "# from PyPDF2 import PdfReader\n",
    "# import io\n",
    "\n",
    "# def pdf_to_text(arxiv_id, save_path='content'):\n",
    "#     # Create directory if it doesn't exist\n",
    "#     if not os.path.exists(save_path):\n",
    "#         os.makedirs(save_path)\n",
    "    \n",
    "#     # Format the PDF URL\n",
    "#     url = f'https://arxiv.org/pdf/{arxiv_id}.pdf'\n",
    "    \n",
    "#     try:\n",
    "#         # Download PDF\n",
    "#         response = requests.get(url)\n",
    "#         response.raise_for_status()\n",
    "        \n",
    "#         # Create a PDF reader object from the downloaded content\n",
    "#         pdf_file = io.BytesIO(response.content)\n",
    "#         pdf_reader = PdfReader(pdf_file)\n",
    "        \n",
    "#         # Extract text from all pages\n",
    "#         text = \"\"\n",
    "#         for page in pdf_reader.pages:\n",
    "#             text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "#         # Save as text file\n",
    "#         txt_filename = os.path.join(save_path, f'{arxiv_id}.txt')\n",
    "#         with open(txt_filename, 'w', encoding='utf-8') as f:\n",
    "#             f.write(text)\n",
    "            \n",
    "#         print(f\"Successfully converted and saved to: {txt_filename}\")\n",
    "        \n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Error downloading PDF: {e}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing PDF: {e}\")\n",
    "\n",
    "\n",
    "# arxiv_id = '2411.04177'\n",
    "# pdf_to_text(arxiv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# from pathlib import Path\n",
    "\n",
    "# def clean_text_files(directory):\n",
    "#     # Convert directory to Path object\n",
    "#     dir_path = Path(directory)\n",
    "    \n",
    "#     # Walk through all files in directory and subdirectories\n",
    "#     for file_path in dir_path.rglob('*.txt'):\n",
    "#         try:\n",
    "#             # Read the original content\n",
    "#             with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#                 content = file.read()\n",
    "            \n",
    "#             # First remove special characters (keeping letters and numbers)\n",
    "#             cleaned_content = re.sub(r'[^a-zA-Z0-9]', '', content)\n",
    "            \n",
    "#             # Write the cleaned content back to the file\n",
    "#             with open(file_path, 'w', encoding='utf-8') as file:\n",
    "#                 file.write(cleaned_content)\n",
    "            \n",
    "#             print(f\"Cleaned: {file_path}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {file_path}: {str(e)}\")\n",
    "\n",
    "# clean_text_files('./content/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
