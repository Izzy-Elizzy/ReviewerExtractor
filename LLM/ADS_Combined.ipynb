{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ads\n",
    "import operator\n",
    "import re\n",
    "import nltk\n",
    "from nltk import ngrams, word_tokenize, bigrams, trigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "import fnmatch\n",
    "import requests\n",
    "import io\n",
    "from PyPDF2 import PdfReader\n",
    "from TextAnalysis import stopword_loader, count_words, topwords, topbigrams, toptrigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline here: ADS_Search -> Download Clean Papers -> Summarize -> Concat Together -> N-Gram -> Output CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "from PyPDF2 import PdfReader\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and process the extracted text\"\"\"\n",
    "    # Remove references and acknowledgements sections\n",
    "    keywords = [\"REFERENCES\", \"ACKNOWLEDGEMENTS\", \"References\", \"Acknowledgements\"]\n",
    "    for keyword in keywords:\n",
    "        pos = text.find(keyword)\n",
    "        if pos != -1:\n",
    "            text = text[:pos]\n",
    "    \n",
    "    # Clean up the text\n",
    "    text = re.sub(r'\\([^)]*\\)|\\[[^\\]]*\\]', '', text)  # Remove bracketed content\n",
    "    \n",
    "    # Process text line by line\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Skip empty lines, single words, and lines starting with numbers/symbols\n",
    "        if (line and \n",
    "            not re.match(r'^\\s*\\w+\\s*$', line) and \n",
    "            not re.match(r'^[\\d\\W].*$', line)):\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    return ' '.join(cleaned_lines)\n",
    "\n",
    "def get_arxiv_text(arxiv_id):  # Updated\n",
    "    \"\"\"Get cleaned text from arXiv PDF\"\"\"\n",
    "    arxiv_id = arxiv_id.split('arXiv:')[-1] #add .pdf\n",
    "    url = f'https://arxiv.org/pdf/{arxiv_id}.pdf'\n",
    "    try:\n",
    "        response = requests.get(url, stream=True) # stream = true since some arXiv files can be large\n",
    "        response.raise_for_status()\n",
    "        pdf = PdfReader(io.BytesIO(response.content))\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        return clean_text(text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading PDF: {e}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(arxiv_id):\n",
    "    return get_arxiv_text(arxiv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(csv_filepath, directorypath):\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "    df['summaries'] = \"\"\n",
    "    df['topwords'] = \"\"\n",
    "    df['topbigrams'] = \"\"\n",
    "    df['toptrigrams'] = \"\"\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        arxiv_ids_str = row['Identifier']\n",
    "\n",
    "        try:\n",
    "            arxiv_ids = eval(arxiv_ids_str)\n",
    "            all_summaries = []\n",
    "            for arxiv_id in arxiv_ids:\n",
    "                try:\n",
    "                    summary = get_summary(arxiv_id)\n",
    "                    if summary:\n",
    "                        all_summaries.append(summary)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {arxiv_id}: {e}\")\n",
    "\n",
    "\n",
    "            combined_summary = ' '.join(all_summaries)\n",
    "\n",
    "\n",
    "            if combined_summary:\n",
    "                # Use functions from text_analysis module\n",
    "                top10words = topwords(combined_summary, directorypath)  \n",
    "                top10bigrams = topbigrams(combined_summary, directorypath)\n",
    "                top10trigrams = toptrigrams(combined_summary, directorypath)\n",
    "\n",
    "\n",
    "                df.at[index, 'topwords'] = top10words\n",
    "                df.at[index, 'topbigrams'] = top10bigrams\n",
    "                df.at[index, 'toptrigrams'] = top10trigrams\n",
    "                df.at[index, 'summaries'] = combined_summary\n",
    "\n",
    "            else:\n",
    "                df.at[index, 'summaries'] = None\n",
    "                df.at[index, 'topwords'] = None\n",
    "                df.at[index, 'topbigrams'] = None\n",
    "                df.at[index, 'toptrigrams'] = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        except (SyntaxError, NameError) as e:\n",
    "            print(f\"Error evaluating identifier string in row {index}: {e}\")\n",
    "            df.at[index, 'summaries'] = None\n",
    "            df.at[index, 'topwords'] = None\n",
    "            df.at[index, 'topbigrams'] = None\n",
    "            df.at[index, 'toptrigrams'] = None\n",
    "\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading PDF: 404 Client Error: Not Found for url: https://arxiv.org/pdf/10.1088/0004-637X/707/1/103.pdf\n",
      "Error downloading PDF: 404 Client Error: Not Found for url: http://arxiv.org/pdf/2009ApJ...707..103E\n",
      "Error downloading PDF: 404 Client Error: Not Found for url: http://arxiv.org/pdf/10.48550/arXiv.0910.2715\n",
      "                                          Identifier  ...                                        toptrigrams\n",
      "0  ['arXiv:0910.2715', '10.1088/0004-637X/707/1/1...  ...  [((density, pro, le), 7), ((envelope, infall, ...\n",
      "\n",
      "[1 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "directorypath = 'stopwords.txt' \n",
    "csv_file = 'small_identifier_sample.csv' \n",
    "processed_df = process_csv(csv_file, directorypath)\n",
    "print(processed_df.head())\n",
    "processed_df.to_csv('combined_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import os\n",
    "# from PyPDF2 import PdfReader\n",
    "# import io\n",
    "# import re\n",
    "# from pathlib import Path\n",
    "\n",
    "# def process_text(text):\n",
    "#     \"\"\"Applies a series of cleaning steps to the text.\"\"\"\n",
    "#     keywords = [\"REFERENCES\", \"ACKNOWLEDGEMENTS\", \"References\", \"Acknowledgements\"]\n",
    "    \n",
    "#     text = find_and_remove_references(text, keywords)\n",
    "#     text = remove_text_in_brackets(text)\n",
    "#     text = remove_lines_starting_with_number_or_symbol(text)\n",
    "#     text = remove_lines_with_one_word(text)\n",
    "#     text = remove_empty_lines(text)\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# def find_and_remove_references(text, keywords):\n",
    "#     \"\"\"Finds and removes text after the first occurrence of any of the given keywords.\"\"\"\n",
    "#     earliest_position = float('inf')\n",
    "#     for keyword in keywords:\n",
    "#         position = text.find(keyword)\n",
    "#         if position != -1:\n",
    "#             earliest_position = min(position, earliest_position)\n",
    "    \n",
    "#     if earliest_position != float('inf'):\n",
    "#         text = text[:earliest_position]\n",
    "#     return text\n",
    "\n",
    "# def remove_text_in_brackets(text):\n",
    "#     \"\"\"Removes text enclosed in parentheses or square brackets.\"\"\"\n",
    "#     pattern = r'\\([^)]*\\)|\\[[^\\]]*\\]'\n",
    "#     return re.sub(pattern, '', text)\n",
    "\n",
    "# def remove_lines_starting_with_number_or_symbol(text):\n",
    "#     \"\"\"Removes lines starting with a number or symbol.\"\"\"\n",
    "#     pattern = r'^[\\d\\W].*$'\n",
    "#     return re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "\n",
    "# def remove_lines_with_one_word(text):\n",
    "#     \"\"\"Removes lines containing only one word.\"\"\"\n",
    "#     lines = text.split('\\n')\n",
    "#     pattern = r'^\\s*\\w+\\s*$'\n",
    "#     filtered_lines = [line for line in lines if not re.match(pattern, line)]\n",
    "#     return '\\n'.join(filtered_lines)\n",
    "\n",
    "# def remove_empty_lines(text):\n",
    "#     \"\"\"Removes empty lines.\"\"\"\n",
    "#     lines = text.split('\\n')\n",
    "#     non_empty_lines = [line for line in lines if line.strip() != '']\n",
    "#     return '\\n'.join(non_empty_lines)\n",
    "\n",
    "# def process_arxiv_paper(arxiv_id, save_path='content'):\n",
    "#     # Create directory if it doesn't exist\n",
    "#     if not os.path.exists(save_path):\n",
    "#         os.makedirs(save_path)\n",
    "    \n",
    "#     # Format the PDF URL\n",
    "#     url = f'https://arxiv.org/pdf/{arxiv_id}.pdf'\n",
    "    \n",
    "#     try:\n",
    "#         # Download PDF and convert to text\n",
    "#         response = requests.get(url)\n",
    "#         response.raise_for_status()\n",
    "        \n",
    "#         # Create a PDF reader object from the downloaded content\n",
    "#         pdf_file = io.BytesIO(response.content)\n",
    "#         pdf_reader = PdfReader(pdf_file)\n",
    "        \n",
    "#         # Extract text from all pages\n",
    "#         text = \"\"\n",
    "#         for page in pdf_reader.pages:\n",
    "#             text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "#         # Apply text processing steps\n",
    "#         text = process_text(text)\n",
    "        \n",
    "#         # Clean the text (remove special characters and whitespace)\n",
    "#         cleaned_text = re.sub(r'[^a-zA-Z0-9]', '', text)\n",
    "        \n",
    "#         # Save as text file\n",
    "#         txt_filename = os.path.join(save_path, f'{arxiv_id}.txt')\n",
    "#         with open(txt_filename, 'w', encoding='utf-8') as f:\n",
    "#             f.write(cleaned_text)\n",
    "            \n",
    "#         print(f\"Successfully downloaded, converted, and cleaned: {txt_filename}\")\n",
    "        \n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Error downloading PDF: {e}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing file: {e}\")\n",
    "\n",
    "# arxiv_id = '0910.2715'\n",
    "# process_arxiv_paper(arxiv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import os\n",
    "# from PyPDF2 import PdfReader\n",
    "# import io\n",
    "\n",
    "# def pdf_to_text(arxiv_id, save_path='content'):\n",
    "#     # Create directory if it doesn't exist\n",
    "#     if not os.path.exists(save_path):\n",
    "#         os.makedirs(save_path)\n",
    "    \n",
    "#     # Format the PDF URL\n",
    "#     url = f'https://arxiv.org/pdf/{arxiv_id}.pdf'\n",
    "    \n",
    "#     try:\n",
    "#         # Download PDF\n",
    "#         response = requests.get(url)\n",
    "#         response.raise_for_status()\n",
    "        \n",
    "#         # Create a PDF reader object from the downloaded content\n",
    "#         pdf_file = io.BytesIO(response.content)\n",
    "#         pdf_reader = PdfReader(pdf_file)\n",
    "        \n",
    "#         # Extract text from all pages\n",
    "#         text = \"\"\n",
    "#         for page in pdf_reader.pages:\n",
    "#             text += page.extract_text() + \"\\n\"\n",
    "        \n",
    "#         # Save as text file\n",
    "#         txt_filename = os.path.join(save_path, f'{arxiv_id}.txt')\n",
    "#         with open(txt_filename, 'w', encoding='utf-8') as f:\n",
    "#             f.write(text)\n",
    "            \n",
    "#         print(f\"Successfully converted and saved to: {txt_filename}\")\n",
    "        \n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Error downloading PDF: {e}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing PDF: {e}\")\n",
    "\n",
    "\n",
    "# arxiv_id = '2411.04177'\n",
    "# pdf_to_text(arxiv_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# from pathlib import Path\n",
    "\n",
    "# def clean_text_files(directory):\n",
    "#     # Convert directory to Path object\n",
    "#     dir_path = Path(directory)\n",
    "    \n",
    "#     # Walk through all files in directory and subdirectories\n",
    "#     for file_path in dir_path.rglob('*.txt'):\n",
    "#         try:\n",
    "#             # Read the original content\n",
    "#             with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#                 content = file.read()\n",
    "            \n",
    "#             # First remove special characters (keeping letters and numbers)\n",
    "#             cleaned_content = re.sub(r'[^a-zA-Z0-9]', '', content)\n",
    "            \n",
    "#             # Write the cleaned content back to the file\n",
    "#             with open(file_path, 'w', encoding='utf-8') as file:\n",
    "#                 file.write(cleaned_content)\n",
    "            \n",
    "#             print(f\"Cleaned: {file_path}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {file_path}: {str(e)}\")\n",
    "\n",
    "# clean_text_files('./content/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8099421958128612"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from benchmark import benchmark\n",
    "\n",
    "benchmark('the cat is on the mat','the feline pet sat on the rug')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
