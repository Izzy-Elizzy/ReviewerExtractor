file_name,Summary,Summaries Top 10 Words,Summaries Top 10 Bigrams,Summaries Top 10 Trigrams
2410.11851v1.txt,"Hawking for everyone: Commemorating half a
century of an unfinished scientific revolution
Jorge Pinochet∗
Centro de Investigaci´on en Educaci´on ,
N´ucleo Pensamiento Computacional y Educaci´on para el Desarrollo Sostenible .
Universidad Metropolitana de Ciencias de la Educaci´on,
Av. Jos´e Pedro Alessandri 774, ˜Nu˜noa, Santiago, Chile.
e-mail: jorge.pinochet@umce.cl
This year marks half a century since Stephen Hawking made his greatest scientific dis-
covery by theoretically proving that “black holes ain’t so black”, as they behave like hot
bodies with an absolute temperature that depends inversely on their mass. This discovery
is expressed by a simple and elegant equation known as the Hawking temperature. The
best way to commemorate this great scientific event is by bringing it to a wide audience.
The simplest and most transparent and intuitive tool to achieve this goal is dimensional
analysis. The objective of this work is to use this tool to derive the Hawking equation,
reveal its meaning, and explore its main physical consequences.
Keywords: Black holes, Hawking temperature, dimensional analysis.
This year marks half a century since Stephen Hawking revolutionised physics by proposing,
in 1974, that “black holes ain’t so black” . Specifically, Hawking theoretically
demonstrated that a black hole emits thermal radiation as if it were a hot body with an
absolute temperature TH, known as the Hawking temperature , which can be calculated
by the equation:
TH =
Despite the scientific relevance of Hawking temperature, and the time that has passed since it
was formulated, it continues to be unknown, and at best misunderstood, by the vast majority
of people.
arXiv:2410.11851v1    1 Oct 2024
The best way to commemorate the 50th anniversary of Hawking’s discovery is to bring it
to a wide audience. The simplest and most transparent and intuitive tool for this task is
dimensional analysis , the application of which requires a minimal knowledge of physics
and mathematics. In this work, we will use dimensional analysis to carry out an original
derivation of the Hawking temperature. Following this, we will explore the meaning of this
equation and its main physical consequences, namely that black holes evaporate and have
entropy.
This manuscript may be useful to gifted high school students, physics teachers, scientists and
engineers, and, more generally, to any educated person who is interested in learning about
Stephen Hawking’s most important scientific contribution.
Figure 1: Stephen Hawking, born in 1942, was a renowned British theoretical physicist known for his
contributions to cosmology and black hole physics. His greatest scientific discovery was the equation
that established that black holes behave as hot bodies with an absolute temperature that depends
inversely on their mass. Despite facing progressive motor paralysis caused by Lou Gehrig’s disease,
Hawking continued to research and communicate his revolutionary ideas until his death in 2018.
Black holes and Hawking’s discovery
Black holes are the most extreme prediction of general relativity, which is the theory of gravity
proposed by Einstein to refine and extend Newton’s law of gravitation . A black hole is a
region of space where there is such a high concentration of matter that nothing can escape
its powerful gravity, not even light . The simplest of these objects is the so-called static
black hole, whose mathematical description depends only on its mass. Eq.  applies to
these objects, so we will focus on them here.
Fig. 2 shows an intuitive representation of a static black hole . The mass of this object is
concentrated in a point region called a singularity, located in the centre of the event horizon,
or simply horizon, which is a spherical surface that defines the outer limit of the black hole.
Although the horizon has no material existence, it can be imagined as a unidirectional mem-
brane that only allows matter or energy to flow inward . The radius of the horizon, called
the gravitational radius , is calculated as :
Rg = 2GMBH
mass of the black hole, that is, the mass confined in the singularity. Since nothing can cross
the horizon to the outside, this region appears completely black. If the description provided
by general relativity is correct, we see that the laws of thermodynamics ensure that the tem-
perature of the horizon is strictly zero; otherwise, the black hole would emit thermal radiation
and would not be black.
Figure 2: Structure of a static black hole.
Broadly speaking, this is the classic black hole paradigm that emerges from general relativ-
ity. In 1974, when Hawking entered the scene, no one questioned this paradigm. However,
Hawking dared to question it, proving that “black holes ain’t so black”. Specifically, Hawking
proved that the horizon behaves as a hot body with an absolute temperature TH, the Hawking
temperature , which is proportional to the gravity at the horizon.
Figure 3: Black holes emit thermal radiation, have entropy, and gradually evaporate.
Table 1: Theories and physical constants involved in the Hawking temperature
Characteristic constant
General relativity
G = 6.67 × 10−11N · m2 · kg−2
Gravitational constant
c = 3 × 108m · s−1
Speed of light in vacuum
Quantum mechanics
ℏ= 1.05 × 10−34kg · m2 · s−1
Planck constant
k = 1.38 × 10−23kg · m2 · K−1
Boltzmann constant
As a consequence, the horizon emits thermal radiation in all directions, the so-called Hawking
radiation .
According to Einstein’s mass-energy equivalence, the energy radiated
by the black hole leads to a gradual reduction of MBH and RS through a process called
evaporation . Furthermore, from the laws of thermodynamics, this implies that a black
hole has entropy, which turns out to be proportional to the area of the horizon. From Eq.
ABH = 4πR2
g = 16πG2M2
the universe at a macroscopic level, with quantum mechanics, which describes the universe at
a microscopic level, and thermodynamics, which describes thermal phenomena . Table 1
shows the fundamental physical constants that characterise these theories, which will be very
useful when dimensionally deriving the Hawking temperature.
Table 2: SI units and their corresponding dimensions
SI unit
SI symbol
To facilitate the derivation, which is presented in the next section, the constants in Table
showing the dimensions that are relevant for our purposes along with their symbols and the
associated SI units.
To guide readers who are not familiar with dimensional analysis, an
appendix has been included that summarises the main concepts of this tool.
Derivation of the Hawking temperature
All the physics we need for a dimensional derivation of the Hawking temperature is sum-
marised in Tables 1 and 2, and in the idea that the description of a static black hole depends
only on its mass. We will start by assuming that we do not know the Hawking equation. Ob-
viously, our ignorance on the subject cannot be complete: in fact, to carry out a dimensional
derivation, it is always necessary to start with a hypothesis about the dimensional parameters
involved  .
To formulate our hypothesis, we must keep in mind that Hawking’s great achievement was to
combine general relativity with quantum mechanics and thermodynamics to demonstrate that
a static black hole has an absolute temperature TH. As shown in Table 1, this suggests that
TH depends on the characteristic constants of these three theories, G, c, ℏ, k. On the other
hand, we also know that a static black hole is completely characterised by its mass, MBH.
This shows us that a total of six dimensional parameters are involved, TH, G, c, ℏ, k, MBH,
and four dimensions, M, L, T, Θ. However, we can reduce these parameters to five, and thus
simplify the dimensional derivation of TH, by introducing a new variable which in astrophysics
is known as the standard gravitational parameter, and for a celestial body of mass m is defined
as µ ≡Gm. The usefulness of this parameter lies in the fact that in problems where gravity
applies, as is the case with Hawking’s discovery, the product Gm appears frequently1.
that the use of µ is essential to this derivation. Indeed, with p = 6 parameters  and d = 4
dimensions , the number of dimensionless groups or Pi groups that can be formed is d −p = 2,
Table 3: Dimensional parameters, symbols and dimensions
Hawking temperature
M0L0T 0Θ1
Speed of light in vacuum
M0L1T −1Θ0
Planck constant
M1L2T −1Θ0
Boltzmann constant
M1L2T −2Θ−1
Standard gravitational parameter
M0L3T −2Θ0
Under these conditions, if we define the standard gravitational parameter as µ ≡GMBH, we
can formulate our hypothesis in the form:
Hypothesis: TH = f.
The specific information needed to derive the Hawking equation is summarised in Table 3,
which contains the dimensional parameters, their symbols, and the corresponding dimensions.
We can rewrite our hypothesis in the form:
TH = Πcx1ℏx2kx3µx4,
unknown, and our main objective is to determine their numerical values. Starting from the
rules of dimensional algebra, and remembering that the notation  means “the dimension of
x” , we can rewrite Eq.  as:
five parameters listed in Table 3, we obtain:
M0L0T 0Θ1 =
M0L0T 0Θ1 = M0x1+x2+x3+0x4Lx1+2x2+2x3+3x4T −x1−x2−2x3−2x4Θ0x1+0x2−x3+0x4.
exponent on the left side, producing a linear system of four equations with four unknowns:
x1 + 2x2 + 2x3 + 3x4 = 0
d = 5 parameters, and since the number of dimensions remains d = 4, the number of Pi groups is reduced to
d −p = 1, and the equation for TH is uniquely determined.
By substitution, the reader can verify that the solution of this system is x1 = 3, x2 = 1, x3 =
TH = Πℏc3
kµ = Π
Eqs.  and  we see that Π = 1/8π. The interested reader can find other simple derivations
of the Hawking temperature in .
The physical meaning of Hawking’s discovery: The Hawking
Hawking temperature is not an isolated result, as it has two physical implications that continue
to raise questions: the first is that black holes evaporate, and the second is that they have
entropy.
To understand these two important results in general terms, we must begin by
exploring the physical meaning of the Hawking temperature.
Table 4: The Hawking effect
Mathematical formulation
Hawking temperature
TH =
Black holes have an absolute temperature TH that depends
inversely on their mass MBH. The greater the value of MBH,
the lower the value ofTH, and vice versa. For known black
holes, TH is undetectable.
Hawking radiation and
tev ∼= 103 G2M3
Black holes evaporate in a characteristic time tev that de-
pends on the cube of their mass. For known black holes, tev
is much older than the age of the universe, and evaporation
is undetectable.
Bekenstein-Hawking
SBH = kc3ABH
Black holes have an entropy SBH that depends on the hori-
zon area ABH, which is proportional to the square of the
mass. The maximum entropy that can accumulate in a re-
gion of fixed size is that of the largest black hole confined in
said region.
Table 4 summarises the equations that describe the Hawking effect, which is the name given
to the set of phenomena associated with Hawking’s discovery. The interested reader will find
very simple derivations of these equations in . In the following sections, we will delve into
the Hawking effect. Due to space limitations, we will not analyse the physical mechanism
responsible for the Hawking temperature.
A reader interested in this topic can refer to
Hawking’s texts , or other works published in this journal, where this topic is addressed
in an accessible way .
Hawking temperature
The first aspect that we must keep in mind in relation to the Hawking temperature is that it
does not depend on the interaction of the black hole with the matter in its environment. A
black hole could be completely isolated and still have a temperature given by Eq. .
We will start by rewriting this equation in a form that allows for quick calculations, using the
values of G, c, ℏ, k from Table 1. If we enter the solar mass M⊙= 1.99 × 1030kg, and multiply
the right side of Eq.  by the dimensionless quotient M⊙/1.99 × 1030kg = 1, we obtain the
Hawking temperature expressed in kelvin  as:
TH = 6.17 × 10−8K
region of its horizon contains no matter or radiation. This equation tells us that “black holes
ain’t so black”, and that they can be very bright and hot if MBH is small enough. To provide
empirical corroboration of Hawking’s discovery, we need to find light black holes. The lightest
ones that have been observed so far have masses of the order of ∼M⊙, while the heaviest
ones can have masses of up to ∼1010M⊙. To obtain an upper bound for TH, we take
MBH = M⊙in Eq. , obtaining TH = 6.17×10−8K. This tiny temperature is undetectable
through astronomical observations.
Figure 4: Graph of Hawking temperature, TH, versus MBH/M⊙.
To illustrate the relationship between TH and MBH more clearly, a graph of Eq.  is shown
in Fig. 4. The horizontal axis has dimensionless units of MBH/M⊙and the vertical axis has
units of 6.17 × 10−8K. It can be observed that TH approaches zero asymptotically as MBH
increases.
Hawking radiation and evaporation
As with any hot body, a black hole emits mainly photons; that is, Hawking radiation is ba-
sically light. However, if TH is high enough, it can also emit neutrinos, electrons, positrons,
etc. This emission process is known as evaporation . Just as the evaporation of a puddle
of water on a summer day gradually reduces the mass of the puddle, Hawking radiation grad-
ually reduces the mass of the black hole, as the photons, electrons, and other particles cross
the horizon to the outside.
Eq.  reveals that the smaller the value of MBH, the more Hawking radiation is emitted
from the horizon, and the more radiation is emitted, the more rapidly MBH is reduced. This
means that evaporation does not occur at a constant rate, but instead increases over time
when they emit more material than they absorb that there will there be a net outward flow
of radiation that will reduce MBH. The time taken for the mass of an isolated black hole of
initial mass MBH to be reduced to zero is called the evaporation time :
tev ∼= 103 G2M3
sionless quotient 3 = 1, the evaporation time is expressed in seconds 
as:
tev = 1073s
M⊙
the vertical axis has units of 1073s. It can be observed that tev increases rapidly with an
increase in MBH.
Eq.  reveals that, in general, tev is extraordinarily large. To obtain a lower bound for tev,
we take MBH = M⊙in Eq. , obtaining tev ∼= 1073s. This figure is 56 orders of magnitude
greater than the age of the universe, which is estimated at ∼1017s. Hence, evaporation is
unobservable, which is consistent with the fact that TH is undetectable.
Figure 5: Graph of evaporation time, tev, versus MBH/M⊙.
Bekenstein-Hawking entropy
The laws of thermodynamics establish that every body with a non-zero temperature has
entropy, and black holes are no exception. According to calculations originally carried out
by the physicist Jacob Bekenstein and perfected by Hawking , the Bekenstein-Hawking
entropy SBH of an isolated black hole is proportional to the area of its event horizon:
SBH = kc3ABH
the black hole and the solar mass:
SBH = 4πkGM2
M⊙
where we have introduced the values of the constants and multiplied the right side of the
equality by the dimensionless quotient 2 = 1, allowing SBH to be expressed
in joules/kelvin , which are the SI units of entropy. Fig. 6 shows a graph of this
equation, where the horizontal axis has units of MBH/M⊙and the vertical axis units of
gives SBH = 1.5 × 1054J · K−1. This is a colossal number, and reveals that black holes are
among the most entropic objects in the universe.
Figure 6: Graph of entropy, SBH, versus MBH/M⊙.
Analogue gravity
If, as the evidence suggests, the smallest black holes have masses on the order of ∼M⊙, with
Hawking temperatures on the order of a millionth of a kelvin, attempts to detect Hawking
radiation may be unsuccessful. To get around this difficulty, the ideal would be to create
black holes in our laboratories, which would allow us to study them in detail, but this is
obviously impossible with current technology , and it may never be possible to develop
such technology.
Due to these difficulties, some specialists have turned their attention towards a field of ex-
perimental physics called analogue gravity, whose objective is implementing physical systems
that imitate the behaviour of gravitational phenomena that are difficult to observe under real
conditions , such as the Hawking effect.
The first person to propose the use of analogue models to study black holes was the Canadian
theoretical physicist William Unruh, who, in 1972, introduced an inventive analogy: he imag-
ined a fish screaming while falling over a waterfall. As the fish falls, it heads toward a point in
the waterfall where the water travels downward faster than sound waves can travel upward.
When the fish crosses this point, its friends in the river above can no longer hear it, since
to enable this, the speed of the sound waves would have to exceed the speed of sound in the
water . According to Unruh, this is analogous to what would happen to an astronaut
who fell into the jaws of a black hole: the astronaut would not be able to communicate with
the base station on Earth, because the signals sent towards the outside would need to move
faster than light, which is prohibited by physical laws.
Figure 7: A fish sends sound waves up a waterfall. The dashed line is an analogue of the horizon,
where the speed with which the waterfall falls is equal to the speed of sound in the water, meaning
that the waves cannot cross the dashed line.
Almost a decade later, Unruh showed that the event horizons of black holes and the sonic
horizons in systems such as his waterfall, which are now known as sonic black holes, can be
described by identical equations . In particular, Unruh showed that sonic black holes can
mimic the Hawking temperature and Hawking radiation. The results of the first experiment
of this type were published in 2014 , so this is a new and developing field of research. The
results obtained so far are encouraging, although not conclusive, and the evidence is consistent
with the discoveries made by Hawking half a century ago.
Final comments: Epitaph for a genius
The great discoveries of physics are expressed through equations that accurately describe a
wide range of phenomena. Some equations stand out for their simplicity, others for their
elegance, and others for their ability to build bridges between apparently dissimilar domains
of reality. However, there is a select group of equations that stand out for combining all these
virtues.
For example, Newton’s law of gravitation falls into this category, as it reconciles celestial
physics with terrestrial physics, as does Einstein’s mass-energy equivalence, which unifies
matter and energy. The Hawking equation also falls into this category, as it bridges general
relativity, quantum mechanics and thermodynamics. Hawking’s tomb in Westminster Abbey
has this equation engraved on it, alongside the inscription: “Here lies what was mortal of
Stephen Hawking”. It is hoped that this work will contribute to a better understanding of
Hawking’s work and his immortal equation, a symbol of a scientific revolution that is far from
over.
Appendix: Dimensional analysis and dimensional algebra
A physical dimension, briefly called a dimension, is a property of an object or phenomenon
that can be measured and expressed in terms of standard fundamental units. Dimensional
analysis is a tool used in science and engineering to study and relate different physical quanti-
ties through their dimensions. For example, length, mass, and time are dimensions that have
associated SI units of the metre, kilogram, and second, respectively. There are seven dimen-
sions, which correspond to the seven fundamental physical magnitudes. These dimensions,
their symbols, and the associated SI units appear in Table A1 below.
In the various applications of dimensional analysis, the notation  is commonly used, which
is read as “the dimensions of a”. For example, if m is the mass of any object, ρ is its density
and v is its speed, we have  = M,  = ML−3 and  = LT −1.
Table A1: SI units and their corresponding dimensions
SI unit
SI symbol
Electric current
Luminous intensity
Amount of substance
To apply dimensional analysis, it is useful to systematise seven principles that, together, we
can call the rules of dimensional algebra. We can consider a set of quantities a, b, c... that
may be pure numbers, variables, physical constants, etc. The rules of dimensional algebra
that can be applied to these quantities are as follows:
i Product rule:  = .
ii Quotient rule:  = /.
iii Commutativity rule:  = .
iv Associativity rule:  = .
v Powers rule:  = n.
vi Homogeneity rule: If a ± b = c then  =  =  = .
vii Dimensionless quantities rule: If a is dimensionless, then  = 1.
I would like to thank to Daniela Balieiro for their valuable comments in the writing of this
paper.","[('hawking', 65), ('black', 55), ('hole', 49), ('temperature', 31), ('mass', 25), ('equation', 21), ('dimensional', 20), ('mbh', 20), ('horizon', 17), ('radiation', 15)]","[(('black', 'hole'), 49), (('hawking', 'temperature'), 22), (('dimensional', 'analysis'), 9), (('hawking', 'radiation'), 8), (('si', 'unit'), 8), (('absolute', 'temperature'), 6), (('hawking', 'discovery'), 6), (('static', 'black'), 6), (('stephen', 'hawking'), 5), (('hot', 'body'), 5)]","[(('static', 'black', 'hole'), 6), (('black', 'holes', 'ain'), 4), (('holes', 'ain', 'black'), 4), (('absolute', 'temperature', 'depends'), 3), (('temperature', 'depends', 'inversely'), 3), (('depends', 'inversely', 'mass'), 3), (('derivation', 'hawking', 'temperature'), 3), (('black', 'holes', 'evaporate'), 3), (('standard', 'gravitational', 'parameter'), 3), (('rules', 'dimensional', 'algebra'), 3)]"
2410.17135v1.txt,"Draft version October 23, 2024
Typeset using LATEX default style in AASTeX631
Reinforcement Learning for Data-Driven Workflows in Radio Interferometry. I.
Principal Demonstration in Calibration
Brian M. Kirk,1, 2, 3 Urvashi Rau,2 and Ramyaa Ramyaa1
Radio interferometry is an observational technique used to study astrophysical phenomena. Data
gathered by an interferometer requires substantial processing before astronomers can extract the scien-
tific information from it. Data processing consists of a sequence of calibration and analysis procedures
where choices must be made about the sequence of procedures as well as the specific configuration of
the procedure itself. These choices are typically based on a combination of measurable data charac-
teristics, an understanding of the instrument itself, an appreciation of the trade-offs between compute
cost and accuracy, and a learned understanding of what is considered “best practice”. A metric of
absolute correctness is not always available and validity is often subject to human judgment. The
underlying principles and software configurations to discern a reasonable workflow for a given dataset
is the subject of training workshops for students and scientists. Our goal is to use objective metrics
that quantify best practice, and numerically map out the decision space with respect to our metrics.
With these objective metrics we demonstrate an automated, data-driven, decision system that is ca-
pable of sequencing the optimal action for processing interferometric data. This paper introduces
a simplified description of the principles behind interferometry and the procedures required for data
processing. We highlight the issues with current automation approaches and propose our ideas for
solving these bottlenecks. A prototype is demonstrated and the results are discussed.
Keywords: Computational Methods  — Computational Astronomy  — Interdisciplinary
astronomy, Radio Astronomy 
Radio astronomy is the study of astronomical objects at radio frequencies and the physical processes generating
that emission.
A radio interferometer uses a collection of antennas to measure the Fourier space of the emission
distribution across the sky. The fundamental observation of every pair of radio antennas is a measurement of the
spatial coherence of the electromagnetic field at their respective locations. Signals from both antennas in each pair are
combined to form a visibility, which is a 2D Fourier component of the sky’s brightness distribution. By accumulating
many different Fourier components through a collection of antennas with different orientations and separations, the
Fourier space representation of the sky can be observed. The  Fourier transform may then be applied to give
astronomers the spatial distribution of the emission intensity in the sky, i.e an image. However, an infinite number of
Fourier components are needed to get a perfect representation of the sky. Radio interferometers have a finite number of
antennas and therefore can only measure a finite number of Fourier components. The missing information introduces
ambiguity into the resulting images. The process of image reconstruction is an inverse model fitting process, subject
to biases introduced by the algorithms and optimization strategies. Additionally, before imaging, the calibration of
each antenna is vitally important. The effect of the antenna and receiver system must be removed from the measured
Fourier components before image reconstruction is attempted. Data that are corrupted by radio frequency interference
or other errors must also be identified and removed as part of the data processing. There are a multitude of different
ways one can process the data, for example using different calibration strategies, choices between outlier detection
arXiv:2410.17135v1    22 Oct 2024
methods, and selecting different imaging algorithms for the inverse modeling. Choices of what to do are often based on
imperfect information and an astronomer’s best judgment, and this can lead to processing the information in multiple
ways that may be considered equally valid .
The main stages of data processing consist of flagging erroneous data, calibrating the instrument hardware, and
reconstructing the image through deconvolution. These stages may be repeated any number of times and in different
sequences for any given dataset.
In the flagging stage, data are identified to be eliminated or ignored from subsequent processing. This can be due to
corruption by radio frequency interference  or other related errors that can lead to outliers. There are different
customizable algorithms suited to identifying different types of RFI. Depending on the situation, data flagging can
be performed multiple times on the raw data before calibration, intermediate calibration products, and or the final
calibrated data. Alternatively, the outlier detection and flagging stage is not always necessary, and depends on the
presence or absence of RFI.
In the calibration stage, observations from a known reference source are used to derive instrumental antenna gains
instrumental effect from the data. Corrections for direction dependent calibration terms are typically expressed as
non-linear matrix equations and can be solved with non-linear least squares approaches in most cases. The accuracy
of these corrections depends on the signal-to-noise ratio  of the observed calibrator data presented to the solver.
With observations recording data across multiple time-steps and across a range of frequencies, the time and frequency
data may be averaged to increase SNR for a more robust correction. However, excessive averaging in time or frequency
can misrepresent the very antenna gain distortions that require modeling and correction. Therefore, a careful data-
dependent choice is required in the amount of averaging that is applied, to maximize SNR for the solver but without
destroying the gain distortions themselves. Additionally, calibration operations may be iterative, to model and remove
different instrumental effects along the signal acquisition chain.
Finally, the image reconstruction stage is an iterative model fitting process. Transforms between the Fourier com-
ponents and the image domain are interleaved with sky modeling steps in a gradient descent approach for chi-square
minimization. There are multiple reconstruction algorithms one can choose from in this stage. Typically, algorithm
selection depends on the type of image being made  and the type of structure being reconstructed . These algorithms differ in image reconstruction quality,
computational runtime, and numerical convergence efficiency. It is a subjective process to choose the algorithms and
the optimal parameters that will give the ‘best’ possible image for a given data set.
The choices made throughout all of the stages is commonly referred to as a ‘recipe’. Any single end-to-end data
processing recipe may be thought of as one possible path out of all possible configurations. For end-to-end processing,
there is a predictable sequence of stages, where each stage may contain several procedures depending on the charac-
teristics of the data. If certain data conditions exist, some procedures, or entire stages, may be revisited as subsequent
procedures, thus forming processing loops .
The full configuration space of the available software tools is quite vast, leading to many possible processing paths.
The understanding needed to create a recipe to process the data is the subject of training workshops where students
and scientists are taught the principles of each stage, how to decide and configure the multiple procedures within a
stage, and how to sequence the stages to prepare data for scientific analysis. It is this decision space that we are
grounding on a numerical basis to enable automated data-driven processing. This work was inspired by the field
of Reinforcement Learning  and specifically AlphaGo . In this paper we demonstrate on a
constrained scenario while only considering the calibration and RFI flagging stages where our objective is to remove
any effect on the visibility data by antenna gains and outliers. This is to validate whether RL methods, in general,
are suitable for our domain and we begin this validation with the simplest RL method, Q-learning. This validation is
fundamental before moving on to more sophisticated RL methods, which are derived from Q-learning.
Q-learning is a Reinforcement Learning method that learns the optimal action-value function from experience. By
repeatedly sampling a Q-table’s states, different actions, and action-values for a variety of transitions, the action value
for a given state-action pair is updated based on the downstream effect it has on the final result. This is a way for the
combined effects of actions that lead to good outcomes  to be reflected in the individual actions that led to
Data Editing
Figure 1. Highlighting the possibility of loops in processing steps  rather than a linear flow .
that outcome. By iteratively updating the action values, the positive  numerical feedback can be used to
steer decisions towards the desired outcome. This is a hallmark of Reinforcement Learning.
Current automated approaches are little more than hard-coding a “few” of the most-used non-looping recipes for
processing, with a small set of heuristics-based decision points.
In the field these hard-coded recipes are called
Pipelines are designed for the general use-case rather than for being specific.
A pipeline includes all
processing steps and parameter settings the average dataset would need, regardless of a particular dataset’s needs.
For example, a dataset with little-to-no RFI may not need any flagging at all. Such hard-coded recipes are inflexible
to different characteristics of an observation which may warrant different processing actions or parameter tuning.
The pipelines , VLA , SRDP , IDIA ,
HiFAST, uGMRT’s CAPTURE, Artip, etc.)
are designed to be robust for datasets that fall within a narrow range of data characteristics and observation modes.
For example, the ALMA pipeline is designed to address a specific range of use-cases and processes 95% of qualifying
datasets through the pipeline, with the remaining 5% processed manually. Out of the qualifying 95% that go through
the pipeline, about a third of datasets typically require human intervention to improve the end result .
The VLA produces a more diverse set of input datasets , all of which are passed through its standard
pipeline  and inspected manually. Out of these, a quarter are reprocessed by humans adjusting
the pipeline scripts, and the rest are handed to the end user as-is to reprocess themselves, if needed .
Finally, both instruments have begun to offer a reprocessing service where end users can request further adjustments
in addition to what was delivered to them , implying that there is often a need for further customization per
dataset. In short, the one-size-fits-all processing via pipelines leaves much room for improvement. This is indicated
by pipelines having yet to achieve outputs that require no further human intervention to customize processing before
being used for astrophysical research. A similar problem is faced in astronomy at other wavelengths , .
Failing to achieve true science-ready automated processing, while that is an issue in itself, causes an additional
penalty: a pipeline’s results may need to be reviewed and reprocessed by a observatory staff. For example, at NRAO
run facilities, trained analysts are used to review the pipeline results. They inspect the results for errors, identify
optimizations that can be made in individual steps or entire sequences, and repeat data processing after the pipeline
has already processed the data. This is because the pipeline results may not meet quality thresholds. This reprocessing
increases operating costs. Furthermore, even after reprocessing is done by a trained analyst, additional enhancement
may still be done by the end-user.
While one can estimate some aspects of an observation, we do not know for certain the signal distribution, observing
conditions, antenna performance, RFI contamination, sources present, and so on which all alter how the data should
be optimally processed. Therefore, what is needed is an automated system sensitive to the contents of the observation
that can decide what the best processing step should be. In this paper we explore Reinforcement Learning methods as a
data-driven approach to automation that can potentially handle observations containing a wide variety of characteristics
and dynamically determine what the best course of action is.
We propose this can be done by measuring the performance of an action in a given situation. By setting an objective
and measuring the effects a particular action has on a wide range of datasets, a function is discovered for how well that
action performs given the state of the data. This can be thought of as a processing action’s “return on investment”
function, or action-value. By discovering this function for each of the different actions, across a large range of datasets
function found, the basis is provided to evaluate a series of actions which is a recipe or workflow. With individual
action-value functions established, we can then investigate how different sequences of actions  will
affect a given dataset. By gaining an understanding of when, or when not, to apply an action results in a flexible
system that can choose the ideal processing step based on the data rather than following a hard-coded pipeline.
Another benefit of Reinforcement Learning is that evaluating actions and estimating action-value functions can be
automated. As more data becomes available these action-value functions and processing workflows can be refined, which
ultimately shape the decision process. This same concept of iterative improvement is currently used by development
teams  but on a much slower time scale. For example, the NRAO pipelines have taken
We speculate that a flexible data-driven approach, such as RL, may provide a way to get science-ready data products
and large scale automation. The following section introduces a fundamental demonstration with a particular decision
in the calibration stage routinely used in processing. We illustrate a way to find the optimal processing action based
on the data itself rather than following hard-coded directions. This is extended to finding the optimal processing
workflow for the sequence of multiple actions. The scope of our proof-of concept demonstration is limited to a highly
simplified analysis scenario, designed to allow unambiguous verification of our algorithms while also applying all the
core machine learning concepts we will need for a more complex data analysis sequence.
Other work involving Reinforcement Learning and Radio astronomy calibration is done in .
Here, the authors are using different reinforcement learning methods to determine a regularization hyper-
parameter which controls the smoothness of their correction models for calibration. In their application the agent is
provided with time series data to learn from and make recommendations for  regularization factor for
the remaining time series of data. In addition to using different reinforcement learning methods, the most noticeable
difference is  use Reinforcement Learning to learn a hyper-parameter that is given to a
pipeline. In contrast, our work is focused on assembling the ideal pipeline for any given dataset, not tuning parameters
that are given to a pipeline.
In that regard, ’s work is closer to our area of research. In this paper the authors propose
a data processing system that assembles processing tasks into workflows for different use cases based on pre-existing
flowcharts defined by scientists. An end user only needs to select the use-case and the EDPS system navigates the
flowchart to assemble the series of tasks needed to process the data into a workflow. In contrast, our research is
focused on discovering the decision tree from which flowcharts can be created. Conceptually, the output of our system
could follow to assemble a workflow in real time. An interesting connection for potential future research would be to
model our output in a form that may be more directly usable by such systems.
Other instances of Reinforcement Learning and Q-learning applied to astronomy include telescope scheduling for
observations .
For our study we use simulated data so we may control an emitting source’s spatial structure as well as the properties
of the corrupting gain signal. This allows us to constrain the complexity  of
the problem to a simpler regime which requires a very limited set of calibration and imaging options. For simulated
data, the hierarchy of organization is as follows: the fundamental data component is the visibility amplitude which is
the amplitude of the interference function representing a given spatial frequency . In a typical
observation, many thousands of visibility amplitudes are measured from the many baselines between antennas as well
as across frequency channels and time. In practice this conglomerate of visibility amplitudes is called as a Measurement
Set but for our purposes we will refer to the generic name of dataset as we have simulated these quantities. We have
created thousands of these datasets which makes up a population and we have created two populations of datasets:
one population containing RFI contamination and one without. The reasons for making these populations will become
clear in the later sections.
At the individual dataset level, we simulated the visibility data by mimicking the VLA D-configuration array of
the observing setup to have 100 frequency channels and 100 time integrations.
The artificial sky that was being
flat spectrum. From this starting set of visibilities we introduced sinusoidal signals to represent direction-independent
antenna gain distortions that would need to be corrected for by calibration.
The first population of simulations consists of datasets with gain distortions that vary in mean value, amplitude,
and period for both frequency and time dimensions. Antenna-to-antenna offsets were also applied to be more realistic.
The noise level was kept constant across the population. For the second population we included RFI contamination in
addition to gain variations. We chose to include a specific type of RFI, broad antenna-based RFI, because this type of
RFI contamination will distort the calibration corrections if it’s not removed before calibration procedure takes place.
To be considered ”broad”, the RFI contamination covers multiple time integrations and frequency channels with very
high amplitudes. For each simulation in this population, the number of RFI contaminants, the amplitude of each
contaminant, and location in the time and frequency varied.
Figure 2. Visibility amplitudes plotted in 3D for one baseline. Other baselines in the dataset are similar in shape but vary
in intensity according to the antenna-to-antenna variations. X-Y axes form the time-frequency plane with visibility amplitude
on the z-axis . Left: A simulation without any gain distortions, which is a flat time-frequency plane
with noise. Center: Visibilities containing a large gain distortion that varies along the time axis that would require calibration
corrections for each timestep.
This simulation would benefit from averaging along frequency prior to calibration.
Right:
A simulation dominated by an RFI outlier that would need to be removed before estimating any potential underlying gain
distortions.
Figure 2 shows some example dataset visibility amplitudes. For each plot the z-axis is visibility amplitude and the
x and y axes correspond to the frequency and time dimensions. A perfect instrument, ignoring noise and atmospheric
effects, observing a flat-spectrum point source at the phase center would be a constant plane in time and frequency at
the mean visibility amplitude. In our case this would be 1 for the 1Jy source simulated . Any distortions
from antenna gains or outliers will result in some distorted shape in the plane .
To demonstrate this application we focus on a specific decision in the calibration stage, namely, the choice for the
amount of averaging to apply  before removal of the antenna gains introduced by the instrument. The purpose
of calibration is to estimate the gain distortions and remove them, leaving the time-frequency plane effectively flat,
meaning the visibilities measurements are unaffected by the instrument.
To make a data-driven decision we first need to define the context for making a decision. We start by deciding what
properties of the dataset are needed to adequately describe the state of the data, what actions are available to choose
from, and by defining the objective. The objective is used to form a metric to steer decisions.
For describing the state of the dataset we use four statistical properties measured across all baselines, referred to as
the state-vector.
indicator of a need for calibration. In this paper, we also refer to this quantity as ’mean gain’ because our
simulations use a 1 Jy point source.
We identified the four quantities listed above as the minimum required for the controlled environment of our ex-
periment. Our sky structure is a single point source at the phase center with all baselines measuring the same value,
and all deviations from a flat visibility function are due only to antenna gain effects. For our limited set of actions,
we only need to distinguish between time versus frequency variability in the data, mean visibility amplitude, and the
existence of outliers that may require flagging. The standard deviation values indicate if there are preferential axes
along which SNR may be improved by averaging. The presence of outliers would also be captured in the mean value,
standard deviation, and max value. For example, the state-vector for Fig. 2 Left is . For Fig. 2 Center
it is , and for Fig. 2 Right it is .
This simplification allows us to carefully validate our approach and unambiguously interpret our results. In subse-
quent work, when we include source structure, we will need more refined metrics to distinguish genuine baseline-to-
baseline variations from antenna gain effects. In a real-data environment, where there may be more effects to account
for, the dimensionality of the state vector will undoubtedly increase, possibly to include image statistics as well.
The actions represent the possible choices one can make in a decision. We selected actions that are relevant for the
decision scenario at hand, which are typical to what an end-user would choose from. The actions we made available
to choose from are the following:
across frequency but has variation in the time axis. Averaging the frequency axis increases SNR for the more
variable time axis which can improve the calibration corrections. The calibration runtime is reduced by averaging
one of the axes.
time but has variation in the frequency axis.
Averaging the time axis increases SNR for the more variable
frequency axis which improves the calibration corrections. The calibration runtime is reduced by averaging one
of the axes.
has minimal variation in both time and frequency and averaging both axes would have negligible impact on
calibration quality. The calibration runtime is further reduced from averaging both axes.
This is best suited when a dataset has variation in both time and
frequency. No averaging is applied as any averaging would result in poor approximations and therefore poor
calibration corrections. This is the most compute intensive as the data size is not reduced from averaging.
removes outlier data based on sliding window measurements. This takes a static runtime.
The actions described here apply to all baselines in the given dataset, not a single baseline. The actions described here
are implemented using the CASA software . The CASA software allows for one to calculate
the corrections without having to apply them unless explicitly directed with another function. For convenience, we
have combined the creation and application of corrections here with Actions 1-4.
Our objective is to remove any effect on the visibility data from the antenna gains and outliers. How we measure
any unwanted contaminants is by looking at the distribution of residuals after model reconstruction of the image. The
ideal residuals in the image domain are a noise-like Gaussian distribution at the theoretical noise level . This forms part of the metric we use to measure how “good” an action is. Our metric is based on two
factors:
Incomplete or improperly estimated calibration corrections and unflagged outliers can change raise the residuals
above what is theoretically possible.
associated with actions; something a human intuitively does but is not quantified in current pipeline heuristics.
The distribution of image-domain residuals is used to evaluate the quality of actions that were applied. To compare
the two distributions, a Savitzky-Golay  filter is applied to a finely binned histogram of the
pixel amplitudes in the residual image as well as the known theoretical distribution. This filtering results in a smoothed
histogram that is less sensitive to noise and represents the distribution shape. The smoothed values at each bin are
compared between the two distributions and a Wasserstein metric, or Earth Mover’s Distance  , is calculated .
This metric is sensitive to distribution shape and its distance from ideal. It responds well to the presence of imaging
artifacts and other error patterns as well as differences in Gaussian random noise levels. It is also agnostic of the source
structure. For these reasons, we chose this metric over typically-used metrics of standard deviation of the residuals or
image fidelity. Figure 4 shows an example of a simulation with each of the actions applied and their respective results.
Figure 3. Figure: An example comparison between theoretical noise distribution  and a model’s residuals distribution
while the solid magenta and green lines represent the distributions after the Savitzky-Golay filter is applied. The blue highlights
the difference between the smoothed distributions from which the EMD is calculated, indicating correctness .
The vertical dashed line indicates where the 0 point is; theoretical noise is centered around 0.
The runtime of the calibration step is estimated as being proportional to the number of elements presented to the
calibration solver, where an element is a distinct time-frequency portion. The calibration solver performs a non-linear
least squares calculation on each element. The time it takes to compute corrections changes based on the amount of
averaging applied to the input data as that changes the number of elements presented to the calibration solver. Note
that while averaging may be used on the input data for the solver, it does not change the original time-frequency plane.
If the data are averaged, the shape is changed for the input data to the calibration solver but the resulting corrections
are applied to each element in the original time-frequency plane. For example, the 100 channels and 100 integrations
is a 10,000 element time-frequency plane . If we choose to average one axis, the data shape changes for
what is given to the solver  but the corrections are applied to the original 10,000 element plane .
We use a runtime approximation based on the time it takes to complete a series of additions versus completing an
iterative non-linear solver for an arbitrary size. From that we assume non-linear solving is approximately 80x slower
than averaging data. The runtime is calculated as:
Therefore, based on the size of our datasets and the approximation used, the runtime cost of each action is as follows:
These values are intentionally hypothetical to exaggerate the runtime differences between actions to test how our
proposed solution would take these values into account.
The EMD and runtime values for an action are summed to give a single numerical measure of performance to represent
how good  that action was to take on the given dataset. This summed value is the action-value. We can
evaluate each individual action this way on a given dataset. The longer the runtime is the higher the runtime penalty
and the higher the EMD value is the higher the image quality penalty, which combine to form a high action-value.
Therefore in our scenario, the action with the lowest action-value is considered the optimal action.
With the EMD value typically being much smaller than the runtime value, the raw runtime value dominates the
result and therefore we need a way to equalize the scales of the two values. In our implementation, a scalar of 1e6
is applied to the raw EMD value for comparative scaling. In general, this scalar provides a convenient way to give
preference to one factor over the other, such as prioritizing runtime over image quality in the action-value statistic.
With the datasets simulated and the decision environment defined , we can now evaluate how different actions perform on a given dataset . In the case of a single decision
we select the optimal action based on how well the action performs towards the desired objective, as measured by our
metric. Figure 4 shows an example dataset that contains gain distortions along the time axis of the left hand side.
This is an example where there is sufficient signal-to-noise to perform accurate calibration on each time and frequency
sample, but we expect that averaging along frequency before calibration will provide similarly good numerical accuracy
at a much lower runtime cost.
Each action is independently applied to this dataset and evaluated. From this diagram it is easy to see there are
two actions, row 1 and 4, that give us the desired result of removing the gain distortion from the time-frequency
plane. The ”incorrect” actions in this scenario, rows 2, 3, 5, and 6, result in much worse residual RMS levels at an
average of 3e-2 Jy/beam with structure still present in the residual image. Row 1 and 4 correspond to Actions 1 and
residuals values between these two actions are similar with RMS levels at 1e-5 Jy/beam and would result in a tie if
only considering imaging performance. However, Action 4 takes 44x longer to finish than Action 1. This demonstrates
why we consider runtime in our action-value metric. With our action-value metric taking into account runtime and
image quality, Action 1 is the clear choice.
While Fig 4 is one example, with a small number of actions and a finite number of simulated datasets, it is feasible
to maintain a table that contains the action value for each state-action pair. This table forms the basis for tabular
methods in Reinforcement Learning. Tabular methods are simple yet foundational to understanding the more ad-
vanced Reinforcement Learning methods. We use the table of known values, a Q-table, as a basis for Q-learning, a
Reinforcement Learning method that learns the optimal action to take in a given state  .
Q-learning is necessary for learning non-greedy behavior, especially if an action’s effects only manifest later in the
decision chain. We intentionally created a population of simulations contaminated with RFI to test solving for the
optimal sequence of actions. RFI introduces a sequencing problem of not only if but when to flag data in relation to
other actions. We want to test whether the correct sequence of actions can be found to get to the global minimum.
Action:
Avg-Freq & 
Action:
Avg-Time & 
Action:
Avg-Both & 
Action:
Do Nothing
Action:
Flag Outliers
Action:
Avg-None & 
Starting Simulation 
Time-Freq Plane
Resulting Time-Freq 
Resulting Residual Image
Resulting Residuals 
Figure 4. Leftmost: Plotted visibilities of a simulated dataset containing gain distortions. Branching from that simulation is
each possible action and the results each action has on the data. Each row of images associated with each action  are the resulting time-frequency plane, the image residuals, and the residuals distribution compared to the theoretical noise
distribution. Color scales and X-Y axes values are synchronized on all plots to make relative visual comparison easy. Image
residuals that look noise-like, and residual distributions that look Gaussian, indicate better numerical outcomes such as row 1
and 4. Rows 1 and 4 have RMS levels of 1e-5Jy/beam whereas rows 2, 3, 5, and 6 have an average of 3e-2Jy/beam.
This section will cover the validation of our decision environment and how well Q-learning was able to find the
optimal sequence of actions. Population 1 datasets were used to evaluate how well our selected input features and
action value metric were at capturing what a domain expert would do faced with same decision. This validation allowed
us to use the same environment setup in more complicated scenarios for Q-learning. Population 2 datasets required
solving a sequence of actions; we will review the performance of the Q-learning at finding the optimal sequence of
actions based on data characteristics themselves .
To help convey ideal actions across an entire population of datasets we construct a visual aid in the form of a
simulations. The state space allows us to plot entire populations of datasets and view their different properties along
with the ideal actions to take on them and the subsequent action regions that are formed. Figure 5 is an example
of a 3D state space with different frequency variations, time variations, and mean gains. By colorizing the plotted
simulations with their best-performing action we can see which actions occupy different regions of state space.
Population 1 was designed that each dataset only needed 1 action applied to bring it into an ideal state.
population contains several thousand datasets, each differing in frequency variation, time variation, mean gain, and
consequently max value. For each dataset every action was independently applied to determine its action value. Figure
Figure 5. 3D state-space with thousands of simulations colorized by best the performing action. State space axes are frequency
variation, time variation, and mean gain. The individual points are simulations with those statistical properties. The points are
colorized by the best performing action, as determined by our metric. We can see regions in the state space that correspond to
ideal actions. Flag action will come later.
Figure 5 shows there are clear and distinct regions within the space. For example, consider all the blue points:
these simulations have a low gain distortion along the time axis but a high enough mean gain to require calibration.
The optimal choice there is to calibrate after averaging in time. Conversely, the red points have low gain distortion
along the frequency axis but a high enough mean to require calibration. The optimal choice there is to calibrate after
averaging in frequency. The purple points indicate these simulations can have both axes averaged before calibration
without degrading calibration corrections. On the other hand, orange points indicate simulations that should not have
any averaging applied before calibration. Any green dots represent simulations that don’t require calibration  and would reside at the 0 point of all 3 axes. This would represent when there is little-to-no variation and the
mean gain is close to the expected value.
Datasets with similar statistical properties are found to have the same optimal action needed to rectify them, creating
the colored regions in Figure 5. This is the most basic validation of the input features, metrics, and objective function
in our simplified environment. The regions associated with a particular action match what a domain expert would do
in the same scenario, validating our environment and metric. This gave us confidence to build more complex decision
scenarios requiring multiple actions to be tested with the same environment setup.
We created Population 2 datasets by adding strong outliers to the existing datasets in Population 1. These outliers
simulate broadband, antenna-based RFI corruptions in an observation . With these outliers, we now have
simulations that warrant the flagging action. However, because these corruptions are placed on top of the existing
gain patterns, both the RFI and gain patterns need to be corrected for. Importantly, this type of RFI will distort the
calibration solutions if they are not removed before calibration takes place. This introduces a new problem to solve
which is finding the optimal action sequence.
For a simulation that has gain distortions and RFI at the levels we have chosen to add, a greedy algorithm will opt
to do gain corrections first since that has an immediate stronger effect on image residuals than flagging RFI. This is
because the gain distortions affect more of the time-frequency elements than sparse RFI, contributing more weight
to the statistical properties of the dataset. However, RFI distorts the gain corrections resulting in a sub-optimal
calibration. This makes the sequence of actions critical with Flagging needing to be run before gain corrections . A greedy algorithm will not select the ideal sequence of actions and will not reach the global minimum.
As described earlier, Q-learning is a way to learn a non-greedy approach for sequencing actions.
Q-learning iteratively updates the action value in a given state-action pair as new combinations of states and actions
are explored. In our scenario for Population 2 there are two decisions  to be made with six actions possible for
each step, totalling 36 possible permutations  of actions. For example, step 1 could be Flag and
step 2 could be Avg-Freq and calibrate. As different pathways are explored the action values for any single action is
updated based on final outcome, known as the total reward. The total reward is a value composed of the cumulative
runtime of all actions in the pathway and the final image quality; intermediate image results are not considered. The
total reward is then used to update the individual action values of the actions making up the pathway. This way the
actions are affected by the final results they create, good or bad.
An example of the evolution of action value estimates for the first action to take on a dataset with RFI can be
seen in Fig. 6. As the Q-learning algorithm explores different action sequences the final reward value influences the
preceding action’s value. With enough iterations to sample different sequences of actions, the values for each action
converge, revealing the optimal action to take in the first decision .
For comparison, the Q-learning method was applied to the same dataset once the RFI was removed. Figure 7 gives
an example of the action-value evolution in this state. Q-learning found Flagging was no longer a rewarding action to
take but to Avg-Freq and calibrate directly.
Figure 6. Action value evolution for a dataset with RFI contamination. After a few iterations of updates to the action-values
the ‘Flag’ action rapidly becomes the most rewarding action to take.
Figure 7. Action value evolution for a dataset without RFI contamination. ‘Flag’ is not the ideal action to take but Avg-Freq
and Calibrate.
We applied the Q-learning algorithm to each dataset in Population 2. Plotting each of these simulations in a 3D
state-space, colorized by the optimal action to take gives Figure 8.
Note that since these datasets required two actions, when the first action was applied to the dataset it changed
its statistical properties resulting in a new state of the data. The resulting state of the data after the first action is
also plotted in the state-space resulting in more points. These are intentionally plotted so the original regions from
Population 1 can be recognized. The yellow ”point cloud” is comprised of the same datasets as Population 1 but
Figure 8. 3D state-space of Population 2 colorized by best the performing action.
contaminated with RFI. Q-learning has estimated the Flag action to be the optimal action to take first; if this was a
greedy approach the actions and therefore colors would be different.
It is important to highlight that although we have only applied Q-learning to two decisions here, this method can
be applied to a sequence of any length.
While the Population 2 datasets provide discrete data points across a wide range of values in the state-space, it
would be beneficial to learn a continuous function that provides an optimal action at any location in the state-space
so that we can create a direct map from an input dataset’s characteristics  and the optimal action.
To that end, we chose two different methods to model the action regions in the state-space, a simple neural network
and a decision tree classifier .
Neural Networks  are able to model any continuous function to an arbitrary precision ) and have become a standard tool for such applications. In short, the fundamental
building blocks of a feedforward NN can be thought of as a collection of piecewise elements that can be 
combined to replicate more complex functions. The contribution of any individual building block to the overall function
is determined by a weight which is optimized through standard optimization techniques. We chose to test NN for their
ability to handle high dimensional inputs and modeling non-linear functions, which will become typical when applied
to more complex decision environments and real data.
The input data was the Population 2 datasets with RFI. The input features were the same four statistical measure-
ments  and the ideal action was the output. We trained
the NN on a subset of the data and used the remaining unseen datasets to validate the NN model. Since there is
inherent variability in NN modeling from the weight initialization and  stochastic gradient descent, one
is unlikely able to recreate NN models with the same prediction accuracy. To that end, we trained 10 models from
scratch and took the average value, measuring 99.6% in prediction accuracy, as shown in Figure 9. An example of
one of the models action predictions given the state vectors for all the simulations in Population 2 shown in Figure 10.
NNs are powerful at modeling continuous functions especially as complexity grows. However, interpretability of
what the NN is learning becomes more difficult. We reserve applying and developing interpretable methods for future
work.
Figure 9. 10 independently instantiated NN models with prediction accuracy of each model as a blue dot. The red line is the
Figure 10. Trained NN model at predicting the ideal action for each dataset in the population. Correct predictions in green,
incorrect predictions in red.
A decision tree classifier  is a simple, easy to understand way to find regions of the state space associated with
different ideal actions. Crucially, the DTC can give the explicit rules behind its regions which gives interpretability to
the method and makes it simple for checking alignment with domain experts.
We fit a DTC to the ideal actions for Population 2 datasets to extract the numerical rules for the different action
regions. While the 3D state-spaces only visualize 3 axes of the data at once , the DTC has access to all
the input features of the data for deciding where to split the data. Figure 11 shows the model found by the DTC.
The DTC model gives us interpretability to the action-regions and shows alignment with domain expertise. In each
of the leaf nodes of the tree we see the unique actions and the DTC’s ability to find numerical boundaries that partition
the data. A learned DTC can be used to process an unseen dataset simply by using the statistical properties of the
dataset  and following the decision tree’s workflow for each step.
Figure 11. A decision tree and its rules based on the state-space. The DTC model gives us interpretability to the boundaries
of the different action-regions and shows alignment with domain expertise. In each of the leaf nodes of the tree we see the
unique actions and the DTC’s ability to find numerical boundaries that best partitions the data.
Software evolution is the continual development of software after its initial release to address problems or changing
requirements. Astronomy software, and CASA in particular, is not immune to this. Different software tools and
algorithms are necessary for handling different use-cases, observing modes, hardware changes, telescope designs and
so on. In some cases, different software tools can be used to accomplish the same goal but with differing accuracies
and efficiencies. If a new software tool is created, or a fix is made to an existing one, there is a practical need to
try and evaluate drop-in replacements. The RL approach outlined here, estimating action-values and finding optimal
pathways, works regardless of whether the tools change. It finds the optimal processing path with the tools it is given.
This is demonstrated by using the two different automatic flagging algorithms that exist in CASA. These automatic
flaggers have their respective use cases and were designed for as such, meaning their accuracies and efficiencies differ.
We used them with their default parameter settings to highlight the differences between them. The RFI we added to
the datasets is easily identified and removed by one algorithm  and not the other . We tested all
the contaminated datasets with both of the R-Flag and TF-Crop flagging algorithms as the Flag action. Since the
performance of each flagging algorithm is different, the optimal pathways found for each dataset are different. This
should be of no surprise, if the tool’s performance is different than the resulting data states are different, potentially
leading to different actions downstream .
The top panel is the TF-Crop algorithm and the bottom panel is the R-Flag algorithm. Both algorithms start with
the same initial dataset  and the resulting time-frequency plane after flagging is applied is shown in the
middle column. You can see for the TF-Crop algorithm, set with default parameters that are known to not be robust
to broadband RFI, there are outliers still remaining after flagging. These leftovers continue to contaminate the data.
For R-Flag, the algorithm is able to correctly identify and remove all outliers, therefore exposing the gain distortions
that are present. The rightmost column shows the 3D state space of all the contaminated simulations colorized by
their ideal action. For TF-Crop the leftover outliers caused data states to exist that could not be rectified by flagging
alone. As flagging would not improve the dataset any further, the best action was to calibrate without any averaging to
get the dataset as-close-to-ideal-as-possible given the tools available. An example is shown in the next section. When
the population of datasets with RFI was processed with R-Flag the action-regions in the state space were much more
apparent and organized. This indicates R-Flag working accurately to remove this type of RFI on the first attempt.
Starting Visibilities 
Visibilities after Flagging
Population State-Space
R-Flag Flagging Algorithm:
TF-Crop Flagging Algorithm:
Population State-Space
Visibilities after Flagging
Starting Visibilities 
Figure 12. Comparing performance of two different automatic flagging algorithms available in CASA. The top panel is the
TF-Crop algorithm and the bottom panel is the R-Flag algorithm. Both algorithms start with the same initial dataset 
and the resulting Time-Frequency plane after flagging is applied is shown in the middle column. The right column shows the
resulting 3D state-space of all datasets contaminated with RFI when processed with the given algorithm. Distinct regions in
the state space indicate better performance at the given task.
Learning when, or when not, to use a certain tool in a situation is the subject of training workshops for end-users
and the work of scientific developers in observatories. RL automates the heuristic discovery of what tool to use and
when, removing human overhead. Secondly, the action-values themselves can be a powerful metric for investigating
the performance of the different algorithms and software tools available.
As seen in Figure 12 when R-Flag was applied to Population 2 it was able to detect and completely remove outliers.
On the other hand, when using the TF-Crop algorithm, there were some simulations that had RFI contamination
leftovers. Those datasets with leftovers provided us a situation to demonstrate Q-learning’s adaptable data-driven
capabilities and repeating flagging as necessary.
Here we highlight one dataset in particular that has high- and low-amplitude RFI contamination in addition to
gain distortions. This is a specific example of TF-Crop flagging the high-amplitude RFI that would affect calibration
solutions, but not catching the low-amplitude RFI whose effect would show up only after calibration and still produce
image artifacts. We did this to test if Q-learning could learn to repeat an action if and when needed.
Figure 13’s columns  show the visibilities amplitudes in the time-frequency plane, the corresponding 
image plane, and the residuals distribution. The top row is a dataset contaminated with RFI. The time-frequency
plane shown in this row shows the max values across all baselines to illustrate the amount of RFI present in total.
Flagging with the TF-Crop algorithm is then applied and the results are shown in the second row. This time-frequency
plane is showing a single baseline only, particularly one that is still contaminated with RFI that was missed by the
TF-Crop algorithm at its default settings. Since the amplitude of the remaining RFI was ”low enough” the dominant
feature from the state vector was now the variations from the antenna gains. Q-learning found the next best action to
take was to calibrate . The calibration correctly flattened the wavy gain
distortion in the time frequency plane but the dataset is still plagued by outliers . Q-learning determined the next best action
was now to flag again because the removal of the gain distortions changed the statistical properties of the dataset
allowing the low-level RFI to be detected and removed.
This is a contrived example to show the dynamic data-driven decision making ability . Without specifying, Q-learning was able to learn staggered
flagging actions to remove all the adverse affects.
This demonstrates the data-driven capabilities of Q-learning.
Flagging multiple times without being explicitly programmed to do so exhibits flexibility beyond what a current
pipeline can do. In current operations, this is a situation that requires a human to fix.
Visibility Amplitudes
Residual Images
Residual Distributions
Flagging results:
Starting state: 
Calibration results:
Flagging results:
Figure 13. Demonstrating Q-Learning’s ability to take actions based on the contents of the data. Columns  show the
visibilities amplitudes in the time-frequency plane, the corresponding  image plane, and the residuals distribution.
The progression from top to bottom is the different actions applied to the data. The decision to flag a second time based on
the results of calibration is data driven and not from prescribed instruction.
Currently the standard way to process data is with a predefined, rigid, sequence of processing steps that is designed
for the average  use-case. For the case of ALMA and VLA pipelines, the sequence and steps have been slowly
shaped over 10+ years of heuristic development . In contrast, Reinforcement Learning methods
learn functions for mapping situations to actions so as to maximize a numerical reward signal. These methods are
driven by the data characteristics themselves, not some predefined recipe. The benefits of this approach would be to
achieve large scale automation while simultaneously achieving science-ready quality of the data products.
RL methods provide a true data-driven approach to these problems with flexibility to adapt to the needs of the data.
This flexibility provides an advantage that can be used for improving data quality or increasing resource efficiency,
with some potential for achieving both. For example, if a dataset does not contain RFI that needs to be removed then
no flagging action is taken, saving time and computing resources. If a dataset contains multiple levels of RFI, the
system is capable of deciding whether or not to flag multiple times to achieve a better image. This also has potential to
reduce human overhead in the entire end-to-end processing for data .
The boundaries of the action regions can be thought of as heuristics. Traditionally, heuristic search is something
that teams of scientists and developers spend time researching and testing for traditional pipelines. Through simulated
data and experimenting with different actions these regions and their boundaries are discovered. This moves the time-
intensive task for humans to the machine. In our tests, we showed the decision regions are not only found but were
clearly separable, quantifiable, interpretable, and aligned with domain expertise. We demonstrated RL is capable of
handling single decisions and sequences of decisions in a non-greedy way. The sequences of decisions can be expanded
to include any size decision chain. We intentionally chose a simplified environment to test this but it served as a proof
of concept to warrant application to more complex data and decision scenarios.
Measuring how well a given processing action  does on a given datasets gives us information on the performance
of the tool. This measured quantity can give us feedback for how well the tool is achieving its purpose. This can be
insightful for investigating a tool’s performance or efficiency in certain situations.
The scope of this research was intentionally limited to a simplified subset of a typical data reduction workflow and
environment to provide a proof of concept with unambiguous validation. This impacted the complexity of the datasets,
the number of features used to estimate the state, the actions available to choose from, the amount of parameter tuning
for each action, and the number of steps needed to get to an optimal state. Each of these constraints will need to be
relaxed as we take this research forward to real data.
In addition, the size of the datasets was kept small along with the number of actions and steps needed for multiple
reasons. Not only was this done for clear interpretation but this kept the computing requirements to a reasonable
size. With only two action steps and 6 available actions to choose from, each of the
processed 36 different ways. We brute-forced every single pathway to have the ground truth to validate our results.
As we move towards real data, more action choices and longer sequences of steps will be needed to process the data.
As such, brute-forcing every possible pathway will become less reasonable. We will need to take advantage of more
sophisticated RL methods beyond Q-learning.
With more complex data and longer processing sequences there is likely to be more complex correlations in high-
dimensional space. The level of interpretability demonstrated here will become more challenging to extract but this
provides an area for doing cutting-edge research.
We have built this prototype to demonstrate the basics of applying RL to this problem and validate the results
upon which to build more complex scenarios that solve real world problems. To that end we are expanding the scope
of the environment as well as the RL methods used.
For the environment, we will increase the number of input
features, actions available, and stages of processing. Most notably will be the inclusion of the imaging stage. This
stage will have a few different deconvolution algorithms to pick from. With the inclusion of the imaging stage and a
few default deconvolution algorithm choices , we anticipate RL methods will
be able to demonstrate ‘self-calibration’ functionality . Additionally, we will build on
the foundational RL methods used here towards more complex Reinforcement Learning methods such as Deep-Q-
Networks and its derivatives, which combine neural networks with Q-learning. Increasing the scope and complexity
of the problems needing solving will undoubtedly require additional computing requirements and therefore require
thoughtful ways to keep training expenses manageable. While the path ahead is uncertain and difficult we believe the
end result will far outweigh the cost.","[('action', 175), ('data', 100), ('time', 59), ('calibration', 52), ('learning', 46), ('dataset', 46), ('frequency', 46), ('rfi', 44), ('state', 44), ('gain', 40)]","[(('time', 'frequency'), 24), (('state', 'space'), 21), (('gain', 'distortion'), 19), (('reinforcement', 'learning'), 16), (('frequency', 'plane'), 15), (('optimal', 'action'), 12), (('visibility', 'amplitude'), 12), (('data', 'driven'), 11), (('antenna', 'gain'), 10), (('population', 'datasets'), 10)]","[(('time', 'frequency', 'plane'), 15), (('reinforcement', 'learning', 'methods'), 6), (('regions', 'state', 'space'), 6), (('tf', 'crop', 'algorithm'), 6), (('subject', 'training', 'workshops'), 3), (('data', 'driven', 'decision'), 3), (('antenna', 'gains', 'outliers'), 3), (('action', 'state', 'action'), 3), (('state', 'action', 'pair'), 3), (('frequency', 'variation', 'time'), 3)]"
2410.17142v1.txt,"Coniferest: a complete active anomaly detection
M. V. Kornilov1,2⋆, V. S. Korolev3, K. L. Malanchev4,5, A. D. Lavrukhina1,6,
E. Russeil7, T. A. Semenikhin1,6, E. Gangler8, E. E. O. Ishida8,
M. V. Pruzhinskaya8, A. A. Volnova7, S. Sreejith9
Universitetsky pr. 13, Moscow, 119234, Russia
Basmannaya Ulitsa, Moscow, 105066, Russia
Carnegie Mellon University, Pittsburgh, PA 15213, USA
Green Street, Urbana, IL 61801, USA
bld. 52, Moscow 119234, Russia
Profsoyuznaya Street, Moscow, 117997, Russia
UK.
Abstract. We present coniferest, an open source generic purpose ac-
tive anomaly detection framework written in Python. The package design
and implemented algorithms are described. Currently, static outlier de-
tection analysis is supported via the Isolation forest algorithm. Moreover,
Active Anomaly Discovery  and Pineforest algorithms are avail-
able to tackle active anomaly detection problems. The algorithms and
package performance are evaluated on a series of synthetic datasets. We
also describe a few success cases which resulted from applying the pack-
age to real astronomical data in active anomaly detection tasks within
the SNAD project.
Keywords: Machine learning · Active learning · Anomaly detection.
The ultimate goal of every machine learning  algorithm is to extract in-
formation present from large data sets, and in the process, optimise the allo-
cation of human resources devote to it. Ultimately, improving human life. In
arXiv:2410.17142v1    22 Oct 2024
M. V. Kornilov et al.
the context of anomaly detection  strategies this translates into enabling
discoveries which would never happen otherwise. Depending on the considered
field, it means that we can discover hidden or upcoming hardware failures, rare
software bugs, malicious or fraud activity in software systems, or even new and
completely unforeseen astrophysical objects. All these cases have implications
either on our understanding of the Universe as a whole, or on crucial aspects of
daily life.
In AD tasks, it is usually assumed that the dataset includes rare objects
hypothesis that all elements in the data set were generate by the same underlying
mechanism. For example, it is often assumed that anomalies are a minority
consisting of a few instances, and that they have attribute values that are very
different from those of nominal instances in . In this context, such objects can
be caught by one of the many outlier detection algorithms available. However, in
the context of scientific research, both assumptions are not fully correct. First,
being rare is not enough attribute to consider an astrophysical source valuable
or scientifically interesting. When outliers are inspected by a human expert,
it often happens that most of them are statistically rare but not useful data
samples – since the final expert request is not to find samples in low density
parts of the feature space but to solve the real-world problems .
Second, sometimes having attribute values slightly above a certain threshold is
enough to be considered a scientifically interesting anomaly. For instance, if we
imagine a white dwarf with a mass greater than the Chandrasekhar limit, it
becomes clear that the mass only needs to be significantly greater within the
measurement error, rather than an order of magnitude greater.
However, identifying such cases is far from a trivial task. Given the volume
and complexity of modern data sets, the expert is usually able to inspect a very
small subset of the objects identified as anomalous by traditional algorithms.
Indeed, inspecting the hardware or checking software behaviour are extremely
time consuming tasks, not to mention the specific case of astronomy, where
additional measurements may be required in order to make a final decision.
Thus, algorithms able to fully, and optimally, exploit past expert decisions when
dealing with new and unlabeled data are required.
Active learning  algorithms constitute a set of learning strategies that
employ expert feedback to fine tune the learning model . In the context of
AD, one possible approach is to sequentially show to the expert the object with
highest anomaly score and using the feedback to update the hyperparameters of
the learning model . The ill-posed nature of the AD problem coupled with the
AL strategy results in personalized models which are trained to identify, within
a large data set, the specific type of anomaly that is interesting to the expert.
While performing research in this field, the SNAD10 team noticed still an-
other practical issue preventing the wide spread use of AL strategies in as-
Coniferest: a complete active anomaly detection framework
tronomy: the lack of AL algorithms in popular scientific software, like scikit-
learn11. Recent efforts in this direction are very sparse and do not completely
address these issues12,13. This motivated us to develop our own software frame-
work, as well as completely new algorithms which were specifically designed
to the application of active AD strategies in astronomy catalog data. This re-
sulted in a fully automatized environment and optimize algorithms which have
fulfilled our expectations throughout the last few years. We believe this tool
is now ready to benefit others, so we decided to provide the community with
coniferest, a ML framework created using our team deep expertise in the field
of active anomaly detection. The package accepts as input any rectangular data
matrix holding one line per object and one column per feature14. Despite our
main interest being on astronomical application, the entire framework is suitable
for a large range of scientific applications.
This paper is organized as follows. In Section 2 we present all currently
implemented algorithms employed in the package. In Section 3 we highlight
package design considerations and overall usage pattern. In Section 4 we provide
package evaluation and conclusions are given in Section 5.
Isolation forest
Isolation forest  is an outlier detection algorithm described in  and later
in . It is not an active anomaly detection method, however it is the base for
other tree-based algorithms implemented within the package. Each member of
the tree ensemble is a particular variant of extremely randomized trees called an
iTree  in the original paper .
Considering a binary tree, each node represents a split point and a feature , resulting into the feature space partitioning into non-overlapping
multidimensional rectangles. An arbitrary value can be assigned to every rect-
angle, for instance a class, for a classification problem, or a real value, for a
regression problem. Note that it is computationally inexpensive to find the cor-
responding rectangle for any input point in the feature space. Its assigned value
is then used as the tree prediction for that particular object. Depending on the
problem, the split value and the feature are evaluated for every node in some
optimal way to create the optimal feature space partitioning, providing good
predictions.
An ensemble of trees — called as forest — consists of a number of weak
predictors  each solving the same problem, being it a classification, a
regression problem, or an outlier isolation problem. Then, the predictions of
but documentation is sparse and it is not adapted to astronomical data.
M. V. Kornilov et al.
every tree are averaged, summed, or taken into account in another manner over
the whole forest, depending on the considered method. Exactly as Leo Tolstoy
writes, Happy families are all alike; every unhappy family is unhappy in its
own way , all trees in the forest provide a slightly wrong prediction in its
own way, but using the ensemble averaging allows us to improve prediction
quality dramatically. Usually, every tree needs to be additionally randomized by
subsampling the dataset or choosing a subset of considered features, otherwise
all trees would be wrong in a similar way and the ensemble averaging would not
help. Decision trees for classification via the random forest method make splits
in some local-optimal manner, trying to separate samples belonging to different
classes as much as possible. In case of AD, isolation trees choose completely
random order for both, feature and the split point. At every node a random
feature and a random split point, from the domain covered by the data, are
chosen. Empty  splits are prohibited, i.e. every single feature
space rectangle encloses at least one sample from the initial dataset. The single-
point rectangle cannot be split further and corresponds to a leaf of the tree. Then
it is fairly obvious that splitting single point into the separate region are more
likely when the point is distant from the bulk of the data in the feature space.
Therefore, the regions of feature space populated by outliers appear earlier, near
the root of the tree. It means that average leaf depths are smaller for the outliers
than those for the nominal data.
For every particular point in the feature space it is possible to evaluate the
leaf depth in every tree of the forest. Then, the leaf depths are averaged and used
as a measure of abnormality. Namely, the following anomality score s is used
within coniferest package, to be consistent with those used in scikit-learn:
s = −2−
c .
dataset size N following:
c ≡2  −1) ≈2
γ −1 + ln + 1
the constant c is essentially an average leaf depth for an isolation tree built for
uniformly distributed data. Moreover, ¯d is an ensemble-averaged leaf depth
for a given data point, x:
i=1
th tree covering point x, d being the leaf l depth, and N corresponding to the
number of samples from the dataset in leaf l. A popular optimization is employed
in our isolation forest implementation: anomaly scores for the nominal data are
Coniferest: a complete active anomaly detection framework
evaluated only approximately, since they are not as important as anomaly scores
for outliers. The tree depth is limited to log2, which allows to speedup the
process of building trees. The term c)) in Equation 3 a consequence of
such optimization.
We notice that s →−1 as ¯d →0, and s →+1 as ¯d →+∞,
which results in outliers having negative scores.
Active Anomaly Discovery
Active Anomaly Discovery  is a generic active learning technique proposed
by . Basically, AAD is not limited by forest algorithms such as Isolation
forest. It is also possible to apply AAD on top of any other outlier detection
algorithm. However, we briefly describe here only the forest flavour of AAD
approach, since it is the one currently implemented in the coniferest package.
To understand some motivation of this technique, let us note that our final
goal is to mimic the decisions provided by a human expert. If the expert could
label the entire dataset, we would have reformulated the problem as a binary
classification one. Then, the Random forest classifier or even Extremely ran-
domized trees could have been applied to solve the problem. The class  would have been assigned to every leaf of the classifier.
The above algorithm seems very similar to our previous description of IF,
except that a leaf depth, instead of a class, is assigned to every leaf of the forest.
Indeed, both IF and Extremely randomized trees, perform a partitioning of the
feature space. The only difference is in the values assigned to the leafs of each
tree. The key idea of AAD is to take IF as a starting point and then iteratively
adapt the leaf values to make the forest similar to a hypothetical Extremely
randomized tree classifier, trained to recognise 2 classes ,
given that the anomaly class is composed by objects that fulfill the expectations
of the expert. Considering that IF is a good enough starting point, we can expect
that the classifier state is potentially reachable using partially labeled data.
There are two possible sources of partially labeled data. First, we can op-
tionally provide the algorithm with the labels known in advance, e.g. given well
established catalogs or literature reports. Second, the human expert can be asked
to input decisions while the active learning loop is running.
For AAD, we have the following equation for the score, which replaces Equa-
tion :
s =
i=1
wi,liφ ) + c))) ,
formation function15 and wi,li are weights which can be adjusted, allowing
Equation  to better predict decisions reported by the human expert. There
φ = 1, or φ = d are also considered .
M. V. Kornilov et al.
are as many different weight parameters as there are leafs in the forest. For sim-
plicity, we also use notation wj further each j corresponds to a leaf in the forest.
Additionally, all weights were normalized, so ∥w∥2 = 1.
Considering A as a known anomaly subset of the data and N as a known
nominal subset, the optimization problem for estimating weights w is defined
as:
w = arg min
i∈A
ReLU  −qτ) +
i∈N
ReLU ) + 1
inals by the model. The second term is a loss due to known nominals wrongly
predicted as anomalies and the third term accounts for regularization. The con-
stant Ca allows us to assign a relative importance of to both kind of errors, our
current default choice being Ca = 1. The threshold score qτ is the limiting score
necessary to identify the 1 −τ percentile of the dataset. Our current default
choice is τ = 0.97. This means that all known anomalies should occupy the top
regularization term is either an uniform vector or the weights from the previous
iteration, while ReLU denotes the rectified linear function:
ReLU ≡
to high number of unknown parameters to be determined. Currently, we use
trust-krylov method from scipy optimization framework, which uses the New-
ton GLTR trust-region algorithm .
At each iteration, Equation  is solved and the scores for the entire dataset
are reevaluated using Equation  and the new weights w. Thus, resulting in
a new unknown object associated to the highest anomaly score and a new τ
score quantile. The most anomalous unknown object is shown to the human
expert. Depending on the expert decision, this object is added to either A or
N, and the next iteration occurs. We emphasize that it is crucial to perform the
model inference as fast as possible, since estimations for the entire dataset are
reevaluated at every iteration.
The total number of iterations, called budget, is an input chosen by the
expert, and should be determined taking into account the available time, as
well as other resources, necessary to properly judge the candidates presented by
the algorithm. Once the budget is exhausted, we can calculate one of the most
important metrics used to evaluate method effectiveness: tje number of expert-
approved anomalies within the budget. In case of simulated data, we can also
use convenient metrics, e.g. confusion matrices. However, this is not possible in a
Coniferest: a complete active anomaly detection framework
real data scenario since the user would not know how many interesting anomalies
are included in the entire data set.
Pineforest16 presents another approach of refining IF by incorporating feedback
from experts. Unlike the previous method, Pineforest does not directly modify
existing trees. Instead, it selectively discards trees that inadequately represent
the underlying data distribution.
In order to determine the suitability of each tree, a scoring mechanism is
devised based on the labeled data. We assign scores to the data points as follows:
yi =
if xi is labeled as an anomaly,
if xi is unlabeled,
if xi is labeled as a regular point.
For any given tree ti, we calculate its score as:
s =
j=1
yj · d),
where d) denotes the depth of the sample xj in tree ti.
Using these scores, we adapt the forest to the labeled data by discarding trees
with lower scores and retaining those with higher ones. This iterative process
involves building an initial set of trees, filtering out a certain percentage of them
based on their scores , and subsequently rebuilding the
forest with the remaining trees. This procedure can be repeated multiple times
for further refinement.
The scoring mechanism is designed to prioritize trees that accurately capture
regular points deep within the forest, while potentially isolating anomalies at
shallower depths. Following the acquisition of new data, the learning process can
be reiterated to incorporate the updated information, thus continually adapting
to evolving data distributions.
coniferest package
The coniferest package is an open source software, and we follow all best prac-
tices currently adopted for developing such tools. The source codes are available
at GitHub: https://github.com/snad-space/coniferest according the terms
of MIT license.
The package is mainly written in Python, since this language is de-facto
standard within contemporary academic machine learning communities. How-
ever, the performance-critical parts, such as tree search algorithms, required the
M. V. Kornilov et al.
use of native binary code. We choose to use Cython to write the performance-
critical code, which is latter compiled to native binary code. Cython is a famous
solution for such cases, for instance scikit-learn is heavily related on Cython.
It is also popular in the machine learning community, which simplifies possible
contributions from the community. Using Cython allows us to support parallel
and fast IF inference, which is required by the AAD algorithm. In alternative to
Cython, performance-critical code could be written in C++ or Rust.
Using native compiled binary code produced by Cython, C++, or Rust com-
plicates the package installation, particularly for non-experienced users. In or-
der to circumvent this issue, we provide pre-built python packages in the Wheel
format, distributed via the PYthon Package index, for all major platforms, in-
cluding Windows , MacOS , and Linux . It means that
pip install coniferest easily does the job for most users. The pre-built pack-
ages are compiled in the GitHub Actions environment automatically, which sim-
plifies package preparation and makes it more reproducible, reliable and secure
for the end user. GitHub Actions are also used to build the code and run unit
tests on different Python versions and operation systems. Employing unit testing
helps finding regressions and dramatically improves overall code quality.
There are only a few dependencies for coniferest. Obviously, numpy is on the
list, and we also currently re-use scikit-learn for building trees. The latter is
a temporarily solution, because currently we have to use internal scikit-learn
interface which is subject to change without notice. Using internal interfaces
of other libraries dramatically complicates development and distribution of the
package, since it requires carefully handling dependencies versions. Addition-
ally, we would like to support parallel  tree building in future
versions of our package, which may come in hand for Pineforest algorithm.
We emphasize that, while IF algorithm is implemented within scikit-learn
package, we had to re-implement it to enhance its performance. While training is
typically the most time-consuming aspect of most machine learning algorithms,
for IF and active learning strategies based upon it, scoring is the more demanding
process. This is because scoring, which is essential for anomaly detection, requires
the use of the entire dataset, whereas training often only uses a small subset of
the original data.
In order to provide useful documentation for the package, we use Python doc-
string, to describe functions and classes, and Markdown, for long read tutorials.
Both are rendered automatically by ReadTheDocs service and available online
at https://coniferest.snad.space.
Even though active anomaly detection usually assumes some active session
and the final product are filtered subsets of the input data, we also provide
support for serializing the trained models to portable ONNX format. This is
useful for re-using the trained models for automatic anomaly detection pipelines.
Under such circumstances, the model cannot be update further, but it can be
already satisfactory pre-trained by the expert to produce a reasonable stream of
data.
Coniferest: a complete active anomaly detection framework
Models and the Session
There are three basic kinds of entities in coniferest packages: datasets, mod-
els, and sessions. The datasets are used solely for testing and demonstrating
purposes. They are not required in production, since each user can analyze their
own data.
The models are scikit-learn-style classes, implementing every supported
algorithm: Isolation forest , Active Anomaly Discovery  and Pinefor-
est. These classes mimic the well-known scikit-learn interface with a few ex-
ceptions, because scikit-learn has poor support for partially-labeled data, and
consequently there was no good interface to follow. However, for the scikit-learn
user it will not be difficult to get acquainted with the coniferest interface.
The sessions are objects to support iterative training of the models, as it
is done in active anomaly detection. The idea behind the session is to create
glue layer between the human expert and the retraining of the model. Unlike
scikit-learn model class, the Session class constructor takes data and meta-
data at the object construction time. The data are ordinary two-dimensional
array of the features. The metadata are one-dimensional array of auxiliary data,
only required to help the human expert in distinguishing the objects inside main
dataset. In the simplest case the sample metadata are their row index, however,
a more description choice is also possible which would help identifying each
sample.
The session constructor also requires the existing model object to be created,
which can either be pre-trained or not. Currently, both AAD and Pineforest
models are supported.
The end user can modify the session behaviour by providing callbacks to
the object constructor. The callbacks are used to input the expert decision, to
visualize the learning process, to snapshot current model and to save results,
among other tasks. For instance, in our workflow, we supply the expert with a
SNAD viewer  URL, pointing to a portal containing extensive information
about the object under consideration. The code then expects a decision  to be input interactively.
An alternative to callbacks could have been to use an abstract class and
inheritance to tune desired session behaviour. However, we found that it could
be difficult for a normal data scientist to grasp the object-oriented concept. So,
we decided to use the functional approach.
Supplied datasets
In order to make the package self-consistent some toy datasets are also supplied
within the code. It is a modern good custom in the machine learning community
to provide the dataset within the code package in order to allow the user to
start exploring the framework immediately. Currently, we provide the following
datasets.
In documentation and tutorial one can find references to an astronomical
dataset ztf_m31. It is a sky field around the M31 galaxy from Zwicky Tran-
M. V. Kornilov et al.
sient Facility17   survey with features extracted as described in
For those who don’t like astronomy, we adopted datasets collected by  for
their work in neural network anomaly detection . The follow-
ing datasets are available: donors , census , fraud , celeba , backdoor , campaign ,
thyroid . For further detailed description, please
see .
There is also a debugging dataset called non_anomalous_outliers, this
dataset is generated by request and consists of nominal data and a required
fraction of outliers.
Isolation forest prediction
CPU wall time, seconds
sklearn 1.4.2
sklearn 1.3.2
sklearn 1.2.2
sklearn 1.1.3
coniferest 0.0.13
sklearn 1.4.2
coniferest 0.0.13
Intel Xeon Gold 6148 
Apple M3 Pro
Fig. 1.
Performance comparison between scikit-learn  and
coniferest. A dataset containing ≈106 samples, each having 2 features is consid-
ered. The dataset anomaly score evaluation has been measured for different versions
of scikit-learn. Additionally, coniferest in single-thread mode is considered. Note,
that scikit-learn is always single-threaded.
As we noted before, it is important to be able to quickly evaluate the model
for AAD and Pineforest. Unfortunately, at the time of the creation of coniferest,
IF prediction within scikit-learn was single-threaded and not quite optimized,
even in single-threaded mode. While scikit-learn performance has been suffi-
ciently improved later, it is still impossible to make multi-threading predictions.
There is two ways to make multi-threading prediction for IF: process each tree
Coniferest: a complete active anomaly detection framework
in a separate thread or process each data sample in a separate thread. We use
the latter approach. Performance comparison is shown in Figure 1.
Success cases
In , coniferest package has been employed within ZWAD pipeline  to per-
form anomaly detection for ZTF DR17 dataset. The human expert was happy
to find anything astrophysically exciting. For instance, a unknown binary mi-
crolensing event AT 2021uey has been found, an optical counterpart for radio
source NVSS J080730+755017, and lots of variable stars and fast transients,
such as red dwarf flares.
The same ZWAD pipeline has been used in . Instead of general anomaly
detection problem, the problem was stated as semi-supervised classification one.
This allowed us to discover 104 supernovae 
in ZTF DR3 dataset. The human expert was serching only for supernovae, and
answered to the active anomaly algorithm accordingly. About 2000 candidates
were inspected, and labeled, leading to the discovery of 100 supernovae. Similarly,
red dwarf flares were of interest in . ZTF DR8 dataset was used to find red
dwarf flare events and about 130 flares were found within 1200 candidates.
Note that instead of inspecting millions of light curves in ZTF dataset only
tiny subsets were scrutinized while looking for supernovae and red dwarf flares.
From astrophysical intuition we understand that it is unlikely to one would find
would mean that the entire Galaxy is consisted of supernovae which is obviously
not true.
We introduced the coniferest package, a multi-algorithm and open-source soft-
ware designed for anomaly detection, developed by the SNAD team. The soft-
ware, written in Python, supports a range of algorithms, including Isolation
Forest  for static anomaly detection, Active Anomaly Discovery  and
Pineforest for active anomaly detection. The framework’s integration of Cython
for performance-critical tasks ensures efficient parallel processing, enhancing the
speed and scalability of anomaly detection tasks. Coniferest’s design aligns with
modern machine learning practices, offering a user-friendly interface similar to
scikit-learn, and supports the serialization of trained models in ONNX format
for seamless integration into automated pipelines.
The successful application of coniferest algorithms for the Zwicky Tran-
sient Facility data highlights its effectiveness in identifying astrophysically signif-
icant events such as binary microlensing, optical counterparts for radio sources,
rare variable stars, and supernovae. This framework not only simplifies the im-
plementation of anomaly detection algorithms, but also bridges the gap between
data scientists and domain experts, fostering more effective and efficient iden-
tification of anomalies in large datasets. As we continue to develop and refine
M. V. Kornilov et al.
coniferest, we anticipate its broader adoption across different fields, facilitat-
ing more accurate and timely detection of anomalies in diverse datasets, which
will be particularly beneficial for the V. Rubin Observatory Legacy Survey of
Space and Time18.
Acknowledgments. M. Kornilov, A. Lavrukhina, A. Volnova and T. Semenikhin
acknowledges support from a Russian Science Foundation grant 24-22-00233, https:
LLC. for K. Malanchev.
Disclosure of Interests. The authors declare no conflicts of interest.","[('anomaly', 49), ('data', 42), ('tree', 36), ('algorithm', 32), ('detection', 29), ('coniferest', 28), ('active', 26), ('package', 25), ('expert', 25), ('forest', 23)]","[(('anomaly', 'detection'), 25), (('active', 'anomaly'), 19), (('scikit', 'learn'), 17), (('human', 'expert'), 9), (('feature', 'space'), 9), (('isolation', 'forest'), 8), (('detection', 'framework'), 7), (('machine', 'learning'), 7), (('leaf', 'depth'), 7), (('coniferest', 'package'), 7)]","[(('active', 'anomaly', 'detection'), 13), (('anomaly', 'detection', 'framework'), 7), (('coniferest', 'complete', 'active'), 6), (('complete', 'active', 'anomaly'), 6), (('active', 'anomaly', 'discovery'), 5), (('partially', 'labeled', 'data'), 3), (('ztf', 'dr', 'dataset'), 3), (('red', 'dwarf', 'flares'), 3), (('algorithm', 'active', 'anomaly'), 2), (('anomaly', 'discovery', 'pineforest'), 2)]"
2410.17163v1.txt,"Draft version October 23, 2024
Typeset using LATEX default style in AASTeX631
Towards a unified injection model of short-lived radioisotopes in N-body simulations of star-forming
Joseph W. Eatson,1 Richard J. Parker,1, ∗and Tim Lichtenberg2
The University of Sheffield
Hicks Building, Hounsfield Road
Sheffield, S3 7RH, UK
University of Groningen
P.O. Box 800, 9700 AV Groningen, NL
Recent research provides compelling evidence that the decay of short-lived radioisotopes , such
as 26Al, provided the bulk of energy for heating and desiccation of volatile-rich planetesimals in the
early Solar System. However, it remains unclear whether the early Solar System was highly enriched
relative to other planetary systems with similar formation characteristics. While the Solar System
possesses an elevated level of SLR enrichment compared to the interstellar medium, determining SLR
enrichment of individual protoplanetary disks observationally has not been performed and is markedly
more difficult. We use N-body simulations to estimate enrichment of SLRs in star-forming regions
through two likely important SLR sources, stellar winds from massive stars and supernovae. We vary
the number of stars and the radii of the star-forming regions and implement two models of stellar wind
SLR propagation for the radioisotopes 26Al and 60Fe. We find that for 26Al enrichment the Solar System
is at the upper end of the expected distribution, while for the more supernovae dependent isotope 60Fe
we find that the Solar System is comparatively very highly enriched. Furthermore, combined with our
previous research, these results suggest that the statistical role of 26Al-driven desiccation on exoplanet
bulk composition may be underestimated in typical interpretations of the low-mass exoplanet census,
and that 60Fe is even less influential as a source of heating than previously assumed.
Keywords: Star forming regions — N-body simulations — Planetesimals — Protoplanetary disks —
Planet formation
evolution of planets, in particular through their influence on planetesimal and protoplanet evolution . As these isotopes have a half-life commensurate with planetary formation timescales
planets around 1 Myr old stars,
Alves et al. 2020; Segura-Cox et al. 2020), they are present throughout the early planetesimal formation process,
concentrating onto planetesimals and debris as the disk agglomerates into a protoplanetary system .
SLRs are important for planetary formation as they provide a significant source of heating through radioactive decay
for nascent protoplanets and planetesimals.
This can increase the rate of chemical segregation within a forming
protoplanet , and is instrumental in the evaporation and removal of volatiles from planetesimals
through outgassing . Volatile outgassing is of particular
interest, as removal of H2O and other atmospheric volatiles from planetesimals prior to protoplanetary formation can
arXiv:2410.17163v1    22 Oct 2024
significantly impact the formation of extrasolar ocean worlds, and instead lead to relatively volatile-poor rocky worlds
instead .
It has been found that the Solar System has a significantly higher fraction of both 26Al and 60Fe than the interstellar
medium  , with an isotopic ratio 26Al/27Al  of ≈5×10−5 based on observations of decay
products in chondritic meteorites . The measurement of 60Fe is much more
controversial, though recent estimates  suggest an isotopic ratio 60Fe/56Fe  of ≈1×10−6,
which is also significantly higher than estimates of the abundance of this isotope in the ISM.
Some researchers have suggested that the parent giant molecular cloud  of the Solar System had a higher level
of SLR enrichment compared to the ISM, due to sequential star formation , whereas other
authors have proposed either internal production or delivery from an external source. Although 26Al can be produced
internally through cosmic ray spallation, this is insufficient and does not reflect the isotropic spatial distribution we
observe in the SLR distribution in the Solar System nor the fractionation between spallation and nucleosynthetic
processes . 60Fe cannot be produced via cosmic ray spallation .
Instead, one of the main proposed external delivery methods is through stellar winds, in which significant quantities
of 26Al are produced in the cores of massive stars , the convective core
circulates 26Al to the surrounding envelope, and is expelled as a part of the stellar wind. Whilst some 60Fe can be
emitted through massive stellar winds, this quantity is many orders of magnitude lower than the total wind mass of
through supernovae; however massive stars take upwards of 10 Myr to explode , by which
time disks are likely already far into producing protoplanets, preventing efficient isotope transfer from the outflow
to the planetary system before planetesimal formation . Finally, there
is additional evidence to suggest that a third category of enrichment is potentially important, which is the case of
the winds of “interloping” asymptotic giant branch  stars depositing SLRs into a recently formed star-forming
region . AGB stars wandering into star-forming regions have been
observed , however there has been limited study on the impact of such interlopers, or the
probability of this occurring with a star-forming region.
In this paper, we report the results of a series of N-body simulations of star-forming regions using AMUSE  where massive stellar winds and supernovae inject 26Al and 60Fe into protoplanetary disks surrounding
low-mass stars. The region size, population and density are varied significantly for each sub-set of simulations to
explore the star-forming region parameter space. This model utilises wind and supernova mass loss rates of SLRs from
work by Limongi & Chieffi , as well as statistical models of star-forming regions and stellar populations. Our
model aims to be more comprehensive than previous attempts to simulate SLR injection within star-forming regions
though there are a number of additional features and avenues of research that we will discuss in brief at the end of
the paper. The following section details the methodology of our work, in particular the programming framework and
implementation of various simulation features, Section 3 details our results, and Section 5 concludes and discusses
further research into this particular topic.
The simulation code used throughout this paper is written in Python, using the AMUSE framework to incorporate
pre-existing stellar evolution and N-body dynamics code. At the beginning of every time step, the code calculates
positions and velocities for each star in the simulated region, as well as the individual star properties such as mass loss
rates. The stellar properties are calculated using the SeBa stellar evolution code , while the dynamical properties are calculated using the BHTree N-body code . These properties are then utilised by the Python sections of the code authored for this paper, which calculate
enrichment through massive stellar winds and supernovae. In addition to this section, code to determine initial mass
functions and disk lifetimes was devised, and will be discussed in brief as well.
The AMUSE framework was used in the simulations in this paper to initialise, translate and manage the external
N-body and stellar evolution codes . While the N-body and stellar evolution codes in AMUSE can be bridged and run alongside
each other, the code written to calculate SLR enrichment cannot be run synchronously. As such, the main execution
loop is divided into a series of timesteps so that the enrichment routines can run in lockstep with the rest of AMUSE.
As closer encounters between stars are fleeting when compared to the simulation timescale, a relatively small timestep
is adopted in order to sample wind injection more accurately. We found that a total number of 1000 timesteps for
the enrichment code over 20 Myr was suitable for our simulations, the N-body and stellar evolution codes operate on
smaller timesteps.
The SeBa stellar evolution code was utilised to calculate mass loss rates for main and post-main sequence evolution
of stars within a star-forming region. SeBa is extremely fast, using an interpolated lookup table for stellar evolution
rather than directly calculating the stars’ properties – such as in the case of a Henyey code like MESA . Whilst this approximation method is less accurate, SeBa is
suitable for this work due to its speed. It is important to note that mass-loss rates due to winds can vary significantly
between stellar evolution models. However, a common factor between all models is that the bulk of mass loss over the
lifespan of a particularly massive star occurs after the main sequence, which is itself a comparatively short period of
time ; as such most enrichment will occur over this time, and should not vary significantly between
models. SeBa is also used to determine whether massive stars have undergone supernovae. Dynamic evolution of the
star-forming region is performed using the BHTree N-body code. BHTree is a 2nd-order accurate Barnes-Hut octree
code designed for large numbers of particles. Whilst pure N-body codes are more accurate, the extremely favourable
scaling characteristics  vs. O of pure solvers) allow hundreds of large scale simulations to be run
within an acceptable length of time on a 10-core Xeon workstation . As such, the reduction in
accuracy was deemed an acceptable trade-off for this project. Performance was further improved by using OpenMPI
to multi-thread BHTree . At every timestep the change in mass calculated through SeBa was
synchronised with BHTree to account for mass loss affecting gravitational attraction.
After the N-body solver and stellar evolution simulations are evolved to the next timestep, the code written for this
paper to calculate SLR enrichment is executed, as detailed in Section 2.3.
Upon programme initialisation a star-forming region with a given number of stars  and radius  is generated
via a Goodwin & Whitworth  box fractal method. The box fractal model spawns fractals by placing a “root”
particle in the centre of a cube of side Ndiv, which spawns N 3 “leaf” cubes, which can also contain their own “leaf”
particles. The probability of each generation producing offspring is equivalent to N D−3
div , fewer generations are produced
with a lower fractal dimension parameter, D, which is included as an input parameter to the algorithm . Lower values of D lead to more substructure and a less uniform appearance, while higher values
lead to a more homogeneous and spherical appearance. For our simulations we adopt a fractal dimension of D = 2.0,
as this gives a moderate amount of substructure, which is observed in many star-forming regions  and in simulations . Subsequent dynamical evolution makes it difficult to ascertain the initial degree of substructure in a typical
star-forming region , but D = 2.0 probably lies towards the
middle of the distribution of expected values. The initial velocities of the parent particles are drawn from a Gaussian
distribution, with a small random component that scales as N D−3
but reduces for each progressive generation. After
generating the positions and velocities of the stars within the star-forming region, the masses are calculated from an
implementation of the Maschberger initial mass function . The Maschberger IMF is described
in the form of a probability density function following the formula:
P ∝M⋆
µ is the scale parameter. As per Maschberger’s prescription, we use values of α = 2.3, β = 1.4 and µ = 0.2. The mass
range of the Maschberger prescription is between 0.01 M⊙and 150 M⊙. When generating a population of stars, we
ensure that there is at least one high-mass star  in the resultant region and the IMF routine is repeatedly
run until this condition is met . Stochastic sampling of the IMF can result in low-mass
stellar populations that  contain massive stars . Nicholson & Parker 
demonstrate that – assuming a standard Cluster Mass Function of the form Nclus ∝M −2
clus – these unusual low-mass
populations that contain high mass stars occur as often as high-mass clusters which are almost certain to contain
massive stars. So whilst we do not simulate those clusters that do not contain massive stars , our enrichment distributions should be interpreted for stellar populations where some enrichment can
occur.
All stars in the simulation have the same formation time; whilst there would be some margin of age difference
in the stars this would introduce an additional parameter to explore, significantly increasing the required number
of simulations. Furthermore, SeBa and by extension AMUSE cannot be used to simulate mixed-age stellar populations
within a single cluster datatype. Finally, binarity of massive stars is not modelled, as wind-wind interactions of massive
stars and how this affects dust formation and growth is not well established – though in ideal cases there would be a
significant increase in 26Al wind abundance . We will study the effects of massive star binarity
on the enrichment of low-mass stars in a future paper.
Each low-mass star  is assumed to form a protoplanetary
disk with a radius of 100 AU, to facilitate a direct comparison with our previous work  – though we note that other prescriptions for setting the disk radius as a function of stellar mass are used in
the literature . Our disks have a fixed mass dependent on the stellar mass following
the formula:
Mdisk = 0.1 M⋆,
SLR’s stable isotope counterparts. We assume a stable aluminium mass fraction of:
M27Al = 0.0085 Mdust,
M56Fe = 0.1828 Mdust,
to occur post-star-formation. Disk truncation through stellar winds and ionising radiation flux are not simulated, but
could be included in successive versions of this model. The lifetime of each protoplanetary disk is pre-calculated and
derived from an exponential probability density function of the form:
p  = 1
β e−x/β,
the mean disk lifetime, consistent with the findings in Richert et al. . An example of the disk population over
time can be seen in Fig. 1. Stars with masses above 3 M⊙are not assigned disks, as they are markedly less likely to
form planetary systems containing rocky bodies . Final enrichment is calculated
at the point where the individual disk has “progressed” from a disk to a protoplanetary system. Beyond this point,
the system is no longer capable of further SLR pollution, and the final enrichment value is stored. A more advanced
model could reduce the efficiency of the disk in absorbing SLRs as planetesimals begin to form, before culminating in
a finished protoplanetary system, though for our work this 2-phase method should suffice.
For this paper we only simulate the effect of wind and SNe-based enrichment of the proto-planetary disk. SLR
enrichment from other mechanisms such as sequential star formation, spallation or molecular cloud pre-enrichment are
not considered. Estimates for wind and supernovae yields of SLRs from massive stars were derived from simulations
performed in Limongi & Chieffi . For this paper we use their recommended  model with a star rotational
velocity of 300 km s−1. There is a small difference in 26Al yield in the 150 km s−1 model and the 300 km s−1 model
between 13 M⊙to 20 M⊙, though for the 20 M⊙case there is a higher yield in the 150 km s−1 model. The 0 km s−1
model was not considered as this was unrealistic 
Remaining disk fraction, Zdisk
Reamining HM star fraction, Z⋆,HM
Figure 1. A comparison between the fraction of remaining disks and the fraction of high-mass stars that have not reached
the end of their lives. Due to the relatively short disk lifetime most of the stellar disks have progressed to the planetesimal
formation phase before most supernovae have occurred. The dotted vertical line indicates when the first stars with an initial
mass under 25 M⊙undergo supernova, contributing to SLR enrichment according to Limongi & Chieffi .
is under debate, de Mink et al. 2013), and significantly suppressed 26Al yields compared to the other two options.
The results in Limongi & Chieffi  provide a final total yield, using this value we calculate the fraction of SLR
emitted relative to the total wind mass loss rate using SeBa to calculate the total mass loss rate before the simulation
begins. The estimated fraction is calculated through an Akima spline interpolation of the data provided in the paper,
which provides a more accurate fit compared to a cubic spline. This fraction can then be used in conjunction with
the simulation mass loss rate from SeBa to approximate the SLR emission rate from massive stars. Whilst directly
calculating the SLR loss rate through their model would yield improved results — as our estimation does not factor in
composition changes in the wind through stellar interior processes such as dredge-up — these issues are offset by the
fact that the majority of the mass loss of early-type stars occurs after they leave the main sequence in a comparatively
short period of time before supernova . This method also works for SLRs beyond 26Al and 60Fe, such as
this paper. Future versions of this code will be able to analyse other SLRs, as the routines governing enrichment and
decay were designed with flexibility in mind.
In the case of supernovae, the total explosive yield of SLRs is provided by Limongi & Chieffi . It is important
to note that the Limongi & Chieffi  model assumes that stars above 25 M⊙collapse directly into a black hole,
producing no supernova explosion and thus resulting in no further enrichment. The debate on this facet of stellar
evolution is still ongoing, the mass border wherein supernovae occur is still somewhat ill-defined, though contemporary
research suggests that supernovae become increasingly rare above 20 M⊙. We
determined that for this paper we would not consider these higher-mass supernovae – in particular due to the rarity
of higher-mass precursor stars – though investigating the effect of higher-mass supernovae on star-forming region
enrichment could be a future avenue of research.
In order to determine the quantity of material swept up by the protoplanetary disk, we must calculate the geometric
cross-section of the disk when interacting with the wind. To perform this calculation we approximate the outflow as a
spherical wind-blown bubble with a constant density. From this we can calculate the sweep-up area, ηsweep, equivalent
to:
ηsweep = 3
disk∆r⋆
Age, t 
Mass, M⋆
Figure 2. A comparison of cumulative yield of 26Al in stellar winds over 10 Myr from stars with a mass range of 20−60 M⊙. The
majority of the mass loss occurs near the end of each star’s life, as the star exits the main sequences and begins its Wolf-Rayet
phase.
where rdisk is the disk radius, ∆r⋆is the distance travelled by the approaching star during a timestep and rbub is the
bubble radius. Two bubble radii are considered, a small bubble, with a radius of 0.1 pc, and a variable bubble size
with a radius equivalent to the current virial radius of the region. These are representative of a small, local wind , and a diffuse, dispersed stellar wind throughout the region  respectively. The
total effective cross-section from wind absorption, ηwind, is given by the equation:
ηwind = ηconηinjηsweep,
the massive star and the disk, though these represent somewhat conservative values for both ,
with the bubble radius being the most influential free parameter. Finally, we can calculate the mass sweep-up rate,
The “global” and “local” models are run concurrently, as they have no influence on the N-body trajectories nor stellar
evolution of the stars within the star-forming region. Additionally, radioactive decay is modelled as the simulation
runtime is significantly longer than the half-life of either SLR. Decay is calculated by determining the fraction of
remaining SLRs between each timestep, based on the 26Al and 60Fe half-lifes of 0.7 Myr and 2.6 Myr, respectively.
Supernovae enrichment is calculated directly from the supernova yield of the star. All stars within the star-forming
region are instantaneously injected with SLRs. Whilst the outflow material from the supernova would not be travelling
instantaneously, the crossing time of the outflow is on the order of 100 yr, significantly smaller than the smallest
simulation timestep of 2 × 104 yr, and as such this is a reasonable abstraction. The amount of material from the
supernova deposited onto a protoplanetary disk is dependent on the cross-sectional area of the disk relative to the
supernova, such that the geometric efficiency, ηgeom, is:
ηgeom = πr2
disk cos θ
where d is the distance from the approaching star and θ is the disk inclination relative to the massive star. We assume
a constant disk inclination of 60◦, such that cos θ = 0.5. The total supernova SLR absorption efficiency, ηSNe, is given
by the equation:
ηSNe = ηconηinjηgeom,
with the following equation:
ΓSNe,SLR = ηSNeMSNe,SLR,
Enrichment through the “global” model and the supernovae-resultant enrichment are calculated using a post-
processing method which calculates the enrichment rates at each simulation checkpoint, while enrichment through
the “local” model is calculated at the end of each N-body timestep. Both methods are very fast, with post-processing
through the global model taking a few seconds per simulation on a reasonably powerful desktop PC, while the local
model takes 5 −10% of the total computational time per simulation step on the same computer. The main source of
compute time is the calculation of distances between high-mass and low-mass stars. Adding more SLRs for consid-
eration would not impact performance significantly as calculating the enrichment rate is comparatively simple. The
contributions through each model are collated as part of a final post-processing step, which is performed before plots
were made.
We performed repeated simulations with varying star number densities by modifying the star forming region radius
and number of stars within the star-forming region. The number of stars, N⋆is varied between 100, 1000, and 104
stars, in order to observe the effect of star number density  and the compound effect of multiple massive stars
and supernovae. The star-forming region radius is varied between 0.3, 1.0 and 3.0 parsecs, to provide representative
examples of extremely compact, compact and dispersed star-forming regions. This correlates to extremely dense and
relatively dispersed star-forming regions, and is done in order to differentiate between effects due to region density and
massive star count. Each set of parameters is repeated 32 times, to reduce the statistical impact of systems enriched
through improbable means such as multiple supernovae passes .
An important factor governing the enrichment of disks within these simulations is that the rate of disk progression
is significantly faster than the average rate of supernovae occurring for any given simulation. Whilst massive stellar
winds make up the bulk of 26Al enrichment, 60Fe enrichment through winds is many orders of magnitude slower,
as enrichment relies more heavily on supernovae, 60Fe enrichment is curtailed. Furthermore, as we make the same
assumption as Limongi & Chieffi  that only stars with an initial mass below 25 M⊙produce supernovae, with
stars higher than this initial mass instead directly collapsing into black holes. As such, enrichment from supernovae
does not occur until ∼7 Myr after the start of the simulation, at a point where only 7% of disks remain, as seen in
Fig. 1.
Table 1 contains detailed statistics of each simulation set, in particular the number of high-mass stars , sim-
ulation supernovae count , fraction of disks undergoing enrichment,  and the fraction of disks that
underwent enough enrichment for significant degassing to occur .
Enrichment calculations are based on the local model.
To calculate the density of a star forming region in these simulations we use the initial median local density . This value is calculated by determining the median value of the stellar density of a sphere
containing the 10 nearest stars to each star in the star forming region at the start of the simulation. Figure 3 compares
the fraction of all disks that underwent any form of enrichment under the local model versus the median local density.
This non-zero enrichment fraction is used to show how likely close passes to massive stars are for disks. We find
that the amount of disks undergoing enrichment for any given simulation is highly dependent on the initial median
Set Name
N⋆
N⋆,HM
Zlocal,enrich
Z26Al,0.1SS
pt-0.3-100-fr
pt-0.3-1000-fr
pt-0.3-10000-fr
pt-1.0-100-fr
pt-1.0-1000-fr
pt-1.0-10000-fr
pt-3.0-100-fr
pt-3.0-1000-fr
pt-3.0-10000-fr
Table 1. A table showing the simulation parameters as well as a number of key parameters, the mean high mass star count in
the simulation N⋆,HM, the mean number of supernovae by the end of the simulation NSNe, the fraction of disks enriched by the
local model, Zlocal,enrich, and the fraction of disks undergoing 26Al enrichment approaching Solar System estimates , Z26Al,0.1SS. It is important to note that for 100-star simulations there is a significantly higher fraction of massive stars,
due to the requirement of a high-mass star in each simulation.
Region median local density, ˜ρ
Disk enrichment fraction, Zlocal,enrich
rc=0.3, N⋆=100
rc=0.3, N⋆=1000
rc=0.3, N⋆=10000
rc=1.0, N⋆=100
rc=1.0, N⋆=1000
rc=1.0, N⋆=10000
rc=3.0, N⋆=100
rc=3.0, N⋆=1000
rc=3.0, N⋆=10000
Figure 3. A plot of all simulations showing the fraction of all stellar disks that underwent any amount of enrichment from the
local wind model versus initial median local density. We observe a clear dependence on region density, which is more important
than the number of stars in the region, despite simulations with a higher number of stars inherently having more massive stars.
local density of the star forming region. This suggests that a compact, low population star forming region with a
single massive star is potentially ideal for SLR enrichment. Such region masses would be significantly more likely
to occur in nature than higher-mass star forming regions, and the number of low-mass regions containing a single
massive star is similar to the number of higher mass regions that always contain massive stars . Furthermore, the lower massive star count would result in less disk disruption due to ionising photon flux and
supernovae shocks . Even in the case of extremely large numbers of stars,
region radius and density appear to be more influential than star counts in keeping massive stellar winds distant from
disks. It should be noted that the 100-star systems have a higher fraction of massive stars to low mass stars, due to the
minimum simulation requirement of one massive star. The percentage of massive stars in simulations with 100 stars
is ∼1.1% compared to ∼0.25% of the more populous simulations. This can potentially overestimate the amount of
enrichment in a particular system; however, simulating systems without an SLR source would have been redundant.
Star-forming region radius, Rc 
Global model & SNe, disk progression simulated
N⋆
Figure 4. A series of violin plots detailing 26Al and 60Fe enrichment. Solar System enrichment is shown as a red dashed line,
a value of Z26Al,SS ≈5 × 10−5 is used for the 26Al graphs , while a low value of Z60Fe,SS = 10−8 and a
high value of Z60Fe,SS = 10−6 are used for 60Fe plots . The enrichment values are
calculated at the point where the disk has “progressed” or at the end of the simulation, whichever is shortest. While there
is a higher amount of 26Al enrichment; the primarily supernovae-driven 60Fe enrichment is suppressed as most disks may not
undergo supernovae interaction.
Fig. 4 shows enrichment values calculated at the end of the disk’s lifetime. Wind enrichment is influential, as can be
seen in the 26Al results, as 60Fe enrichment is extremely minimal  through stellar winds. We see that
for low radius star-forming regions there are a significant number of stars with 26Al enrichment greater than the Solar
System estimate, however for regions with a higher radius we see that this drops off rapidly, with only the simulation
set where rc = 1 pc and N⋆= 104 exceeding the Solar System average.
In the case of 60Fe enrichment there is an even greater discrepancy between the two models, as the 60Fe yield from
SNe is multiple orders of magnitude greater than the total wind yield from a massive star, 60Fe enrichment relies almost
entirely on supernovae. As disk population is reduced when supernovae begin to occur  we find that there
is a vanishingly small population of stars that have Solar System-like levels of 60Fe enrichment. The longer half-life of
As we discussed in our previous paper , 60Fe enrichment is only important for the evolution and
desiccation of planetesimals in the case of extremely high levels of enrichment . Our
results in Fig. 4 show that these levels of enrichment would be almost impossible, and nothing even close was observed
in our suite of simulations.
We can infer from these results that disk lifetimes are highly influential in the calculation of SLR enrichment. SLRs
that make up a significant fraction of the stellar wind, such as 26Al, are continuously distributed over the lifespan of
Star-forming region radius, Rc 
Local model & SNe, disk progression simulated
N⋆
Figure 5. A series of violin plots showing 26Al and 60Fe enrichment, enrichment is calculated through the local model while
disk progression is simulated in a similar manner to Fig. 4. Solar System enrichment is shown as a red dashed line, a value
of Z26Al,SS ≈5 × 10−5 is used for the 26Al graphs , while a low value of Z60Fe,SS = 10−8 and a high value
of Z60Fe,SS = 10−6 are used for 60Fe plots . Compared to the global model SLR
enrichment is significantly elevated; however many simulations undergo no enrichment whatsoever with this model .
the massive stars within the star-forming region. However, the shorter half-life of 26Al results in significantly skewed
results if the measurement of final 26Al enrichment occurs at the end of the simulation. Furthermore, disks that survive
long enough that a local star enters the Wolf-Rayet phase and goes supernova will see a significantly increased 26Al
enrichment, as well as some 60Fe enrichment.
Fig. 5 shows the results for simulations where the “local” model is used instead, we observe that a greater number
of disks become highly 26Al enriched, and in some cases can be enriched more than two orders of magnitude higher
than the Solar System baseline. However, it should be noted that with the “local” model a disk system can have zero
enrichment, which would not be represented in the violin plot. Enrichment fractions of disks for each simulation set
are shown in Table 1. This enrichment fraction is shown in more detail in Fig. 6, where we can clearly see a small
fraction of disks in compact star-forming regions being enriched to above-Solar System amounts, even in the case of
regions with a low overall population of stars. Above-Solar System enrichment can occur in star-forming regions with
a higher initial radius, though this is significantly less likely, and is similarly independent of the total number of stars
in the region. The total number of systems undergoing 60Fe enrichment is similarly changed, as the SLR flux onto
a disk is significantly increased in the local model when a disk is proximal to a massive star. However, this effect
does not drastically increase the number of systems with Solar System-like levels of 60Fe, which is still dominated by
rc=0.3, N⋆=102
rc=1.0, N⋆=102
rc=0.3, N⋆=103
rc=1.0, N⋆=103
rc=0.3, N⋆=104
rc=1.0, N⋆=104
Figure 6. A cumulative distribution function  of final 26Al enrichment fraction  for all disks grouped by subset
with disk progression simulated. A significant population of disks have a higher enrichment fraction than the Solar System upon
entering the planetesimal formation phase in the cases of systems with extremely large numbers of stars.
supernovae rather than stellar winds, and as such the impact of accounting for disk lifetimes has a more marked effect
on enrichment than the wind model.
From previous work in Lichtenberg et al.  and Eatson et al.  it is found that 26Al is highly important
in the process of planetesimal desiccation, with enrichment levels greater than 0.1 × Z26Al,SS causing a reduction in
final water retention fraction of larger early planetesimals. Table 1 shows that only a small fraction of disks gain this
level of enrichment, requiring compact, dense star-forming regions. The main outlier is the simulation set of regions
with 104 stars and radii of 0.3 pc; these super-dense star-forming regions frequently produce highly enriched disks,
though occurrence of such massive star-forming regions is rare, and also the large number of supernovae that occur
would result in significant disk disruption . It is also important to note that at a radius of
counts and supernovae counts, as these values are inclusive of supernovae deposition. 60Fe enrichment is markedly less
influential to planetesimal evolution, due to the radioisotopes lower decay rate, decay energy and disk mass fraction.
In our previous paper we found that extreme levels of 60Fe enrichment  are required to produce
the level of heating that results in dessication . We find in this paper that Solar System levels of
enrichment are unlikely, let alone enrichment to 103 times Solar System amounts. This further rules out 60Fe as an
influential SLR for planetesimal heating, and the lack of 60Fe enrichment to Solar System amounts in our simulations
suggests the presence of alternative mechanisms of 60Fe enrichment beyond supernovae — such as AGB stars . Finally, comparing this result to previous work such as Parker et al.  we find that 26Al
contribution from stellar winds is very significant, and that stellar density is very important. As we vary both the star
forming region and total population we find that more compact star forming regions produce greater levels of both
local wind bubble model.
Comparing the disks generated in this simulation with the Solar protoplanetary disk, we find that initial Solar System
abundances for both 26Al and 60Fe do occur, but are typically less common. This suggests that the Solar System is
on the upper-end of SLR enrichment, except in the case of very dense, populous  star-forming regions.
These results also suggest that the probability of ocean worlds is relatively high, as radioisotopic heating would be
insufficient to cause significant devolatilization in planetesimals, resulting in a significantly higher water budget for a
would be necessary to estimate the population of ocean worlds in more detail. In recent years the physical cause of
the density dichotomy between super-Earths and sub-Neptune exoplanets has come under closer scrutiny by formation
models that involve water enrichment during the disk phase  instead of
solely relying on post-disk atmospheric escape. Hence, a population synthesis or statistical approach involving 26Al
desiccation with more modern formation models  appears a natural extension for future
work to interpret the growing data from the low-mass exoplanet census .
Star count does not particularly increase the maximum 26Al enrichment, though can result in more disks being
significantly enriched; instead we find that star-forming region mass density significantly affects the final disk enrich-
ment. This is the case even with the “global” wind model, as the rate of SLR deposition is significantly curtailed
as the bubble radius is larger. In the more common case of compact, low mass star-forming regions, we find that
around 5% of disks have undergone levels of enrichment on the order of our Solar System, enough for 26Al decay
heating to have a pronounced effect on planetesimal evolution, particularly affecting retained water fraction. Whilst
massive stellar winds and supernovae are a prominent source of SLRs, the adverse affects of UV photoevaporation
and supernovae shocks can disrupt disks before planetesimals form , making 
star-forming regions with fewer massive stars more conducive to planet formation. As such, the combination of high
levels of SLRs in lower population star-forming regions would result in more water poor rocky planets. Incorporating
disk progression and measuring the final enrichment from the moment the disk has decayed is a more accurate method
of determining enrichment, and in the case of 26Al with its significantly shorter half-life, this can significantly impact
results compared to an observation at an arbitrary time or the end of the simulation. In the case of 60Fe however we
find that supernova-based enrichment is severely inhibited by the disk lifetime typically being significantly shorter than
the lifetime of high-mass stars. We find that by the time supernovae begin to occur only a quarter of protoplanetary
disks remain , this is also seen in our results where disk “decay” skews the 60Fe yields down more than any
contributing factor. This is further inhibited when factoring in high-mass stars failing to undergo supernovae. These
results are corroborated by the findings of Williams & Gaidos  and Gounelle & Meibom , whose results
suggest a similar unlikelihood of Solar System level enrichment, and only in large clusters with very high mass stars.
The “R” model described in Limongi & Chieffi  assumes that direct black hole collapse occurs when a star
with an initial mass greater than 25 M⊙dies, resulting in zero SLR yield. At the point where these contributing
supernovae begin to occur, less than 10% of all disks remain, resulting in further inhibited 60Fe enrichment. Whilst
our model does not take into account mixed-age stellar populations, we do not believe that this would significantly
impact our results, and would be offset by other unsimulated interactions, such as evaporation of disks from UV flux
from massive stars and supernovae shocks . Furthermore, the “low”  60Fe solar
system estimate from Tang & Dauphas  can be explained through processes such as residual galactic chemical
evolution; this mechanism does not explain the higher enrichment levels determined by other studies, such as Mishra
et al.  and Cook et al. . It appears unlikely that these higher estimates can be explained purely through
supernovae. Alternative 60Fe injection mechanisms such as interloping AGB stars could prove a suitable explanation
for this discrepancy .
In this paper we performed a series of simulations to determine typical enrichment rates of star-forming regions for
specific densities and stellar populations. Results differ from previous work ,
as whilst we observe a similar amount of 26Al enrichment, the local wind model produces a higher number of disks
undergoing 26Al enrichment significant enough to influence planetary evolution. Much of the changes in enrichment
compared to previous work are due to the length of time required for supernovae to occur, which is much longer than
the average time a disk needs to progress. Stratification of enrichment from varying simulation parameters is very
clear, with enrichment increasing with star-forming region mass density. We also find that enrichment method and
disk model are particularly influential on the final SLR enrichment amount. 26Al enrichment is primarily through wind
deposition — as this is relatively consistent for either model assuming the system is sufficiently populous or dense
deposition, and as such enrichment to Solar System levels is unlikely, and enrichment to levels where 60Fe is the
dominant heating source of planetesimals is almost non-existent.
In the immediate future, including additional SLR sources such as from “interloping” AGB stars would provide an
interesting comparison. As 60Fe is formed in the core of more massive AGB stars and can be efficiently dredged up
and deposited via the stellar wind – this could result in another avenue of 60Fe enrichment in lieu of the relatively
ineffective massive stellar wind mechanism .
JWE and RJP acknowledge support from the Royal Society in the form of a Dorothy Hodgkin fellowship, and associated
enhancement awards. TL acknowledges support by the Branco Weiss Foundation, the Alfred P. Sloan Foundation","[('star', 155), ('enrichment', 109), ('disk', 87), ('region', 64), ('mass', 62), ('al', 57), ('simulation', 55), ('supernova', 52), ('forming', 49), ('model', 47)]","[(('star', 'forming'), 48), (('forming', 'region'), 47), (('massive', 'star'), 33), (('solar', 'system'), 28), (('stellar', 'wind'), 19), (('fe', 'enrichment'), 18), (('mass', 'star'), 16), (('slr', 'enrichment'), 13), (('rc', 'rc'), 13), (('al', 'enrichment'), 12)]","[(('star', 'forming', 'region'), 24), (('star', 'forming', 'regions'), 23), (('rc', 'rc', 'rc'), 11), (('pt', 'fr', 'pt'), 8), (('fr', 'pt', 'fr'), 8), (('massive', 'stellar', 'winds'), 6), (('stars', 'star', 'forming'), 6), (('high', 'mass', 'stars'), 6), (('enrichment', 'solar', 'system'), 4), (('low', 'mass', 'stars'), 4)]"
2410.17169v1.txt,"Draft version October 23, 2024
Typeset using LATEX twocolumn style in AASTeX631
Effects of Planetary Parameters on Disequilibrium Chemistry in Irradiated Planetary Atmospheres:
From Gas Giants to Sub-Neptunes
Sagnick Mukherjee
Kazumasa Ohno
A primary goal of characterizing exoplanet atmospheres is to constrain planetary bulk properties,
such as their metallicity, C/O ratio, and intrinsic heat. However, there are significant uncertainties in
many aspects of atmospheric physics, such as the strength of vertical mixing. Here we use PICASO and
the photochem model to explore how atmospheric chemistry is influenced by planetary properties like
metallicity, C/O ratio, Tint, Teq, and Kzz in hydrogen-dominated atmospheres. We vary the Teq of the
planets between 400 K- 1600 K, across “cold”,“warm,” and ‘hot” objects. We also explore an extensive
range of Tint values between 30-500 K, representing sub-Neptunes to massive gas giants. We find that
gases like CO and CO2 show a drastically different dependence on Kzz and C/O for planets with cold
interiors  compared to planets with hotter interiors ,
for the same Teq. We also find that gases like CS and CS2 can carry a significant portion of the S-
inventory in the upper atmosphere near Teq ≤600 K, below which SO2 ceases to be abundant. For
solar C/O, we show that the CO/CH4 ratio in the upper atmospheres of planets can become ≤1 for
planets with low Teq, but only if their interiors are cold . We find that photochemical haze
precursor molecules in the upper atmosphere show very complex dependence on C/O, Kzz, Teq, and
Tint for planets with cold interiors . We also briefly explore fully coupling PICASO
and photochem to generate self-consistent radiative-convective-photochemical-equilibrium models.
Major goals of characterizing exoplanet atmospheres
are to understand the formation and evolutionary his-
tory of planets.
This process involves observing exo-
planetary atmospheres and intrepreting these observa-
tions to estimate some key bulk planetary parameters,
like metal enrichment of the atmosphere, which can be
used to understand their formation scenario and evolu-
tionary history. However, our interpretation of observa-
tional data of exoplanetary atmospheres requires a ro-
bust understanding of the various physical and chemical
processes ongoing in the planet’s atmosphere. In this
work, we explore how a series of atmospheric processes
can affect our interpretation of exoplanet atmospheric
observations. We explore how these atmospheric pro-
samukher@ucsc.edu
cesses specifically impact atmospheric chemistry, which
is a crucial link between understanding exoplanetary at-
mospheres and constraining their formation and evolu-
tionary processes.
There has been a considerable amount of modeling
work revolving around the idea that planets retain some
amount of information about their formation scenario
and subsequent evolution in their current state .
This information can be retained within im-
portant planetary parameters like the planetary radius,
bulk metal enrichment, elemental abundance ratios, in-
ternal heat flux of the planet, etc. .
For transiting planets, parameters like
planetary mass and radius are directly measurable. But
other critical parameters like bulk metal-enrichment,
bulk elemental abundance ratios, and internal heat flux
arXiv:2410.17169v1    22 Oct 2024
of planets need to be inferred by observing and under-
standing their atmospheres. As planetary atmospheres
are quite complex, drawing inferences about their bulk
properties requires a detailed understanding of the mul-
tiple interconnected physical processes at play.
JWST observations have opened a new era of precision
chemical analysis of transiting planet atmospheres .
This is especially interesting because the atmospheric
chemistry of planets with H/He atmospheres  is sensitive to three
key planetary parameters – 1) bulk metal-enrichment , 2) bulk elemental abundance ratios, and 3)
internal heat flux of planets .
The metallicity of a planet’s atmosphere is defined as
planet/sun, where X represents the number
of atoms of a certain element  in the
object and H is the number of hydrogen atoms. There-
fore a planet with higher metallicity will have signifi-
cantly different atmospheric chemical composition than
one with lower metallicity. Two planets with the same
metallicity might have the “metals” distributed differ-
ently among various elements. For example, one planet
can have a higher C/H ratio and lower O/H ratio than
the other and yet maintain the same bulk metallicity
abundance ratios are important potential markers of the
formation location of planets because of the presence
of various ice lines in propoplanetary disks . It
is clear and well-studied how both of these bulk parame-
ters can affect atmospheric chemistry especially if the at-
mosphere is in thermochemical equilibrium throughout
planets and brown dwarf atmospheres have shown that
disequilibrium chemical processes in addition to ther-
mochemistry can play a significant role across planetary
atmospheres .
The chemical structure of a planet’s atmosphere is
shaped by a number of other physical processes too,
namely – 1) atmospheric mixing, 2) photochemistry, 3)
condensation, and 4) molecular diffusion. As a result,
the photospheric abundance of a gas like CH4 is not only
controlled by the metallicity, C/O ratio, and heat flux
from the deep interior of the planet but also by parame-
ters like the stellar UV flux incident on the planet along
with atmospheric properties like strength of vertical and
horizontal dynamics in the atmosphere . Molecular diffusion is an important pro-
cess controlling the chemical nature of atmospheres at
very low pressures but the importance of this process
is often overshadowed by photochemical processes, es-
pecially in highly irradiated giant exoplanets . Atmospheric mixing, photochemistry, con-
densation, and internal heat flux of exoplanets remain
the least understood/constrained processes/parameters
in exoplanet atmospheres. We briefly explain these pro-
cesses here.
Atmospheric Mixing – Atmospheres of planets are dy-
namic, not static, in nature. This dynamics can play out
in three dimensions causing both vertical and horizon-
tal bulk transport of gases and aerosols in atmospheres
photospheric abundances of gases such as CH4, CO, NH3
can be significantly altered depending on the strength of
dynamics in the atmosphere . 3D models of tidally
locked close-in giant exoplanets have revealed three ma-
jor types of transport operational in their atmospheres
gases and aerosols from the deeper parts of the atmo-
sphere across several pressure scale-heights, while the
day-to-night winds can transport gases from the hot-
ter day-side of the planet to their colder night-side.
Clearly, a full understanding of the role of atmospheric
dynamics on planet-wide chemistry requires the use of
sional parameter space involved. This vast parameter
space includes dimensions like the stellar flux incident
on the planet, the planet’s gravity, atmospheric com-
position, rotational period, interior properties such as
interior heat flux, cloud composition, etc. The compu-
tational run times involved in 3D atmospheric models
often prohibit a full scale parameter space exploration.
However, 1D models can somewhat capture the verti-
cal transport operating in these atmospheres by simpli-
fying the transport as a diffusive process, parametrized
with the 1D eddy diffusion coefficient– Kzz.
Kzz is
defined as vmixLmix, where vmix is the typical veloc-
ity of transport and Lmix is the typical length-scale at
which this bulk transport is operational . But, observational constraints on Kzz are very
weak.
Recently, several studies have constrained Kzz
to a certain extent in brown dwarf atmospheres with
ground-based and space-based data  but we have only started to constrain it for irradi-
ated planets . Various 3D and 2D modeling
efforts have found a large range of Kzz values in model
atmospheres , keeping the parameter uncertain by sev-
eral orders of magnitude in irradiated planets. It is im-
portant to especially mitigate this uncertainty due to
the influence of Kzz on key gas abundances that are
used to infer bulk properties of planets such as C/O or
metallicity.
Internal Heat Flux – As planets evolve, they slowly
lose their gravitational heat of formation to space. This
energy is carried from the deep interior through convec-
tion to the deep atmosphere up to the point where the
atmosphere is opaque at all wavelengths . Beyond that, the heat is lost to space via
radiation in the partially or fully optically thin radia-
tive atmosphere. Therefore, detailed properties of the
atmosphere, such as its composition and thermal struc-
ture, control the rate of cooling of the whole planet over
its entire lifetime.
This rate of cooling, in turn, also
controls the contraction of the planet over time. Addi-
tionally, external energy sources like tidal heating can
also deposit energy to the deep interior of the planet,
the extent of which depends on various orbital and bulk
planet properties.
The internal heat flux cannot be measured directly
through transmission and emission spectroscopy of
close-in exoplanet atmospheres because it typically only
alters the temperature structure of the planet where its
atmosphere is optically thick, leading to little or no af-
fect on its photospheric temperatures. However, internal
heat flux can indirectly affect the photospheric chemi-
cal abundances due to the vertical mixing phenomenon
discussed previously . If gases are mixed up to the photospheres
of the planets from the deep optically thick atmosphere,
where internal heat flux affects chemistry, then they can
alter the observable photospheric chemistry of the planet
and let us infer the internal heat flux of the planet as
well. This works for gases which are stable for long times
and not chemically destroyed in the photosphere imme-
diately after being transported. Gases such as CO and
N2 fit into this category as destroying them chemically
at low temperature photospheres and converting them to
gases like CH4 and NH3 is generally energetically pro-
hibitive.
Therefore, photospheric abundances of such
gases are strongly influenced by the internal heat flux of
the planet. Recently, JWST transmission spectroscopy
observations of WASP-107b were used to constrain its
internal heat flux by Welbanks et al.  and Sing
et al. . Both of these analysis revealed a high Tint
heating of the planet’s interior. Similar constraints on
other transiting planets have also been obtained using
HST . These studies are an excel-
lent example of how understanding atmospheric chem-
istry can reveal details of close-in giant planet evolution.
The internal heat flux of a planet at a given age de-
pends on the mass of the planet.
For example, at a
similar age, a more massive planet is expected to have
a hotter interior than a less massive planet, if there are
no external perturbations on the planet such as tidal
heating.
However, interior temperatures also depend
on aspects of atmospheric physics.
For example, the
strength of vertical mixing plays a major role in affect-
ing both atmospheric clouds and chemistry.
Both of
these things can delay/accelerate the cooling of planets
over time .
Photochemistry – Planets can have significant X-ray
and UV photons incident on them from their host
stars. These photons can cause the photolysis of weakly
bonded molecules such as CH4, NH3, H2S, etc. in the
upper atmosphere . This can trigger a whole new
set of chemical reactions altering photospheric compo-
sition. The first concrete evidence of a photochemical
by-product was confirmed with JWST in WASP-39b by
the presence of photochemically produced SO2 in its up-
per atmosphere . While
significant modeling work on understanding photochem-
istry in close-in planets has been done previously , this JWST
work demonstrated that photochemistry cannot be ne-
glected while modeling the photospheric chemical na-
ture of planets and using abundances to infer planetary
properties.
Additionally, photolysis of gases like CH4 and NH3
can lead to the production of C- and N- bearing
molecules like C2H2 and HCN. These gases in the up-
per atmospheres of H2/He-rich atmospheres can act as
precursors for further reactions to form photochemical
hazes . The optical properties of these hazes
are such that they can play a significant role in shaping
the observable spectra of exoplanet atmospheres .
The discussion above shows that the observational
data from transiting planets today is at a stage where
its chemistry can be leveraged to not only understand
atmospheric elemental compositions of planets but also
should be leveraged to understand exoplanetary atmo-
spheric processes better as well.
These include con-
straining key atmospheric processes such as atmospheric
dynamics and parameters like internal heat flux of plan-
ets.
Therefore, it is necessary to explore how atmo-
spheric dynamics and internal heat flux can influence
the photospheric abundance of gasses in exoplanets with
varying bulk properties such as equilibrium tempera-
ture, C/O ratio, metallicity, and varying amounts of
incident UV flux. It is also important to identify the
key molecules that can produce the best constraints on
very uncertain parameters like Kzz in different parts of
this vast parameter space.
This paper presents a broad parameter space study
to address these questions. Our work is applicable to a
wide variety of planets. In terms of internal heat flux,
this includes sub-Neptune mass planets with lower inter-
nal heat fluxes, to Jupiter mass giant planets with much
hotter interiors. In terms of planetary temperatures, we
explore planets between cold giant planets 
to hot Jupiters . We also explore the at-
mospheric chemistry of planets with varying C- and O-
abundances. We also investigate the effects of a wide
range of the atmospheric mixing parameter Kzz and at-
mospheric metallicity on the chemistry of these planets.
We describe our modeling setup in§2 followed by the
results presented in §3. We discuss the limitations and
caveats in this analysis in §4 followed by summarizing
our key conclusions in §5.
The aim of the modeling presented in this work is
to explore a significant parameter space of properties
that can influence atmospheric chemistry of transit-
ing planets with H/He atmospheres.
The parameters
we consider in this study are the equilibrium temper-
ature , interior heat flux through the parameter
Tint, strength of vertical mixing parameterized with
Kzz, C/O ratio, and metallicity. The modeling setup
to achieve this parameter space exploration can be di-
vided into two different components – 1) 1D radiative–
convective atmospheric modeling, and 2) chemical ki-
netics modeling. We describe each component in detail
below.
We use the 1D radiative–convective equilibrium model
PICASO  to
generate atmospheric temperature–pressure ) pro-
files for irradiated planets. We generate the T pro-
files for a planet with gravity= 4.5 ms−2 around a star
with Teff=5327 K, log=4.38, = -0.03, and Ra-
dius= 0.932 R⊙. These system parameters are chosen
to be the same as WASP-39b. We divide the planet at-
mosphere into 91 plane-parallel levels  loga-
rithmically spaced in pressure between 10−6 to 100/5000
bars. We generate the T profiles using the heat re-
circulation factor rfacv= 0.5, which corresponds to the
case of full heat redistribution across the planet.
place the planet at different distances from the star such
that the Teq of the planet  are 1600
K, 1400 K, 1100 K, 900 K, 800 K, 600 K, and 400 K. For
each of these cases, we generate 5 different cases with dif-
ferent Tint values of 30 K, 100 K, 200 K, 300 K, and 500
K. The lowest value, 30 K, is representative of Gyr+ old
sub-Neptunes . Jupiter’s value
today is 100 K. More massive, younger, or inflated gas
giants  would
have larger Tint values . The equi-
librium chemistry calculation and gaseous opacities used
for computing these T profiles are the same as used
in Mukherjee et al. . Figure 1 shows these gen-
erated T profiles where each panel corresponds to a
different Teq case. Note that these radiative–convective–
thermochemical–equilibrium  models were cal-
culated by including TiO and VO opacities, which are
known to cause thermal inversions in atmospheres which
have Teq≥1600 K. We generate these T profiles at
librium throughout the atmosphere.
To simulate disequilibrium chemistry within our
model atmospheres, we use the python wrapped Pho-
Teq=1600 K
Tint=30 K
Tint=100 K
Tint=200 K
Tint=300 K
Tint=500 K
Teq=1400 K
Tint=30 K
Tint=100 K
Tint=200 K
Tint=300 K
Tint=500 K
Teq=1100 K
Tint=30 K
Tint=100 K
Tint=200 K
Tint=300 K
Tint=500 K
Teq=900 K
Tint=30 K
Tint=100 K
Tint=200 K
Tint=300 K
Tint=500 K
Teq=800 K
Tint=30 K
Tint=100 K
Tint=200 K
Tint=300 K
Tint=500 K
Teq=600 K
Tint=30 K
Tint=100 K
Tint=200 K
Tint=300 K
Tint=500 K
Figure 1. T profiles computed using 1D radiative–convective–equilibrium modeling at various Teq and Tint values are shown.
Each panel shows the model T profiles computed for a Teq value at five different Tint values between 30 K and 500 K. The
six panels correspond to Teq= 1600 K, 1400 K, 1100 K, 900 K, 800 K, and 600 K. The kink at 1 mbar and 1700 K in the hottest
models  is due to a small amount of TiO opacity. All the T profiles have been computed for a planet with
gravity=4.5 ms−2 around a star with Teff= 5326.6 K, log=4.38, and Radius=0.93R⊙.
tochem1 chemical kinetics model .
We use the T profiles computed using the
method described in the previous section as inputs in
the Photochem package. In addition to the Photochem
package, we use the Cantera software  to simulate thermochemical equilibrium in the
very deep atmosphere of the model planet atmospheres.
For each model, we use the quench time approxima-
tion with chemical timescales from Zahnle & Marley
like CH4, NH3, CO2, etc. At atmospheric levels with
pressures much greater than the highest quench pres-
sures among these gases , we use Cantera to
calculate the thermochemical equilibrium state of the at-
mospheres. At pressures smaller than this pressure level
we use Photochem to simulate chemical disequilibrium
with its kinetics calculations.
The convergence crite-
ria used in our photochemistry modeling is presented in
tochemical models significantly quicker than using the
chemical kinetics calculation for all the pressure layers
in the atmosphere including the very high pressure and
high temperature deep atmosphere where chemical reac-
tions are expected to be very fast. The setup saves com-
putational time by preventing the kinetics model from
taking extremely small time steps to model the very
rapid chemical reactions in the very deep atmosphere,
which is bound to remain in thermochemical equilib-
rium anyway. To maintain sufficient accuracy in the 1D
chemical kinetics calculations, we interpolate the atmo-
spheric T profiles with 91 pressure levels to a finer
grid of pressures with 180 pressure levels before ingest-
ing them within Photochem.
For a given metallicity, we scale the C/H, O/H, N/H,
S/H from the solar values with the metallicity factor
in our model.
To change the C/O ratio for a given
metallicity, we change both the C/H and O/H such that
tain the same atmospheric metallicity while changing
the C/O ratio of the atmosphere.
To account for O-
atoms locked up in various expected condensates such
as silicates, we remove 20% of the O from the atmo-
spheric gas phase.
Tint= 100 K
Tint= 200 K
Tint= 300 K
Tint= 500 K
Figure 2.
T profiles computed assuming radiative–convective–thermochemical equilibrium  and radiative–
convective–photochemical equilibrium  are shown with dashed black lines and solid red lines, respectively. The assumed
Teq, , and C/O of the model is 800 K, +1.0, and 1×solar, respectively. Each panel corresponds to a different Tint from
and brown, respectively. The dashed VMR profiles are from the RCTE models whereas the solid VMR profiles correspond to
the RCPE models. The narrower panel accompanying each of the four panels shows the different in the T profile between
the two modeling assumptions in each case as a function of pressure.
PICASO and Photochem can be fully and iteratively
coupled, instead of PICASO using pre-computed equilib-
rium chemistry tables ,
or using on-the-fly non-equilibrium chemistry due to
mixing implemented using quench time approximation
the difference in the T profile between a fully self-
consistent treatment of photochemistry within the cli-
mate model and a profile computed assuming radiative–
convective–thermochemical equilibrium  for the
Teq=
K,
and C/O=1×solar.
The radiative–
convective–photochemical equilibrium   model was
computed by combining the Photochem kinetics model
mate calculation framework.
Each panel of Figure 2
illustrates the difference between the T profiles for
four different Tint values of 100 K, 200 K, 300 K, and
RCPE  chemical profiles for CH4 , NH3
It is clear that for lower values of Tint, the maximum
difference in the T profile is typically around ∼100
K in the 0.1 mbar to 0.1 bar range.
Whereas as the
Tint increases, small differences of the order of ∼20
K remain in the profiles, even at deeper pressures till
around 100 bars.
This shows that the corrections in
the T profiles due to radiative feedback from the
changed atmospheric chemistry due to vertical mixing
or photochemistry is an important effect, especially for
high Tint values. This will be especially relevant for in-
terpreting emission spectra of planets. Even though this
work focuses on modeling disequilibrium chemistry ef-
fects, we choose not to use the RCPE approach for our
T profile calculations mainly because this can vastly
increase the computational time involved in running a
transmission spectroscopy in this paper where temper-
ature has a relatively small effect compared to emission
spectroscopy. Moreover, in our parameter space explo-
ration, we vary both metallicity and C/O ratio but we
do not self-consistently include the variation of the T
profile with metallicity and C/O in our models. We dis-
cuss the effect of these approximations further in §4.
Our parameter space exploration  is focused on
investigating roles of C/O and Kzz across planets with
different Teq and Tint. For this, we run photochemical
models for 13 different C/O values between a range of
C/O ratios starting from 0.0458 to 1.125 with a step size
of 0.0916, exploring the very O- rich to very C- rich at-
mospheres at 10×solar metallicity. We also vary the Kzz
parameter between 106 to 1013 cm2s−1 with steps of 1 in
the log10 space. This range reflects the huge range
of uncertainty on this parameter covering cases of very
slow vertical mixing to rapid mixing. Therefore, in the
first parameter space exploration we compute chemical
kinetics models for each T profile shown in Figure 1
for a range of C/O and Kzz. For each model, we use
the X-ray/UV spectra for WASP-39b used in Tsai et al.
on the planet as we move the planet closer or further
away from the star to change its Teq.
We make this
choice as we want to explore the chemical trends in at-
mospheres as a function of varying Teq with a uniform
X-ray/UV flux incident on the planet. This is to avoid
changing this additional parameter while we sweep the
Teq parameter space. Our second part of the parame-
ter space exploration  is focused on exploring the
role of metallicity, C/O, Kzz, and Tint on the photo-
spheric abundances but at a fixed Teq. We present the
trends in photospheric abundance with varying Teq in
various planetary parameters in §3.4. We also present
trends in our estimates of photochemical precursors to
haze/soot across the parameter space in §3.5.
and Tint across different Teq
In this section, we present the variation of key molec-
ular abundances at the pressures typically probed by
transmission spectroscopy for transiting planets.
top three panels of Figure 3 shows an example of how
CH4 abundance profiles change with changing Kzz, Tint,
and C/O, respectively for a planet with Teq= 800 K.
The middle and bottom three panels of Figure 3 shows
the same but for SO2 and CO2, respectively. The ap-
proximate pressure range probed by transmission spec-
troscopy is shown with the red shaded region in each
panel. It is clear that Tint, Kzz, and C/O can cause sig-
nificant changes to the photospheric abundance of gases
like CH4 but Figure 3 also shows that not all gases are
equally sensitive to all of these parameters.
In order to present the trends in photospheric abun-
dance of gases with variations in these parameters, we
use a box-shaped kernel between 10 mbar and 0.1 mbar
which is also the region shaded in Figure 3. For each
gas abundance profile, we calculate the logarithm of the
average photospheric abundance using,
log10 =
R P2
P1 log10)dp
R P2
P1 dp
p1 is 10 mbar, and p2 is 0.1 mbar. This range covers
a typical pressure range probed by transmission spec-
troscopy. We present the trends in Xw for various gases
in the following sections.
CH4 is one of the major C-bearing gases in giant
planet atmospheres. Figure 4 shows the quantity Xw
for CH4 with a heat map as a function of Kzz and C/O
at various Teq and Tint values. Each row in Figure 4 cor-
responds to a different Teq from 1100 K to 600 K, while
each column corresponds to a different Tint from 30 K
to 500 K. We do not show the heat maps computed for
Teq of 1400 and 1600 K here as they are qualitatively
similar to the 1100 K case. The left most part of each
sub-figure shows the trend in CH4 Xw in the case of
thermochemical equilibrium.
Figure 4 shows that for Teq ≥1100 K, CH4 only
becomes abundant in the photosphere when the atmo-
sphere is very C- rich with C/O > 2×solar or when Kzz
C/O dependence for CH4, albeit their calculations as-
sumed thermochemical equilibrium. The amount of CH4
in the photosphere for these hotter planets shows some
Tint dependence when the vertical mixing is extremely
vigorous with Kzz ≥1012 cm2s−1.
In these cases, a
colder interior planet  shows
higher amount of photospheric CH4 than planets with
much hotter interior . This shows that
the Tint parameter, which mostly affects the T and
CH4 with Kzz
Kzz=106
Kzz=107
Kzz=109
Kzz=1010
Kzz=1011
Kzz=1012
Kzz=1013
CH4 with Tint
Tint=30 K
Tint=100 K
Tint=200 K
Tint=300 K
Tint=500 K
CH4 with C/O
C/O= 0.1×
C/O= 0.5×
C/O= 1.1×
C/O= 1.5×
C/O= 2.5×
SO2 with Kzz
Kzz=106
Kzz=107
Kzz=109
Kzz=1010
Kzz=1011
Kzz=1012
Kzz=1013
SO2 with Tint
Tint=30 K
Tint=100 K
Tint=200 K
Tint=300 K
Tint=500 K
SO2 with C/O
C/O= 0.1×
C/O= 0.5×
C/O= 1.1×
C/O= 1.5×
C/O= 2.5×
CO2 with Kzz
Kzz=106
Kzz=107
Kzz=109
Kzz=1010
Kzz=1011
Kzz=1012
Kzz=1013
CO2 with Tint
Tint=30 K
Tint=100 K
Tint=200 K
Tint=300 K
Tint=500 K
CO2 with C/O
C/O= 0.1×
C/O= 0.5×
C/O= 1.1×
C/O= 1.5×
C/O= 2.5×
Figure 3. The dependence of chemical abundance profiles on Kzz, Tint, and C/O are shown in the left, middle, and right
columns, respectively. The upper row shows this dependence for CH4 while the middle and lower rows show this dependence
for SO2 and CO2, respectively. The pressure values typically probed by transmission spectra are shown with the red shaded
region.
chemistry in the deep atmosphere, already starts to af-
fect the CH4 abundance at much smaller pressures at
Teq=1100 K, when mixing is vigorous.
Figure 4 shows that for a given C/O, Tint, or Kzz,
the CH4 abundance is much higher in the Teq ≤900 K
planets than the Teq ≥1100 K planets. For Teq= 900
K, Figure 4 shows that the CH4 abundance varies very
strongly with Kzz. For a given C/O, the photospheric
CH4 abundance shows 6-7 orders of magnitude increase
with the Kzz increasing by 7 orders of magnitude. The
abundance of photospheric CH4 is much lower than the
expectations from thermochemical equilibrium for Kzz
cal mixing, the amount of CH4 rises to thermochemical
equilibrium expectations for these colder interior plan-
ets. Whereas, the CH4 abundance is always lower than
the expected amount from thermochemical equilibrium
for hotter interiors for all the Kzz values explored in
Figure 4. Equal abundance contours plotted on Figure
that the variation of CH4 abundance with changing Kzz
is stronger than its variation with C/O. For a given C/O
and Kzz, the CH4 abundance typically decreases by 3-4
orders of magnitude with the Tint increasing from 30 K
to 500 K for Teq=900 K. Figure 4 shows that the qual-
itative behavior shown by the CH4 abundance is very
similar between Teq=900 K and Teq=800 K panels ex-
cept the CH4 abundances are higher in the Teq=800 K
panels than the Teq=900 K panels for a given C/O, Tint,
and Kzz value.
C/O 
Tint= 30 K
Tint= 100 K
Tint= 200 K
Tint= 300 K
Teq = 1100 K
Tint= 500 K
C/O 
Teq = 900 K
C/O 
Teq = 800 K
C/O 
Teq = 600 K
Figure 4. The weighted abundance of CH4 calculated using Equation 1 as a function of C/O and Kzz in each panel. Each row
corresponds to a different Teq value from 1100 K to 600 K from top to bottom. Each column correspond to a different Tint value
between 30 K and 500 K from left to right. Note that the C/O is shown relative to an assumed solar value of 0.458 in this plot.
The last row in Figure 4 shows CH4 abundances for
Teq= 600 K. For these cold Teq values, CH4 abun-
dance shows a significant increase compared to the
Teq= 800 K cases. This increase is especially high for
Kzz≤1010cm2s−1. Also, the equal abundance contours
for Teq= 600 K, are nearly horizontal which shows that
the CH4 abundance is much more sensitive to C/O than
Kzz for these cold planets. At Teq= 600 K, for a given
C/O and Kzz value, the CH4 abundance shows a 2-3
orders of magnitude decrease with Tint increasing from
in Figure 4 show that while interpreting the C/O of a
planet from its measured CH4 abundance, one should
be particularly careful in parts of the parameter space
where the contours in Figure 4 are not horizontal. This
means that the same photospheric CH4 abundance can
be a result of a wide range C/O in these parts of the
parameter space. This also suggests that the CH4 abun-
dance, if detected, can be a good diagnostic of Kzz in
parts of the parameter space where the equal abundance
contours are nearly vertical. Figure 4 suggests that CH4
becomes a favorable marker of vertical transport when
Teq is between 800–900 K, which is slightly colder than
the ‘sweet-spot’ identified in the 3D models presented in
Zamyatina et al. . Note that we have shown these
trends at a fixed metallicity .
Figure 5 presents the Xw for CO across different C/O
ratios and Kzz.
Again, each panel corresponds to a
different Teq and Tint combination. For Teq= 1100 K,
Figure 5 shows that the CO abundance is only sensi-
tive to C/O showing a rapid increase with increasing
C/O around C/O∼0.3×solar.
Figure 5 suggests that
the CO gas can be a great tracer of C/O in planets
with Teq ≥1100 K irrespective of the strength of Kzz.
C/O 
Tint= 30 K
Tint= 100 K
Tint= 200 K
Tint= 300 K
Teq = 1100 K
Tint= 500 K
C/O 
Teq = 900 K
C/O 
Teq = 800 K
C/O 
Teq = 600 K
Figure 5. The weighted abundance of CO calculated using Equation 1 has been shown as a heat map as a function of C/O
and Kzz in each panel. Each row corresponds to a different Teq value from 1100 K to 600 K from top to bottom. Each column
correspond to a different Tint value between 30 K and 500 K from left to right. Note that the C/O has been shown relative to
the assumed solar value of 0.458 in this plot.
The CO abundane for a given C/O also shows almost
no Tint dependence for Teq≥1100 K. The difference in
CO abundance between the thermochemical equilibrium
and disequilibrium chemistry at Teq= 1100 K and 900
K is almost negligible. However, some Kzz dependence
of CO abundance appears at Teq= 900 K, when Tint≤
creasing Kzz when Kzz≥109cm2s−1. This decrease of
CO abundance with increasing Kzz beyond 109cm2s−1
is much more pronounced when Tint=30 K than the case
of Tint= 100 K. Similar behavior is also shown by the
Teq= 800 K models, where the Kzz dependence of CO
abundance for cold interior planets  sets
in at a slightly lower Kzz of 108cm2s−1. This suggests
that for planets with Teq at 800 K or 900 K and hot
interiors , CO can be a great tracer of
C/O, irrespective of Kzz and with minimal dependence
on Tint as well. But for such planets with cold interiors
Tint ≤100 K, whether CO will trace the C/O of the
atmosphere depends largely on the strength of Kzz.
The same behavior is not only shown but enhanced
further in Teq= 600 K planets shown in the last row of
Figure 5. The CO abundance mainly depends on Kzz
only in these cases for Tint ≤100 K and is dependant
only on C/O for Tint ≥200 K. This suggests that for
planets with colder Teq  and cold interiors
whereas for all the other cases of hotter planets or hotter
interiors, CO abundance is a great tracer of atmospheric
C/O.
The CO abundance in planets with cold interiors and
colder Teq values shows a decrease with increasing Kzz,
beyond a certain Kzz value, because their T profiles
straddle the CO=CH4 equal abundance curve . This has been shown in Figure 7 right panel,
where the black dashed curve shows the CO=CH4 equal
C/O 
Tint= 30 K
Tint= 100 K
Tint= 200 K
Tint= 300 K
Teq = 1100 K
Tint= 500 K
C/O 
Teq = 900 K
C/O 
Teq = 800 K
C/O 
Teq = 600 K
Figure 6. The weighted abundance of CO2 calculated using Equation 1 has been shown as a heat map as a function of C/O
and Kzz in each panel. Each row corresponds to a different Teq value from 1100 K to 600 K from top to bottom. Each column
correspond to a different Tint value between 30 K and 500 K from left to right. Note that the C/O has been shown relative to
the assumed solar value of 0.458 in this plot.
abundance curve and the solid T profiles represent
Tint=30 K models with Teq lying between 1100 K and
tion of the figure and CH4 abundance decreases towards
the top right direction of the plot.
The black points
on each T profile show the quench points for CO,
when the Kzz is 106, 108, 1010, and 1012 cm2s−1. The
CO abundance remains almost constant with changing
Kzz if the quench pressure lie in the CO dominated re-
gion, which is the top right part above the CO=CH4
equal abundance curve.
If quenching happens in this
region, the quenched CO abundance is expected to be
insensitive to Kzz. However, if CO is quenched in the
lower left half, where CO abundance is rapidly decreas-
ing, then the CO abundance shows a rapid decrease with
increasing Kzz. This decrease is sharper because of the
isothermal nature of the T profile, which are far from
being parallel to the equal abundance contours of CO in
this part of the T −P space.
Another important behavior reflected in Figure 5 is
that the photospheric CO abundance remains the same
between thermochemical equilibrium and chemical dis-
equilibrium for Teq ≥900 K, but for colder planets, the
CO abundance in the thermochemical equilibrium cases
are smaller than the models with low Kzz values , irrespective of Tint. This suggests that even
though the CO abundance can act as a tracer of C/O
teriors, disequilibrium chemistry calculations must still
be used to interpret the correct C/O of a planet from
its measured CO abundance. This behavior is a direct
effect of the shape of the T profiles of these plan-
ets, when their Teq is ≤800 K and can also be under-
stood with the left panel of Figure 7. The T pro-
files of planets with Teq≤800 K transition from the CO-
CH4 decreasing
CO decreasing
Teq= 1100 K, Tint= 30 K
Teq= 900 K, Tint= 30 K
Teq= 800 K, Tint= 30 K
Teq= 600 K, Tint= 30 K
CO=CH4
Teq= 1100 K, Tint= 200 K
Teq= 900 K, Tint= 200 K
Teq= 800 K, Tint= 200 K
Teq= 600 K, Tint= 200 K
Teq= 1100 K, Tint= 30 K
Teq= 1100 K, Tint= 30 K
Teq= 1100 K, Tint= 30 K
Teq= 1100 K, Tint= 30 K
Figure 7. Left panel: The T profiles of planets with Tint= 30 K and Teq between 1100 and 600 K are shown with the
four solid lines. The black dashed line shows the CO=CH4 equal abundance curve from chemical equilibrium calculations at
black markers on each T profile show the quench pressures of CO for 106, 108, 1010, and 1012 cm2s−1. Right panel: The
T profiles of planets with Tint= 200 K and Teq between 1100 and 600 K are shown with the four solid lines, whereas the
dashed lines show the models for Tint= 30 K. Equal abundance contours for CO2 from thermochemical equilibrium are shown
in red.
dominated to the CH4 dominated regions in the T −P
space, as shown in Figure 7. This leads to a sharp de-
crease in photospheric CO in these objects. However,
even for low Kzz values , the CO gets
quenched at much deeper pressures, where their T
profiles cross equal abundance contours corresponding
to higher CO abundance than the low photospheric CO
abundance expected from thermochemical equilibrium.
As the CO abundance becomes roughly constant on the
top right side of the CO=CH4 line, this doesn’t happen
when Teq≥900 K.
Figure 6 shows the Xw for CO2. At Teq= 1100 K, the
CO2 abundance shows a sharp decrease when C/O ≥
show such a sharp change in its abundance when C/O
is ≥1.9×solar as shown in the top panels of Figure 5.
CO2 shows this sharp decline beyond C/O ≥1.9×solar
due to the unavailability of enough O- atoms to form
CO2 when the C/O is approaching 1. At this Teq, CO2
remains practically independent of Kzz across all Tint
values, except for some slight Kzz sensitivity for Kzz≥
tospheric CO2 abundance doesn’t show much change
with changing C/O between 0.3×solar≤C/O≤1.2×solar
across all Tint values. This might limit the utility of CO2
as a C/O diagnostic for this range of C/O values. This
suggests the CO2 can be a good tracer for C/O in these
Teq ≥1100 K planets, except for very high strengths of
vertical mixing or for 0.3×solar≤C/O≤1.2×solar. CO2
also doesn’t show much dependence on Tint at this Teq.
CO2 has a larger Kzz dependence for planets with
Teq≤900 K. For Teq= 900 K and Tint= 30 K, the CO2
remains insensitive to Kzz for Kzz≤109cm2s−1. How-
ever, the CO2 shows very strong sensitivity to Kzz for
higher values of Kzz causing the CO2 abundance to drop
by two orders of magnitude with Kzz increasing by four
orders of magnitude. This behavior is present for hot-
ter Tint models as well, albeit with a slower decrease of
CO2 abundance with increasing Kzz. With increasing
Tint, the Kzz value at which CO2 transitions from being
insensitive to sensitive to Kzz also shifts slowly towards
higher value. The Teq= 800 K models shown in the third
row of Figure 6, also show the same qualitative behavior
as the Teq =900 K models, but with an even stronger de-
pendence of CO2 abundance on Kzz once the Kzz value
is higher than the “transition” Kzz value. This behavior
can be readily understood with the right panel of Figure
Each profile corresponds to a different Teq. The equal
abundance contours for CO2 from thermochemical equi-
librium are shown with the red lines. For planets with
hotter Teq or hotter Tint, most of the T profile lie in
parts of the T −P space where the CO2 equal abun-
dance contours appear less dense. This suggests that a
large change in quench pressure of CO2 due to changing
Kzz will only cause a small change in the quenched CO2
abundance for such T profiles. However, for planets
with colder Teq or colder Tint, the T profiles traverse
a denser region of equal abundance contours of CO2. For
these models, a change in quench pressure of CO2 due
to changing Kzz will produce a much more pronounced
effect on the quenched CO2 abundance. These models
suggest that for Teq=800 and 900 K planets, irrespective
of Tint, the CO2 abundance remains sensitive to Kzz to
varying degrees if Kzz is higher than a “transition” value
which is typically somewhere between 109-1010cm2s−1.
Other than the coldest interior case of Tint=30 K, the
CO2 abundance doesn’t show very strong dependence
on Tint.
Although for Teq≥800 K, the photospheric CO2 abun-
dances between thermochemical equilibrium and low
Kzz values are nearly the same, this breaks down for
Teq= 600 K. The CO2 abundance shows a sharp in-
crease between thermochemical equilibrium models and
Kzz=106cm2s−1 for these models.
This effect can be
understood with the interplay between the ‘U’ shaped
CO2 equal abundance contours and the T profiles
shown in the right panel of Figure 7. Most of the T
profiles of planets with Teq≥800 K remain in the T −P
space where the CO2 contours are less dense and nearly
vertical. As a result, the equilibrium abundance of CO2
monotonically and slowly increases with decreasing pres-
sure in these objects. Therefore, there is generally not
a large difference between the quenched abundance and
chemical equilibrium abundance of CO2 in these objects.
However, the Teq= 600 K model has a complex shape
with a nearly horizontal region traversing the densely
packed contours, where the chemical equilibrium CO2
abundance decreases rapidly. If the CO2 abundance is
quenched in this region, the quenched abundance will
be significantly higher than expected equilibrium chem-
istry abundance of CO2 in the photosphere. Moreover,
at Teq=600 K, CO2 depends almost solely depends on
Kzz, and not C/O, for Tint≤100 K. Some C/O depen-
dence of the photospheric CO2 abundance sets in for
higher Tint values. This suggests that near Teq=600 K,
the CO2 abundance can be a good tracer of Kzz for plan-
ets with cold interiors  but for planets with
hotter interiors, it can be quite degenerate between Kzz
and C/O.
The Xw for H2O is shown in Figure 8.
The range
of variation in Xw for H2O is much lower when com-
pared with other gases shown in Figures 4, 5, and 6.
H2O shows strong sensitivity to C/O when Teq= 1100
K without much dependence on Kzz. At Teq= 900 K,
when the Tint≥200 K, this behaviour of H2O abun-
dance being only dependant on C/O continues. The de-
pendence of H2O abundance on Tint is also extremely
weak in these cases.
H2O starts to show some Kzz
dependence when Kzz ≥1010cm2s−1 for Tint ≤100
K at Teq= 900 K. This behaviour continues for Tint≤
pendence of H2O abundance sets in a lower value of
Kzz≥109cm2s−1.
The H2O abundance shows an in-
crease with Kzz beyond this value which is also accom-
panied by a sharp decrease in CO and CO2 abundance
seen in this part of the parameter space in Figures 5
and 6, especially at Tint= 30 K. The H2O abundance
shows a decrease between thermochemical equilibrium
and the Kzz=106cm2s−1 chemical disequilibrium calcu-
lations for models with Teq≤800 K. This decrease is par-
ticularly strong for C/O≥1.7×solar for Teq= 800 K mod-
els. This decrease in H2O abundance between thermo-
chemical equilibrium and Kzz=106cm2s−1 is also seen
for Teq=600 K models.
However, the amount of de-
crease seen in these models increases with increasing
Tint at this Teq. This highlights that for these hotter
Tint values  at Teq ≤900 K, H2O can
still be a good tracer of C/O but chemical disequilib-
rium calculations must still be carried out to interpret
the C/O from H2O abundance accurately. The Teq=600
K models shown in the last row of Figure 8 show much
less sensitivity to Kzz compared to the Teq=900 K and
sensitivity shown by CH4 abundance to Kzz in Figure 4
at Teq= 600 K.
Figure 9 shows the Xw metric for NH3. NH3 neither
contains C- or O- and therefore is expected to not de-
pend strongly on C/O. All panels of Figure 9 reflect
this behavior. At Teq= 1100 K, the photospheres are
very NH3 poor. For Teq= 900 K, the photospheric NH3
abundance slowly starts to rise, much more for the colder
Tint models. The vertical equal abundance contours in
Figure 9 show that NH3 is very sensitive to Kzz in ad-
dition to high sensitivity to Tint. Similar to the CH4
behavior seen in the previous section, NH3 abundance
monotonically increases with Kzz. For the coldest inte-
rior of Tint= 30 K at Teq= 900 K or 800 K, NH3 abun-
dance increases by 7 orders of magnitude between Kzz
of 106cm2s−1 and 1013cm2s−1.
The equal abundance
curves for NH3 are known to be nearly parallel to the
adiabats for H2/He gas mixture in the deep atmosphere
the deep convective atmosphere, its photospheric abun-
dance will not be very sensitive to Kzz. NH3 can become
C/O 
Tint= 30 K
Tint= 100 K
Tint= 200 K
Tint= 300 K
Teq = 1100 K
Tint= 500 K
C/O 
Teq = 900 K
C/O 
Teq = 800 K
C/O 
Teq = 600 K
Figure 8. The weighted abundance of H2O calculated using Equation 1 has been shown as a heat map as a function of C/O
and Kzz in each panel. Each row corresponds to a different Teq value from 1100 K to 600 K from top to bottom. Each column
correspond to a different Tint value between 30 K and 500 K from left to right. Note that the C/O has been shown relative to
the assumed solar value of 0.458 in this plot.
more sensitive to Kzz if it is quenched in the radiative
atmosphere in irradiated planets, where the lapse rate
of the T profile is very different than the adiabatic
lapse rate. The photodissociation of NH3 due to the in-
cident UV radiation also depends on Kzz. A low Kzz
in the upper atmosphere causes the NH3 abundance to
be depleted very rapidly with decreasing pressure. The
depletion of NH3 with decreasing pressure is much less if
the Kzz in the upper atmosphere is high. Both of these
effects shape the Kzz dependence of the NH3 abundance
shown in Figure 9 in the pressures typically probed by
transmission spectroscopy.
At Teq≤900 K, the NH3 abundance also declines by
several orders of magnitude as Tint rises from 30 K
to 500 K. Similar to the findings of Ohno & Fortney
photospheric NH3 abundance shows a very sharp rise
with Teq below Teq=800 K when vertical mixing is slow
the photospheric NH3 abundance shows a much more
gradual rise with Teq, as was also found by Ohno & Fort-
ney . Even though detecting NH3 has proven to
be a difficult task so far with JWST , its abundance can be of great use for reducing
the degeneracies discussed in the previous section aris-
ing from just fitting CH4 abundances and measuring Kzz
and Tint, especially for warm planets.
HCN is a molecule which is quenched due to Kzz
as well as produced photochemically.
Moreover, un-
like NH3, it has C- and therefore should be sensitive to
C/O. Figure 10 shows the Xw calculated for HCN. As
expected, photospheric HCN shows sensitivity to both
C/O and Kzz, and also depends on Tint. Figure 10 shows
that the disequilibrium chemistry abundance of HCN is
much higher than the expected thermochemical equilib-
rium abundance. The equal abundance contours in the
top row for Teq= 1100 K run diagonally which shows
that same HCN abundance can be very degenerate cor-
responding to a wide range of C/O ratios and Kzz for
these hotter planets. At Teq= 900 K ,800 K and 600
K, the HCN abundance is mostly sensitive to Kzz and
not much to C/O if Kzz is either low or very high. The
HCN abundance is not sensitive to either Kzz or C/O
for intermediate values of Kzz.
The detection of SO2 in WASP-39b has garnered great
interest in the community due to its photochemical ori-
gin. Figure 3 shows how SO2 is produced in a narrow
pressure region in the upper atmosphere. But as trans-
mission spectra of gas giant planets probes this partic-
ular pressure region as well, SO2 features can be easily
observed in transmission spectroscopy. Figure 11 shows
the Xw for SO2. At all Teq values except for 600 K, SO2
shows a sharp increase from thermochemical equilibrium
to disequilibrium chemistry cases.
For Teq= 1100 K,
the SO2 abundance only depends on C/O and shows a
small dependence on Kzz. For lower C/O values than
increasing Kzz for Kzz ≤1012cm2s−1 followed by a small
decrease for higher Kzz values. The nearly horizontal
equal abundance contours in the top row of Figure 11
shows that the SO2 abundance is much more sensitive
to C/O than Kzz at Teq=1100 K. The SO2 abundance
shows a gradual decrease with increasing C/O, which
is expected as the SO2 molecule contains two O- atoms
and no C- atoms. The decrease of SO2 with increas-
ing C/O is particularly rapid when C/O≥1.7×solar for
these hot planets, similar to CO2. This is again because
of the lack of enough O- atoms to form SO2 in these
very C- rich atmospheres. For these hotter planets, SO2
also remains independent of Tint.
For 800 K≤Teq≤900 K, the SO2 abundance is overall
higher than the Teq=1100 K. The SO2 abundance starts
to show strong dependence on Kzz at this Teq, espe-
cially for high Kzz values. This dependence particularly
sets in when the Kzz is higher than another “transi-
tion” value of Kzz. Below this transition value, the SO2
shows little to no dependence on Kzz, but once the Kzz
is higher than this value, SO2 abundance shows a rapid
decline with increasing Kzz. Figure 11 second row shows
that this transition Kzz depends on both C/O and Tint.
For example, at Teq=900 K, this “transition” Kzz can
be identified to be around 1010cm2s−1 when Tint=30 K.
But this “transition” Kzz is higher at Kzz=1012cm2s−1
for Tint=300 K. Qualitatively similar behavior is also ex-
hibited in the Teq=800 K models. SO2 shows another in-
teresting behavior for Tint≤200 K where the SO2 shows a
rapid increase with Kzz when C/O≥1.5×solar and Kzz
is between 1010-1012cm2s−1. The SO2 abundance then
declines rapidly when Kzz is increased further, which is
the effect discussed just above. This rapid increase of
SO2 with increasing Kzz before showing a decline occurs
for C/O≥1.0×solar in the Teq=800 K models shown in
the third row of Figure 11.
The SO2 abundance in the photosphere of the Teq=
tude than the Teq=800 K models.
This decline is
close to 6-7 orders of magnitude for values of Kzz
lower or higher than a narrow range of Kzz between
row range of Kzz values, the decline is only about by
narrow range of Kzz where SO2 is still relatively more
abundant than the rest of the parameter space, still ex-
ists. However, this region of the parameter space shrinks
progressively along both the C/O and Kzz direction
with increasing Tint.
This shows that below Teq≤600
K, it is very difficult to photochemically produce a de-
tectable amount of SO2, unless the atmospheric metal-
licity is much higher than 10× solar. The depletion of
SO2 at cool Teq is consistent with previous work and
is attributed to the depletion of OH radicals that are
needed to form SO2 
SO2 is not the only important S- bearing molecule in
these atmospheres. Figure 12, upper set of panels, shows
the volume mixing ratio of various S-bearing molecules
as a function of Teq and C/O at a fixed Kzz=109cm2s−1
and Tint=200 K. The volume mixing ratios at 1 mbar are
shown. The right-most panel in the middle row of Figure
declines sharply when Teq≤600 K or C/O ≥2×solar.
The same behaviour is also shown by SO. Figure 12 also
shows that SO is an almost equally abundant product of
photochemistry expected in the same part of the param-
eter space as SO2. Comparing the SO and SO2 panels
in Figure 12 also shows that SO declines slower with
increasing Teq compared to SO2. As both SO and SO2
decline at high C/O≥2×solar, most of the S- in the pho-
tosphere goes into CS instead. With the decline of SO
and SO2 at Teq≤600 K, CS and CS2 also start to carry
a large fraction of the S- inventory. This is particularly
interesting in the context of the recent tentative detec-
tion of CS2 in the atmosphere of TOI-270 d with JWST
which has a Teq between 350-380 K but is inferred to be
much more metal rich than 10×solar.
C/O 
Tint= 30 K
Tint= 100 K
Tint= 200 K
Tint= 300 K
Teq = 1100 K
Tint= 500 K
C/O 
Teq = 900 K
C/O 
Teq = 800 K
C/O 
Teq = 600 K
Figure 9. The weighted abundance of NH3 calculated using Equation 1 has been shown as a heat map as a function of C/O
and Kzz in each panel. Each row corresponds to a different Teq value from 1100 K to 600 K from top to bottom. Each column
correspond to a different Tint value between 30 K and 500 K from left to right. Note that the C/O has been shown relative to
the assumed solar value of 0.458 in this plot.
Among the other Sx kind of molecules S2 seems the
most abundant. Molecules like S3 and S4 are at least
estingly, the abundance of S8 is almost negligible for
Teq≥600 K, but Figure 12 shows that S8 and OCS shows
a very sharp increase in abundance for Teq≤400 K. Fig-
ure 12 shows that among the HxS gases, H2S is more
abundant than HS at 1 mbar, especially for Teq≥1200
K. We note that S- polymerization kinetics is still not
very well understood  and there
is also significant uncertainty in the OCS recombination
rates .
Figure 12, lower set of panels, shows the abundance
of the same gases but at 0.1 bar instead. It is clear that
almost all of the S- is present in the form of H2S with
a minor amount in the form of HS in the deeper atmo-
spheres. The abundance of gases like SO, SO2, and CS,
which are the dominant S- bearers near the pressures
probed by transmission spectroscopy, are much lower
at these deeper pressures.
This is expected as these
gasses are photochemically enhanced at the smaller pres-
sures. Interestingly, even though CS is more abundant
than CS2 in the upper atmosphere probed by transmis-
sion spectroscopy, CS2 appears to be more abundant
molecule in the deeper atmosphere probed by emission
spectroscopy. The most abundant S- carrying gas after
H2S in the deeper atmosphere is HS for Teq≥1000 K.
But below Teq≤1000 K, CS2 becomes the second-most
abundant S- carrying gas after H2S in the deep atmo-
sphere. For Teq lower than 500 K, OCS replaces CS2 as
the second most S- carrying gas after H2S.
and Tint across different metallicities
Metallicity scales the abundance of C-, O-, N-, and S-
relative to H in our modeling framework. §3.1 showed
C/O 
Tint= 30 K
Tint= 100 K
Tint= 200 K
Tint= 300 K
Teq = 1100 K
Tint= 500 K
C/O 
Teq = 900 K
C/O 
Teq = 800 K
C/O 
Teq = 600 K
Figure 10. The weighted abundance of HCN calculated using Equation 1 has been shown as a heat map as a function of C/O
and Kzz in each panel. Each row corresponds to a different Teq value from 1100 K to 600 K from top to bottom. Each column
correspond to a different Tint value between 30 K and 500 K from left to right. Note that the C/O has been shown relative to
the assumed solar value of 0.458 in this plot.
that C/O can be degenerate with Kzz and Tint for most
gases at small or big parts of the parameter space. Here,
we explore how atmospheric metallicity influences pho-
tospheric abundance of gases across the same parameter
space. We use models for a planet with Teq= 800 K for
this purpose with a Tint=200 K.
Figure 13 shows Xw for CH4, NH3, SO2, CO2,
CO, OCS, H2O, and H2S as a function of  and
Kzz in the different panels.
These models are shown
for a very O- rich atmosphere with C/O=0.1× solar
whereas Figure 14 shows the same quantities but for
C/O=1×solar.
The CH4 panel of Figure 13 shows
that at C/O=0.1×solar, increasing Kzz leads to increase
in CH4 when Kzz≤1010cm2s−1.
This holds true at
C/O=1×solar in Figure 14 too. The overall CH4 abun-
dances are much lower in the C/O=0.1×solar than the
C/O=1×solar, as expected. The equal abundance con-
tours for CH4 in the C/O=0.1×solar case 
are nearly vertical which suggests that the photospheric
CH4 abundance varies a lot with changing Kzz, but
not much with . This holds true at C/O=1×solar
as well  but only for ≤+0.5 and
shows that CH4 abundance can depend both on 
and Kzz, increasing as both of these parameters are
increased.
For metallicities higher than =+2.5,
CH4 abundance starts to decrease rapidly with increas-
ing metallicity for the very O- rich case shown in Figure
in the solar C/O models in Figure 14. Note that our
models do not account for graphite saturation , which could draw down the CH4 concen-
tration and impact other carbon-bearing species in Fig-
ure 14 for high metallicities .
NH3 abundance in Figures 13 and 14 also show in-
crease with increasing Kzz. The equal abundance con-
C/O 
Tint= 30 K
Tint= 100 K
Tint= 200 K
Tint= 300 K
Teq = 1100 K
Tint= 500 K
C/O 
Teq = 900 K
C/O 
Teq = 800 K
C/O 
Teq = 600 K
Figure 11. The weighted abundance of SO2 calculated using Equation 1 has been shown as a heat map as a function of C/O
and Kzz in each panel. Each row corresponds to a different Teq value from 1100 K to 600 K from top to bottom. Each column
correspond to a different Tint value between 30 K and 500 K from left to right. Note that the C/O has been shown relative to
the assumed solar value of 0.458 in this plot.
tours of NH3 are along the diagonal direction, which
suggests that the NH3 abundance shows a dependence
on both  and Kzz.
For a given Kzz, higher
lower  values, except for ≥+2.5 models
with C/O=0.1×solar. Both SO2 and CO2 show much
stronger  dependence than Kzz dependence, un-
like CH4 and NH3. This is expected as both SO2 and
CO2 require one C- or S- and two O- atoms per molecule.
It is interesting that for very O- rich atmospheres , the rate of increase of SO2 with increasing
This behavior also happens in the C/O=1×solar atmo-
spheres in Figure 14, but not to the extent present in
Figure 13. The metallicity dependence of SO2 only dis-
sappears when the Kzz is higher than a threshold value.
Beyond this value, the SO2 abundance shows a sharp
decline with increasing Kzz without much  depen-
dence. In Figure 13, this threshold appears to be close to
dependence for high Kzz values, but not to the extent
of the sharp change shown by SO2. For a given ,
the CO2 abundance shows a slight increase with increas-
ing Kzz when the Kzz is higher than 1011cm2s−1. For
lower Kzz values, the CO2 abundance solely depends on
Like CO2, CO shows little to no dependence on Kzz
and strong dependence on metallicity. The main reason
behind this is the relatively higher Tint=200 K of these
models. Interestingly, CO abundances in Figure 13 are
generally lower than the CO abundances in Figure 14.
This is the case as CO contains equal numbers of C- and
O- atoms, so its abundance is diminished in a very O-
rich and C- poor atmosphere. CO also shows some mi-
nor dependance on Kzz in the very O- rich atmospheres
C/O 
C/O 
C/O 
C/O 
C/O 
C/O 
Figure 12. The top set of panels show the abundance of various Sulfur species at 1 mbar as a heat map as a function of
C/O and Teq in each panel. Each panel corresponds to a different S- bearing gas. All models shown here have Tint=200 K
and Kzz=109cm2s−1. The lower panels show the abundance of the same species but at 0.1 bar instead. This is to show the
abundance of S- bearing gases both at pressures probed by transmission and emission spectroscopy, respetively.
Figure 13. Each panel shows how Xw for a different gas varies with  and Kzz as a heat map. All the models shown here
have Teq= 800 K, Tint= 200 K, and C/O= 0.1×solar.
Figure 14. Each panel shows how Xw for a different gas varies with  and Kzz as a heat map. All the models shown here
have Teq= 800 K, Tint= 200 K, and C/O= 1×solar.
ilar behavior as CO, where its abundance is diminished
in the very O- rich/C- poor atmospheres compared to
solar C/O atmospheres. The OCS abundance seems to
primarily depend on atmospheric  unless Kzz is
high. For high Kzz, the OCS abundance starts to in-
crease with increasing Kzz and this increase is particu-
larly rapid for significantly metal-enriched atmospheres.
This behavior is also mimicked by H2S, where its much
less abundant in the O- rich/C- poor atmosphere than
the solar C/O atmosphere. The photospheric abundance
of H2S also shows a rapid increase with increasing Kzz
beyond a threshold value, particularly in high 
atmospheres. Figures 13 and 14 show that H2O abun-
dance in the photosphere only depends on , with
almost no Kzz dependence at all.
Fortney et al.  and Ohno & Fortney 
presented how fundamental chemical transitions in the
atmosphere, like from being CO rich to CH4 rich, can
depend on both Tint and Kzz in addition to Teq. Re-
cently, Bell et al.  reported the discovery of CH4
in the atmosphere of WASP-80b detected in both trans-
mission and emission spectroscopy of the planet with
JWST. There have been efforts to measure the Teq value
below which CH4 first starts to appear in observations of
exoplanetary atmospheres. But this can be a complex
function of Tint, Kzz, C/O, and  .
Figure 15 presents these transitions/behavior from our
photochemical 1D chemical kinetics grid for three pairs
of molecules – CO-CH4, N2-NH3, and H2S-SO2-S at
how CO and CH4 photospheric abundance change with
decreasing Teq for Tint=30 K, 100 K, 200 K, and 500
K from left to right, respectively. Each panel in Fig-
ure 15 shows three colored solid lines denoting the CH4
abundance for three different Kzz values, whereas the
dotted lines indicate the CO abundance in each case.
It is clear that the upper atmosphere probed by trans-
mission spectroscopy can only have CH4≥CO in planets
with cold interiors Tint ∼100 K or lower. For hotter
interiors, CO always remains the majority C- carrier
molecule with higher abundance than CH4. This tran-
sition was seen to be happening at or below Tint=150
K too by Fortney et al. , even though they used
quench-time approximation for approximating the effect
of Kzz on chemistry and also did not treat photochem-
istry in their models.
We note that this threshold of
Tint=150 K was found at 3×solar metallicity in Fortney
et al.  instead of the 10×solar metallicity assumed
in our analysis. Figure 15 also shows that the Teq value
where this transition from CO rich to CH4 rich occurs is
a very strong function of Kzz. When Kzz=1012cm2s−1,
this transition happens near Teq= 1000 K for atmo-
spheres with Tint≤100 K. But, for Kzz=109cm2s−1 or
Kzz=106cm2s−1 , atmospheres can remain CO rich for
Teq above 600 K. When Tint= 500 K, shown in the right
most column of Figure 15, the CH4 abundance is still
an order of magnitude lower than CO even for Teq= 400
K.
The second row of Figure 15 shows the same quan-
tities as the first row but for NH3  and N2
the NH3 can only become the major N- carrier if the Tint
is 30 K. Even for such cold interior, the mixing needs
to be high to make NH3 carry most of the N- atoms
in the photosphere. For example, the NH3 in the pho-
tosphere still remains almost two orders of magnitude
lower than that of N2 abundance if Kzz=106cm2s−1,
even if Tint=30 K. This result is also consistent with the
findings of both Fortney et al.  and Ohno & Fort-
ney . Ohno & Fortney  found that NH3
can become the dominant carrier of N- atoms in giant
planets only in a small parameter space where the planet
has a low mass and is very old, such that the interior is
sufficiently cool and vertical mixing can dredge the NH3
up to the upper atmosphere.
The last row of Figure 15 shows the behavior of the
abundance of three major S- carrying species – H2S
in Figure 15 is that neither SO2 or H2S carries major-
ity of the S- inventory of the atmosphere, not atleast
at =+1.0. Instead, neutral S- atoms form a much
more substantial fraction of the total S- inventory in the
upper atmosphere than SO2. These are shown with the
faded dash dotted lines in all the panels and the same
phenomenon is also shown in Figure 12. This was seen in
previous work  as well. Figure
further. But the decrease of SO2 with decreasing Teq is
much more rapid as was also seen in Figure 12.
While we have presented our results on the detailed
chemical abundances at different parts of the parameter
space till now, here we present how this chemistry affects
the observable – the transmission spectra. The top panel
of Figure 16 shows the transmission spectra computed
from the Teq= 800 K, Tint= 200 K, C/O=1.1×solar, and
Tint= 30 K
Tint= 100 K
logKzz=6
logKzz=9
logKzz=12
Tint= 200 K
Tint= 500 K
logKzz=6
logKzz=9
logKzz=12
logKzz=6
logKzz=9
logKzz=12
Figure 15. The variation of photospheric abundance of key atmospheric gases as a function of Teq is shown here. Each column
shows the variation for a different value of Tint. The top row shows the evolution of CH4 and CO abundance with Teq. The
middle and last row shows the same for N2-NH3, and H2S-SO2-S, respectively. In each panel, the three colored lines shows the
abundances for a three different values of Kzz. The C/O has been fixed to be 1×solar in these models.
It is clear that certain parts of the transmission spectra
are quite sensitive to Kzz while others are not as much.
For example, the CH4 band between 3-3.5 µm, and the
SO2 band at 4µm and 7-10 µm are quite sensitive to
Kzz. On the other hand, the H2O bands between 1-1.5
µm does not show as much sensitivity to Kzz.
The lower panel in Figure 16 shows the sensitiv-
ity of the transmission spectrum to Tint with a fixed
Kzz=1010cm2s−1.
The Tint has been varied from 30
K to 500 K. It is clear that the spectra is very CH4
dominated for cold interiors with Tint=30 K and 100
K. As the Tint increases further, the effect of CH4 on
the spectra slowly decreases. Looking at Figure 16, it
is not exactly clear at how the transmission spectrum
responds to changing Kzz and Tint in different parts of
the vast parameter space explored in this work. There-
fore, we present those trends in the upcoming section by
computing the differential of the transmission spectra at
each wavelength bin with changing Kzz and Tint.
To highlight the wavelength ranges where the trans-
mission spectra changes the most when a parameter
like Kzz is changed, we first renormalise all the trans-
mission spectra across our grid such that the mini-
mum transit depth of each transmission spectra is zero.
Then, we calculate the change in the renormalised tran-
sit depth within each wavelength bin from one grid
point to the next grid point.
For example, to high-
light which part of the transmission spectra changes the
most with a change in Kzz near Tint=30 K, Teq=800 K,
C/O=0.1×solar, =+1.0, and Kzz=108cm2s−1,
dF/dlogKzz=
F)/-log10), where F
de-
notes the renormalized transmission spectra at the
above mentioned grid point parameters. We plot this
quantity dF/dlogKzz in the top left panel of Figure
in Figure 17 shows the same quantity for a different
Tint value while each column corresponds to a different
CO2, H2O
CO2, H2O
Figure 16. The top panel shows the variation of transmission spectra with changing Kzz while the bottom panel shows the
variation of the transmission spectra of the planet with changing Tint. The models shown here are for Teq=800 K, =+1.0,
and C/O=1×solar. The Tint has been fixed at 200 K for the top panel while the Kzz has been fixed at 1010cm2s−1 in the bottom
panel. Note that the transmission spectra have been renormalized in such a manner that the minimum transit depth of each
model is set at zero.
C/O for a fixed Teq= 800 K. The dF/dlogKzz quan-
tity shows how the transit depth in a given wavelength
change in parts-per-million  in a particular part
of the parameter space if Kzz is changed by one order
of magnitude there. We note that the dF/dlogKzz
quantity is sparsely sampled in the Kzz direction, and
as a result the trends shown in Figure 17 and 18 may
appear smoother than expected.
This quantity helps
us to identify which are the best wavelength range or
molecular features to constrain Kzz in a particular part
of the parameter space.
Figure 17 top row shows that if the interior of the
planet has Tint=30 K, the CH4 and CO2 bands near 3.3
µm and 4.2-4.3µm, respectively, show the most sensi-
tivity to changing Kzz.
However, it is clear that the
maximum change on the spectra due to changing Kzz
appears at different Kzz values depending on the C/O.
For example, in the C/O=0.1×solar panel on the top
left, the maximum change in the CH4 feature appears
at high Kzz values near 1010cm2s−1.
This region of
maximum sensitivity of the CH4 band shifts to lower
Kzz=108cm2s−1 value when the C/O is 1×solar and to
even lower Kzz= 107cm2s−1 when the C/O is 2×solar
in the top row. However, Figure 17 top row shows that
the CO2 band near 4.2-4.3 µm remains quite sensitive
to Kzz for almost all Kzz values in this cold interior
scenario.
As the Tint is increased from 30 K to 500 K , the
sensitivity of the CH4 bands to Kzz slowly diminishes
while the SO2 bands near 4µm and 7-9 µm become in-
creasingly sensitive to changing Kzz. This happens only
in the C/O = 0.1× and 1×solar columns. The sensitiv-
ity of the SO2 bands set in particularly for high values
of Kzz above 1010cm2s−1. The rest of the transmission
spectra still remains sensitive to Kzz but to a smaller
degree.
When C/O=2×solar, the CH4 bands remain
the most sensitive parts of the transmission spectra till
Tint≤300 K. The sensitivity of the 4.2-4.3 µm CO2 slowly
diminishes in this case as the interior warms up from a
Tint of 30 K to 500 K.
Tint = 30 K
C/O= 0.1×Solar
C/O= 1×Solar
C/O= 2×Solar
Tint = 100 K
Tint = 200 K
Tint = 300 K
Tint = 500 K
Figure 17.
Sensitivity of the transmission spectra to changing Kzz is shown here as a function of Kzz and wavelength. Each
column corresponds to a different C/O value whereas each row shows models for different Tint values. All models shown here
have Teq=800 K. A brighter red or a brighter blue color indicates that the transmission spectra is very sensitive to changing
Kzz at that Kzz and wavelength value.
Next, we present the trends in dF/dlogKzz at three
different Teq values in Figure 18, instead of the three dif-
ferent C/O values shown in Figure 17. Each column in
Figure 18 shows the trends for Teq= 600 K, 800 K, and
less sensitive to Kzz than those at Teq= 800 K and Teq=
spectrum arises in the CO2 bands  and CH4
bands  at low values of Tint. The
CH4 bands show some sensitivity but only when Kzz is
low whereas the CO2 band shows sensitivity for a larger
range of Kzz values. When the Tint ≥300 K, there is
reduced sensitivity to Kzz across the transmission spec-
trum at Teq=600 K.
The trends in the middle column of Figure 18 and
the middle panel of Figure 17 represent the same set
of models, so we don’t describe them again here. The
Teq=1100 K models shown in the right columns of Fig-
ure 18 show that the CH4 bands  are particularly sensitive to Kzz, especially at
high Kzz values but this sensitivity diminishes slowly
with increasing Tint value. The CO2 feature 
shows high sensitivity to Kzz across all Kzz values, but
this sensitivity slowly diminishes with increasing Tint at
Teq=1100 K. While the 4 µm SO2 feature doesn’t show
much sensitivity to Kzz for this Teq value, but the 7-9
µm SO2 is very sensitive to Kzz throughout all Tint and
Kzz values for the Teq=1100 K case. As it is not possi-
ble to present these trends for the whole suite of models
within this paper in a wavelength resolved manner, we
now move on to showing sensitivity of the transmission
spectra to changing Tint instead of changing Kzz.
Figure 19 presents the sensitivity of the transmission
spectra to changing Tint when the Tint changes within
our five Tint grid points of 30 K, 100 K, 200 K, 300 K, and
unlike for Kzz, we plot the quantity dF here, which is
simply the difference in the renormalized transit depths
Tint = 30 K
Teq= 600 K
Teq= 800 K
Teq= 1100 K
Tint = 100 K
Tint = 200 K
Tint = 300 K
Tint = 500 K
Figure 18. Sensitivity of the transmission spectra to changing Kzz is shown here as a function of Kzz and wavelength. Each
column corresponds to a different Teq value whereas each row shows models for different Tint values. All models shown here
have C/O=1×solar. A brighter red or a brighter blue color indicates that the transmission spectra is very sensitive to changing
Kzz at that Kzz and wavelength value.
between two Tint values. Like Figure 17, each column
of Figure 19 also represents different C/O values. But
unlike Figure 17, each row of Figure 19 represents a
different value of Kzz with Kzz increasing from the top
to the bottom row. It is clear that for the very O- rich
case of C/O=0.1×solar shown in the left columns, the
sensitivity of the transmission spectra is very subtle to
changing Tint, especially for low Kzz values. But as the
Kzz values increase, the CH4 band between 
starts to show high sensitivity to changing Tint.
The C/O=1×solar models shown in middle column
of Figure 19 show a relatively higher level of sensitivity
to changing Tint value compared to the C/O=0.1×solar
models, especially for Kzz≥109cm2s−1. Most of the sen-
sitivity here is shown in the CH4 bands  and the sensitivity of the CH4 bands to chang-
ing Tint increases with increasing Kzz at this C/O value.
The CO2 absorption bands also show high sensitivity to
changing Tint values, especially for higher Kzz values.
For the C/O=2×solar models shown in Figure 19, most
of the sensitivity of the transit depth is mainly seen in
the CH4 bands  and the the CO2
band between 4.2-4.3 µm.
Figure 20 shows the sensitivity of the transmission
spectra at three different Teq values shown in each col-
umn for 600 K, 800 K, and 1100 K from left to right,
respectively. The C/O ratio has been fixed to 1×solar
for this purpose.
It is clear that the overall sensitiv-
ity of the transmission spectra to changing Tint dimin-
ishes with increasing Teq.
For Teq=600 K, the CH4
and CO2 bands show the most sensitivity to changing
Tint. This sensitivity increases progressively from low
Kzz to high Kzz at Tint=600 K. The Teq=1100 K mod-
els shown in the right column show much lower sensi-
tivity of the transit depths to changing Tint than the
Teq=600 K and Teq=800 K models. This is especially
true for Kzz≤109cm2s−1. However, when Kzz is higher
than this value, some Tint sensitivity starts to appear in
the spectrum. This sensitivity appears primarily in the
logKzz = 6
C/O= 0.1×Solar
C/O= 1×Solar
C/O= 2×Solar
logKzz = 8
logKzz = 9
logKzz = 11
logKzz = 13
Figure 19. Similar to Figure 17 but the sensitivity of transmission spectra to changing Tint is shown here as a function of
Tint and wavelength, instead of Kzz in Figure 17. Each column corresponds to a different C/O value whereas each row shows
models for different Kzz values. All models shown here have Teq=800 K. A brighter red or a brighter blue color indicates that
the transmission spectra is very sensitive to changing Tint at that Tint and wavelength value.
CH4, CO2 and CO bands  in the Kzz=1011
and 1013cm2s−1 cases.
Our sensitivity analysis of the transmission spectra re-
veals that the different wavelength regions in the trans-
mission spectra show complex sensitivity with changing
Kzz and Tint in different parts of the C/O, Teq, Kzz, and
Tint parameter space. This suggests that the precision to
which parameters like Tint and Kzz can be constrained
for giant transiting planets depends on where that planet
lands in this multi-dimensional parameter space. This
kind of analysis can also be used to carefully design
the wavelength region and the required signal-to-noise
to achieve a certain science goal  with instruments like JWST. We
avoid presenting similar analysis for eclipse spectroscopy
as we chose to ignore the disequilibrium chemistry pro-
cesses self-consistently within our T profile calcula-
tion and that would matter for emission spectra of these
planets to a much larger extent than the transmission
spectra analysis presented here.
Photolysis of C- bearing gases  in the up-
per atmospheres of gas giants can lead to formation of
gases like C2H2, C2H4, C2H6, etc, which in turn can act
as precursors to haze/soot creation, if the conditions are
favorable to further polymerization of these gases .
polymerization can be enabled by the reducing nature
of the upper atmosphere of these planets.
Hazes can
have large impact on the observed transmission, reflec-
tion, and emission spectra of exoplanets . While the Pho-
tochem kinetics model doesn’t track the growth of these
hydrocarbons to large polymers, we can still quantita-
tively estimate the abundances of these haze precursors
across the vast parameter space explored in this work.
We adopt the methodology presented in Morley et al.
sors above 10−3 bars.
Following Morley et al. 
logKzz = 6
Teq= 600 K
Teq= 800 K
Teq= 1100 K
logKzz = 8
logKzz = 9
logKzz = 11
logKzz = 13
Figure 20. Sensitivity of the transmission spectra to changing Tint is shown here as a function of Tint and wavelength. Each
column corresponds to a different Teq value whereas each row shows models for different Kzz values. All models shown here
have C/O=1×solar. A brighter red or a brighter blue color indicates that the transmission spectra is very sensitive to changing
Tint at that Tint and wavelength value.
and Tsai et al. , we include the following C- bear-
ing molecules to estimate the abundance of haze/soot
precursors– C2H2, C2H4, C2H6, C4H2, HCN, CH3CN,
and CS2. We calculate the column density of these gases
at pressures smaller than 10−3 bars across our model
grid.
Each panel of Figure 21 shows the logarithm of the
column density of these precursors as a function of C/O
and Kzz. Similar to Figure 4, each column represents a
value of Tint while each row represents a different value
of Teq. Figure 21 shows that the abundance of haze pre-
cursors can be very strongly dependant on Kzz in plan-
ets with Tint≤100 K. This sensitivity of the haze pre-
cursor column density to Kzz diminishes with increas-
ing Tint at all Teq values. For Teq≤900 K models with
Tint=30 K, the abundance of haze precursors increases
first as a function of increasing Kzz before reaching a
peak, beyond which it decreases with increasing Kzz,
for a fixed C/O. The value of Kzz where this peak in
haze precursor abundance occurs is a strong function of
Teq.
For Teq=1100 K and Tint=30 K, this peak hap-
pens at or beyond Kzz=1013cm2/s, whereas at Teq=800
K and Tint=30 K, this peak occurs at Kzz=1010cm2/s.
At Teq=600 K, the peak occurs near Kzz=108cm2/s,
when Tint=30 K. Similar to the findings of Fortney et al.
dance of haze precursors shows an increase with de-
creasing Teq between Teq=1100 K and Teq= 900 K for
a fixed C/O and Kzz.
Below Teq=900 K, for models
with Tint≤100 K, whether the column density of pre-
cursors increases or decreases with decreasing Teq de-
pends largely on the Kzz and C/O. However, when the
Tint≥200 K, the column density of the precursors do not
show much change with decreasing Teq below Teq= 900
K, for a given C/O and Kzz value.
Above Tint≥100 K, C/O starts to influence the col-
umn density of haze precursors significantly, in addition
to Kzz. Figure 21 shows that the influence of C/O on
the precursors becomes stronger with increasing Tint for
all Teq. For planets with Tint ≥300 K, the precursor
C/O 
Tint= 30 K
Tint= 100 K
Tint= 200 K
Tint= 300 K
Teq = 1100 K
Tint= 500 K
C/O 
Teq = 900 K
C/O 
Teq = 800 K
C/O 
Teq = 600 K
Figure 21. The column density of haze precursors at pressures smaller than 10−3 bars has been shown as a heat map as a
function of C/O and Kzz in each panel. Each row corresponds to a different Teq value from 1100 K to 600 K from top to bottom.
Each column correspond to a different Tint value between 30 K and 500 K from left to right. The abundance of gases– C2H2,
C2H4, C2H6, C4H2, HCN, CH3CN, and CS2 were used to compute the column density of the haze precursor molecules.
column density shows a sharp increase when C/O ≥
teriors, the abundance of the precursors mostly remain
independent of Kzz. For a given value of C/O and Kzz,
Figure 21 also shows that the abundance of the precur-
sors show an overall decline with increasing Tint for all
Teq values.
This can be seen in the overall fading of
the brightness of the panels in Figure 21 as one goes
from left to right.
Figure 21 proves that the precur-
sors to larger haze molecules in the upper atmosphere
of H2/He rich atmospheres show very complex depen-
dence on Kzz, Tint, C/O, and Teq.
For planets with
hotter interiors both C/O and Kzz influence the avail-
ability of haze precursors in the upper atmosphere, but
for planets with low Tint, Kzz almost solely influences
the column density of these precursors in the upper at-
mosphere. This suggests that for low mass planets at
old ages, C/O might not be very correlated to the abun-
dance of hazes in their atmospheres.
However, these
results are only for 10×solar metal enrichment.
Figure 22 shows the how the column density of the
haze precursors vary with metallicity and Kzz.
top rows show models with Tint=30 K while the bot-
tom row shows models with Tint= 200 K. The left and
right columns show models at C/O= 0.1×solar and
C/O=1×solar, respectively.
All the models shown in
Figure 22 are for Teq=800 K. Figure 22 shows that
the amount of haze precursors in the upper atmosphere
generally depends both on  and Kzz, to varying
degrees, in all the four panels. When Tint=30 K and
C/O=0.1×solar , the amount of haze
precursor starts to show some  dependence when
the Kzz is between 109 and 1012cm2s−1. At Kzz val-
ues lower or higher than this range, the abundance of
haze precursors shows strong dependence on Kzz. For
C/O=1×solar and Tint=30 K models shown in the top
Tint= 30 K
C/O=1×solar
Tint= 200 K
Figure 22. The column density of haze precursors at pressures smaller than 10−3 bars has been shown as a heat map as a
function of  and Kzz in each panel. The top row corresponds to Tint= 30 K, while the bottom row shows models with
Tint= 200 K. The Teq has been set to 800 K for all the panels. The left column shows models for C/O=0.1×solar while the right
column shows models at C/O=1×solar. The abundance of gases– C2H2, C2H4, C2H6, C4H2, HCN, CH3CN, and CS2 were used
to compute the column density of the haze precursor molecules.
right panel, the abundance of haze precursors show
Kzz outside this range, the haze precursor column den-
sity becomes very sensitive to mixing with little to no
dependence on . When the Tint is higher , the sensitivity of the haze precursors to 
and Kzz are also different.
At Tint=200 K, the C/O=0.1× cases show depen-
dence of the precursor column density to  when
But even when Kzz lies in
between these values, the variation of the precursor
abundances with changing  is much less rapid in
this hotter interior model compared to the top panel.
However, outside this range of Kzz values, the precur-
sor show strong dependence on Kzz. When the atmo-
sphere becomes more C- rich in these hotter interior
models , the haze precursor abun-
dance shows greater metallicity dependence. For these
models, the haze precursors become sensitive to Kzz for
Kzz≥1012 cm2s−1 and Kzz≤108 cm2s−1.
The maximum abundance of haze precursors occurs
between +1.0≤≤+1.5 for the top left panel, while
the bottom left panel shows maximum precursors be-
tween +0.5≤≤+1.5. For the top and bottom right
panels, maximum precursor abundances are reached be-
tween +1.3≤≤+2.5 and +1.5≤≤+2.0, re-
spectively. The top right panel, with C/O=1×solar and
Tint= 30 K, also shows that the maximum haze pre-
cursor abundance extends to higher metallicities , if Kzz is high. All the panels show that
the haze precursor abundances can be very low if mixing
is not vigorous enough. Comparing the top to the bot-
tom panel in both the columns of Figure 22 shows that
planets with lower Tint show enhanced levels of haze pre-
cursors at all , Kzz, and C/O values compared to
planets with hotter interiors. Both Figure 21 and Figure
planets at older ages might have a higher haze content
than higher mass planets. However, this only holds true
if their interiors have not been heated through external
perturbations.
Figure 2 has already established that there can be sig-
nificant differences in the T structure calculated us-
ing RCTE and RCPE models. This has been shown in
previous studies as well .
This self-
consistency between disequilibrium chemistry and T
profiles has been found to be important for atmospheric
models used to interpret the emission spectra of brown
dwarfs and directly imaged planets .
For the
high signal-to-noise spectra of these objects, these at-
mospheric details matter.
However, we have ignored the self-consistent approach
for the bulk of this work as it is not expected to matter
much in a parameter space exploration and identifica-
tion of major trends in chemistry, which is the main
focus of this work. The correction of T profile due
to RCPE calculations relative to RCTE models will typ-
ically be dependant on Kzz. Figure 23 shows the T
profiles computed with RCPE modeling for three differ-
ent Kzz values for a Teq=800 K object with =+1.0
and solar C/O. We calculate these RCPE profiles for
two Tint values of 100 K and 500 K, which are shown
in Figure 23 along with the RCTE T profiles com-
puted in each case. The chemical abundances in each
model for CH4, NH3, and SO2 are also shown.
It is
clear that the correction on the T profiles relative
to RCTE models is quite a strong function of both Kzz
and Tint. For example, the absolute δT in the Tint=500
K cases are larger but not very Kzz dependant, whereas
the absolute δT is relatively lower but much more Kzz
dependant in the Tint=100 K. Moreover, we also find
that the RCPE models have slightly different radiative-
convective boundaries from the RCTE models too. This
sensitivity can have modest effects on the absolute abun-
dances presented in this work but does not appear to be
strong enough to alter the trends presented here.
We have also made this simplification because here we
only explore the effect of the planetary parameters on
transmission spectra of planets. Drummond et al. 
has shown that the effect of the difference between
the RCTE and RCPE calculations is much stronger for
eclipse spectroscopy. Figure 23 reiterates this point and
suggests that RCPE models should be used when eclipse
spectroscopy data of warm and cold transiting planets
are interpreted.
UV spectra of host stars
Figure 3 shows that photochemistry shapes a signifi-
cant portion of the abundance profiles of gases like CH4
and SO2 in/near the pressures typically probed by trans-
mission spectroscopy. Therefore, another planetary pa-
rameter which can be influential on the photospheric
abundances of gases in irradiated giant planets is the
host star X-ray/UV spectra incident on the planetary
atmosphere.
The black-dashed line in the bottom panel of Figure
ter space exploration presented in §3. The other colored
lines show the same UV spectra multiplied with different
factors of 100×, 10×, 0.1×, and 0.01×. The top pan-
els in Figure 24 show the chemical abundance profiles
calculated for the same planet but with these different
incident UV fluxes on the planet. We use the Teq= 800
K, Tint= 300 K, Kzz= 106cm2s−1, and C/O= 1.1×solar
case for all results presented in this section. Abundance
profiles for SO2, CH4, NH3, and CO2 are shown from
top left to right, respectively. It is clear that changes in
the whole UV flux by five orders of magnitude does not
cause the peak SO2 abundance to change much, but in-
stead has a significant impact on the pressure where it is
produced, which can change by two orders of magnitude.
For CH4 and NH3, that same change in UV flux can
cause the pressure at which they are photochemically
depleted to change by a factor of 10. On the other hand,
the UV flux has almost no effect on the CO2 abundance
profiles. The middle panels in Figure 24 show the effect
of UV flux on the abundances when Kzz=1010cm2s−1.
The effect of the UV flux on the abundances are more
pronounced in this case compared to the lower Kzz case
shown in the top panel. Additionally, CO2 shows much
stronger variation in the upper atmosphere with chang-
ing UV flux when the mixing is strong. This exercise
shows that having constraints on the overall flux levels
of UV photons incident on giant planets is important for
interpreting abundances of some gases like SO2 or CH4
from observations.
We further explore the sensitivity of atmospheric
chemistry to different wavelength regions of the UV
spectrum. We take the nominal UV spectra from Fig-
ure 24 and divide it into a large number of wavelength
bins. Then, we boost the flux at each wavelength bin
by a factor of 100× and recalculate the chemistry while
keeping the rest of the fluxes in other wavelength bins
the same. Figure 25 shows the results from this exercise.
Tint= 100 K
logKzz=9
logKzz=11
logKzz=13
Tint= 500 K
logKzz=9
logKzz=11
logKzz=13
logKzz=9
logKzz=10
logKzz=11
Figure 23. Difference between the T profiles calculated using the RCPE model for three different Kzz values and the RCTE
model is shown in the top left panel for a Tint=100 K model. The same differences are shown for a Tint=500 K model in the
top right panel. The three bottom left panels show the abundance profiles for each model for CH4, SO2, and NH3 at Tint=100
K. The same quantities for the Tint= 500 K model are shown in the three bottom right panels. All the models shown here are
for Teq=800 K, =+1.0, and C/O=1×solar.
The nominal UV spectra are shown in the three panels
in light gray, and the many vertical lines at the bottom
of each panel depict the wavelength bins that we use.
Each panel shows the quantity,
Y = log
X)
of a gas X calculated when the UV flux at wavelength
w has been boosted by a factor of 100. X) is the
abundance of the same gas calculated using the nominal
UV flux. When Y in Equation 2 is greater/lower than
zero, it means that the boost in UV flux in that wave-
length bin causes an increase/decrease in the abundance
of molecule X relative to the abundance calculated using
the nominal UV flux.
The top left most panel in Figure 25 shows Y for SO2.
It shows that the SO2 abundance near 0.1 to 3 mbar
shows a decrease if the UV flux between 200 to 240 nm
is boosted by a factor of 100. This sensitivity is due to
the high UV cross-section of SO2 between 200-240 nm
shown in Figure 26 in Appendix §7. It also shows that
the SO2 abundance is slightly enhanced between 1-10
mbar if the UV flux near 350-400 nm is boosted. The
SO2 also shows a small increase near the 0.1 mbar level
if the UV flux between 300-340 nm is increased. Both
of these enhancements in SO2 are linked with the addi-
tional destruction of S2 and CS2 when the flux between
tom left and bottom middle panels of Figure 25. This
enhanced destruction of S2 and CS2 is due to the strong
UV cross-sections of S2 and CS2 between 300-400 nm
and 300-350 nm, respectively. Interestingly, the Y map
for H2O in the bottom left panel shows the opposite
behavior than SO2 when fluxes in the same wavelength
ranges are boosted. But this is not visible in the plot as
we choose to keep the same color scale across all pan-
els in Figure 25 so that the sensitivity of each gas to
changing UV flux can be compared with the rest.
The top middle panel in Figure 25 shows Y for CH4.
CH4 abundance shows a small decrease near 0.1-3 mbar
region when fluxes between 350-400 nm are boosted.
SO2 VMR
F×0.01
F×0.1
F×1
F×10
F×100
CH4 VMR
F×0.01
F×0.1
F×1
F×10
F×100
NH3 VMR
F×0.01
F×0.1
F×1
F×10
F×100
log=6
CO2 VMR
F×0.01
F×0.1
F×1
F×10
F×100
F×0.01
F×0.1
F×1
F×10
F×100
F×0.01
F×0.1
F×1
F×10
F×100
F×0.01
F×0.1
F×1
F×10
F×100
log=10
F×0.01
F×0.1
F×1
F×10
F×100
F×0.01
F×0.1
F×1
F×10
F×100
Figure 24. The upper set of panels show the change in abundance profile of key molecules due to change in the X-ray/UV flux
incident on the planet when Kzz=106cm2s−1. Profiles for SO2, CH4, NH3, and CO2 are shown from left to right, respectively.
The black-dashed line shows the profile for the nominal case. The middle row compares the abundances when Kzz=1010cm2s−1.
The lower panel shows the X-ray/UV spectrum used to calculate each of these models.
CH4 also shows a sharp decrease near the 0.1-0.6 mbar
level when flux between 300-350 nm is increased by a
factor of 100. Like SO2, NH3 abundance also shows a
sharp decrease between 0.1-3 mbar when flux between
UV cross-section of NH3  between 200-
when UV fluxes at wavelengths higher than 300 nm are
boosted. Comparing Figure 24 and Figure 25 shows that
determining the level of UV flux incident on the whole
planet in very broad wavelength bands is much more
important than determining the very detailed nature of
the UV SED to interpret the chemical abundances in
irradiated planet atmospheres and connecting them to
planetary bulk properties. However, we note that pho-
tochemical kinetics calculations are highly non-linear in
nature and our calculations have several simplifications.
For example, a more comprehensive radiative–transfer
approach like Monte–Carlo radiative transfer and T −P
dependant UV cross-sections might provide additional
insights.
Our photochemical model does not track alkali met-
als and other condensibles relevant to the deep portions
of gas-giant atmospheres. In reality, species like Na2S
and MnS, which we do not model, should condense at
relatively high pressures and temperatures which may
prevent some amount of sulfur from reaching the up-
per atmosphere .
Therefore,
our omission of these species should cause our simula-
tions to over-predict the abundance of sulfur gases  in the upper atmosphere. However, we expect our
Figure 25. This plot shows how the abundance of a certain gas changes at a given pressure when the UV flux at a given
wavelength bin is increased by a factor of ×100. This has been quantified with Y in Equation 2. This Y quantity has been
plotted as a heat map in each panel for SO2, CH4, NH3,H2O, CO2, SO, S2, CS2, and HCN. The X-axis in each panel is the
wavelength of the UV flux and the Y-axis is pressure in the atmosphere. The wavelength bins used for boosting the UV fluxes
are also shown as vertical lines at the bottom of each panel along with the nominal UV spectra incident on the planet. A positive
value for Y at a given pressure and wavelength means that the molecular abundance has increased relative to the nominal case
when UV flux at those wavelengths were boosted. A negative Y value corresponds to a decrease instead.
over-prediction to be small and inconsequential, because
sulfur should be far more abundant than the alkali met-
als. For example, the Sun has ∼10× more sulfur atoms
than sodium atoms , meaning Na2S
condensation could, at most, only sequester about 5% of
the available sulfur. In summary, we expect the lack of
condensibles in our photochemical model to cause our
results to over-estimate the sulfur in the upper atmo-
sphere by an amount that does not effect our overall
conclusions which is based on order-of-magnitude trends
in sulfur bearing species.
To achieve a photochemical steady-state, the Pho-
tochem code integrates the photochemical equations for-
ward in time until the atmosphere ceases to change. Our
code identifies convergence to such a steady-state with
the following criteria:
fij −fij
tn/2
is the current integration time, making ∆ymax the max-
imum relative change over the last tn/2 seconds. We
assume a steady-state is achieved when ∆ymax < 0.05
and ∆ymax
appropriate over a wide parameter space. Furthermore,
the VULCAN photochemical code  uses
similar convergence requirements that have proved reli-
able for warm gas-rich planets ).
Photochem, like many other chemical kinetics models
mospheric chemistry in a grid of atmospheric altitudes
instead of the pressure space used in the PICASO climate
model. As a result, as the chemistry of the atmosphere
evolves in the chemical kinetics model, the mean molecu-
lar weight of the atmosphere also changes. So, when the
altitudes are converted back to pressures after the kinet-
ics models have converged, the T profile effectively
becomes ever so slightly different than the input T
profile. To avoid this, the Photochem code adaptively
re-grids and adjusts the temperature profile during inte-
gration as to ensure the T profile matches the input
PICASO T profile. This approach is particularly very
important for the RCPE models we have presented in
this work, where the chemical kinetics model is itera-
tively coupled directly with the climate model.
We have explored the variation of atmospheric chem-
istry in close-in giant transiting exoplanets across a vast
parameter space of bulk planetary properties like Teq,
Tint, , and C/O. Our exploration includes chemi-
cal disequilibrium processes such as vertical mixing and
photochemistry and is applicable for a wide variety of
planets in terms of their mass, age, and proximity to
their host stars. We have arrived at the following con-
clusions from this exploration,
dances of gases like CH4 can be sensitive to just
the Kzz parameter or just the C/O ratio or can
also be quite degenerate between the two depend-
ing on the bulk properties of the planet like Tint or
Teq and also atmospheric properties like the vigor
of vertical mixing. We have identified these trends
in this work at different parts of the parameter
space.
C/O ratio and remains largely independent of Kzz,
unless the Tint of the planet is ≤100 K. For these
cold interior planets, CO can become much better
tracers of Kzz than C/O.
Kzz if the Teq≤900 K. This sensitivity is partic-
ularly enhanced for cold interiors. We also show
that NH3 is an excellent tracer of Kzz, without
much dependence on C/O, as expected.
on the other
hand depends quite strongly on the C/O of the
planet unless the Kzz exceeds high values like ∼
shows a very sharp decline once the Teq≤600 K.
For these colder Teq planets, we find that the
S- content of the photosphere is carried by other
gases like CS, CS2, and S8.
We also find that
atomic S is a major carrier of S- near the photo-
sphere irrespective of Teq.
gases in the atmosphere behave in the  vs.
Kzz space. Among other trends, we find that other
S- carriers like OCS and H2S can show a rapid
increase with Kzz in the photosphere, especially
for metal enriched objects.
in the atmosphere with Teq, we explore the Teq vs.
abundance trends of gas combinations like CO-
CH4, NH3-N2, and SO2-H2S-S. Our findings on
these key transitions reflect the findings from pre-
vious work . Interestingly, we find that even
in the photosphere, atomic S can carry a signifi-
cant fraction of the S- inventory along with H2S
and SO2.
sion spectra to parameters like Kzz and Tint in
different parts of the parameter space, we present
a wavelength resolved sensitivity analysis of the
transmission spectra to these parameters. We find
that the sensitivity of the transmission spectra to
these parameters is quite complex. In some parts
of the parameter space, the CH4 absorption bands
might show great sensitivity to Kzz, while in some
parts the CO2 or SO2 bands might show the high-
est sensitivity to changing Kzz or Tint.
molecules like C2Hx, HCN, etc, which can act as
precursors to photochemical hazes in the upper at-
mosphere. We find that these precursors molecules
show very strong dependence on Kzz and almost
no dependence on C/O for planets with cold interi-
ors  at 10×solar metallicity. We also
find that planets with low Tint are likely to have
significantly higher abundance of haze precursors
in the upper atmosphere compared to planets with
hotter interiors. This is especially relevant to older
low mass planets with no external heating.
There are several aspects of our modeling of giant
planet atmospheric chemistry which can be improved
in the near future. These include inclusion of more ele-
ments in our chemical kinetics calculations and inclusion
of condensation and rainout chemistry within our chem-
istry calculation as well.
We think that the assump-
tions made in our modeling approach are sufficient for
the typical quality of transmission spectra obtained with
JWST, especially in the near-future. But these kind of
improvements might lead to new avenues of character-
izing the physics and chemistry of giant planet atmo-
spheres and also their bulk properties.
SM acknowledges the Templeton TEX cross-training
fellowship for supporting this work. JJF acknowledges
the support of JWST Theory Grant JWST-AR-02232-
ENHI Grant Number JP23K19072. N.W. was supported
by the NASA Postdoctoral Program. We acknowledge
use of the lux supercomputer at UC Santa Cruz, funded
by NSF MRI grant AST 1828315.
Software:
PICASO 3.0 ,
PICASO , pandas ,
NumPy , IPython , Jupyter , matplotlib , the model grid will be formally released via Zen-
odo.
Here we present the UV cross-sections of key gases
used in the Photochem chemical kinetics model. Figure
major molecules. Each panel shows the cross-section for
a different molecule.","[('kzz', 319), ('tint', 311), ('teq', 253), ('abundance', 208), ('show', 194), ('planet', 157), ('figure', 156), ('model', 120), ('atmosphere', 104), ('shown', 88)]","[(('tint', 'tint'), 80), (('figure', 'show'), 57), (('tint', 'teq'), 54), (('teq', 'tint'), 45), (('kzz', 'cm'), 41), (('kzz', 'kzz'), 40), (('parameter', 'space'), 34), (('transmission', 'spectrum'), 34), (('teq', 'teq'), 31), (('kzz', 'value'), 27)]","[(('tint', 'tint', 'tint'), 53), (('tint', 'teq', 'tint'), 24), (('teq', 'tint', 'teq'), 19), (('kzz', 'kzz', 'kzz'), 18), (('internal', 'heat', 'flux'), 14), (('logkzz', 'logkzz', 'logkzz'), 14), (('tint', 'tint', 'teq'), 13), (('equal', 'abundance', 'contours'), 12), (('teq', 'teq', 'teq'), 11), (('heat', 'map', 'function'), 10)]"
2410.17176v1.txt,"Opacity of Ejecta in Calculations of Supernova
Light Curves
M.Sh. Potashov1,2,4*, S.I. Blinnikov1,2,3,4,**, and E.I. Sorokina2,5
Abstract—The plasma opacity in stars depends mainly on the
local state of matter , but in supernova ejecta
it also depends on the expansion velocity gradient, because the
Doppler effect shifts the spectral lines differently in different
ejecta layers. This effect is known in the literature as the
expansion opacity. The existing approaches to the inclusion of
this effect, in some cases, predict different results in identical
conditions. In this paper we compare the approaches of Blinnikov
to calculating the opacity in supernova ejecta and give examples
of the influence of different approximations on the model light
curves of supernovae.
Keywords supernovae · expansion opacity · light curves ·
radiative transfer
Radiative transfer is of paramount importance in a high-
temperature plasma. It is especially important in astrophysics,
because most of the data on the Universe are obtained from
radiation. The optical plasma properties determine the interac-
tion of matter with radiation and are an important part of any
problem with radiative transfer. The interaction of matter with
radiation is characterized by the opacity . The opacity itself gives an idea of the atomic
and ionic structure of materials. In some cases, bound–bound
transitions in ions create a thick “forest” of spectral lines that
contribute significantly to the opacity. It is not easy to calculate
this contribution, and additional physical effects  can make this calculation more
difficult. In moving matter all line frequencies are shifted due
to the Doppler effect. The light emitted in the rest frame
interacts with a moving plasma with the absorptivity calculated
at the shifted frequency ∆ν. If the matter moves nonuniformly
necessary to sum the contributions of different lines at different
points on the path of the light in the plasma.
By the “velocity gradient” we mean the spatial derivative of
the velocity component along the general expansion direction
dr ≡∂vr
at the stage of free expansion with kinematics v = r/t,
i.e., the velocity gradient is equal to the reciprocal of the
time after explosion. The plasma opacity can depend on the
velocity gradient, because the Doppler effect shifts the spectral
lines differently in different layers. For the cases where there
are many bound–bound transitions, i.e., a large number of
lines contribute to the opacity, the latter is enhanced when
the plasma expands with a nonuniform velocity field. The
expansion opacity approximation, whose interpretation still
remains debatable, was introduced to perform calculations
in such situations. The problem of a proper approximation
to describing the absorption and scattering of radiation in a
plasma moving with a velocity gradient was considered in
a number of papers. Several approaches to calculating the
expansion opacity are described in the well-known book by
Castor , in particular, those from Friend and Castor
et al. , and Wehrse et al. . A puzzling fact is that
in some situations the photon mean free paths calculated by
different methods differ by orders of magnitude. In this paper
we will check the differences for the expansion opacity from
Friend and Castor  and Eastman and Pinto , on
the one hand, and Blinnikov , on the other hand. Below
we will denote the models computed using the approach of
Friend and Castor  and Eastman and Pinto  by the
index E, because it is based in part on heuristic considerations.
The second, Blinnikov’s approach will be denoted by the index
arXiv:2410.17176v1    22 Oct 2024
Opacity of Ejecta in Calculations of Supernova Light Curves
H, corresponding to the Hilbert expansion used in Blinnikov
We will also consider the influence of different opacity
parameters on the predicted light curves of supernovae ,
other things being equal.
THE LIMITING CASES OF STRONG AND WEAK LINES
In this section we will show that the expressions for the
expansion opacity derived by Blinnikov  are reduced
to the Friend–Castor  formulas in two
limiting cases: when all lines are strong and when all lines are
weak. We will adopt the following notation: χ is the opacity
opacity parameter, where t is the time from SN explosion
from Blinnikov  as
χ−1
exp = χ−1
Nν−1
i=Nν
χ−1
j=Nν
sj−1
νj−1
that can affect the observer due to the expansion redshift, and
χi is the mean opacity in the continuum 
between adjacent lines νi and νi+1. The continuum can be
formed through free–bound and free–free transitions, electron
scattering, and a superimposed quasi-continuum formed by a
rest frame. The values of the parameter si between the lines
can differ due to the differences of χi in the continuum. In the
last sum over j νj−1 is assumed to be equal to ν at j = Nν.
It is convenient to rewrite Eq.  partly via the wavelength
λ = c/ν by introducing δλi ≡λi −λi+1:
χ−1
exp ≈
i=Nν−1
χ−1
j=Nν
si−1
and we will assume that the entire range important for the
effect is ∆ν << ν. This inequality is not so strong as the pre-
vious one, because either ∆ν = ν/s or ∆ν ∼ν. The
stronger of these inequalities must hold: at small s it is clear
that the Doppler effect ceases to work at ∆ν > ν.
We will then get
χ−1
exp ≈
i=Nν−1
χ−1
j=Nν
si−1
was no need to assume that δλi << λ, although this condition
is almost always fulfilled in practice. If si is great, i.e.,
the lines are few, and the parameter si is great, then all of the
exponentials in  are small and the expansion effect vanishes:
χexp = χNν−1. The case where the parameter si is not too
great, say, si < 103 and δνi/ν ∼10−6 is less trivial. Then,
si is small and the first exponential can be expanded:
we have
χ−1
exp ≈ct
i=Nν−1
ν exp
j=Nν
si−1
χ−1
exp ≈ct
i=Nν−1
λ exp
j=Nν
si−1
the expansion opacity proposed by Friend and Castor 
and Eastman and Pinto  . In this approximation the
contribution of the lines to the opacity in a given frequency
interval  is assumed to be given in the case of
homologous expansion by the expression
χE =
and τj is the Sobolev optical depth in line j :
τj = hc
our χexpν – we obtain the “monochromatic” χν at frequency
ν, while Friend–Castor and E obtain the mean in an interval
to average our χexpν over the interval ∆ν. Let us define the
mean over the interval ∆ν as the mean free path:
χν exp
tic considerations. The E opacity or rather the mean extinction
coefficient in an interval  is the mean number of
interactions between a photon and lines as they are Doppler
shifted by ∆ν divided by the distance traveled ∼ct∆ν/ν.
One would think, on the first impression, that Eq.  cannot
be derived from our Eq. , because the exponentials of −τj
enter into both expressions, but for the mean free path in our
case and for the reciprocal of the mean free path in E. In fact,
the estimates of the mean opacities from  agree with .
From  in the case of strong lines, i.e., lines with a Sobolev
optical depth τj > 1, we find that the sum does not go up to
Opacity of Ejecta in Calculations of Supernova Light Curves
Nmax, but is truncated at the first k, such that τk > 1 and
Pk−1
i=Nν τi < 1. Then, we have the following estimate:
χ−1
exp ≈ctνk −ν
Nstrongν ,
We see that this coincides with E, because mostly Nstrong
strong lines with a Sobolev optical depth greater than unity
contribute to the sum in .
If there are no strong lines in the interval, then the result of
averaging  also coincides with E in the case of weak lines.
Let all lines in the interval ∆ν have a small optical depth,
τi < 1, but there are many lines, so that P
i τi >> 1 over the
interval ∆ν. In this case, the summation does not go up to
Nmax either, but is truncated at the line with the first number
k, such that Pk
i τi > 1  terms enter into the
sum, because the summation begins with line Nν). We now
obtain the following estimate:
χ−1
exp ≈ctνk −ν
depth of weak lines in the interval ∆ν; then, Nweak⟨τ⟩is the
total optical depth of weak lines in this interval. We obtain
the same expression for weak lines from  by substituting
τj for 1 −exp . Thus, both methods for weak lines yield
the same result that is reduced simply to the summation of
the extinction coefficients in lines , just as in the case of a medium at rest.
Thus, the simple heuristic E approximation  correctly
conveys the limiting cases of the rigorously derived approxi-
mate expression , and it may well be used in practice. When
deriving their approximation, the authors of E did not have the
rigorous derivation done in this section, but they tested their
recipe by a comparison with a rigorous numerical calculation
with a large number of lines and obtained satisfactory agree-
ment. The same formula was derived even earlier by Friend
and Castor  based on the Poisson distribution of line
strengths in a frequency interval. The parameter s vanished in
all of the last approximate formulas – recall that this occurred
only because we assumed the condition si << 1. This
case is particularly important for practice, because the role of
lines is particularly great if it is fulfilled. Since the parameter
s drops out in this case, the cases of both strong and weak
lines considered by us can be described by our formulas and
by Eq.  even at a continuum mean free path exceeding the
SN ejecta sizes. In this case, the diffusion approximation can
be completely maintained by the forest of spectral lines. Such
a situation actually comes already near the maximum light of
SNe I .
In the case of weak lines, our results also coincide with
those from Wagoner et al. . However, for strong lines,
when the dependences of the result on letter parameters
coincide, we obtain disagreement with this paper by a factor
of 2, because Wagoner et al.  adopted the definition of
χexp for the transfer equation and not for the flux, as in our
case.
COMPARISON OF THE OPACITIES FOR DIFFERENT
METHODS OF AVERAGING OVER A FREQUENCY INTERVAL
In the previous section we showed that the mean opacity
derived from Eq.  agrees with its values inferred from 
only in the limiting cases of strong and weak lines. In the real
case, a mixture of strong and weak lines, we have to resort to
numerical calculations.
We will compare the expansion opacities derived for
the E and H cases from Eq.  and , respectively.
wavelength range from log 1 Å to log 50 000 Å into
is calculated from Eq.  at s < 30. For large values
of this parameter we apply Eq. derived in Blinnikov
χ−1
exp = χ−1
j=Nν
i=Nν
j=Nν+1
τj−1
νj−1
stability of this formula at larger s. This is physically justified
by the fact that larger s correspond to a smaller expansion
opacity effect, because the nonthermal broadening of spectral
lines due to the motion of the entire envelope is ν/s. In this
case, the region from which the continuum radiation comes to
a given point is relatively small and a forest of lines is formed
against the background of a constant continuum in numerical
wavelength bins.
Figures 1 and 2 present the computations of the mean
opacities in accordance with the E  and H  approaches.
In Fig. 1 the region for the W7 SN Ia model  was chosen for a mixture of silicon and iron 10 days
after explosion. The H and E opacities in the visible and
infrared ranges are seen to differ noticeably, which can lead
Opacity of Ejecta in Calculations of Supernova Light Curves
t = 15.0 
ρ = 10−14 
T = 5×103 
Figure 1: Comparison of the ejecta opacities averaged by different methods over the computational cells of the wavelength grid.
The blue curve  corresponds to the E approximation; the orange curve  corresponds to the Blinnikov approximation.
The computations were performed for matter composed of 90% silicon and 10% iron by mass. The velocity gradient corresponds
to a free expansion for 15 days since explosion. The density and the temperature for which the computations were performed
are indicated in the figure. All parameters roughly correspond to the SN Ia ejecta layers responsible for the generation of
radiation before maximum light.
to a redistribution of fluxes between different spectral ranges
and to a change in the broadband light curves.
In turn, for the parameters typical of an SN IIP in Fig. 2
we compared the behaviors of the two approaches for two
times  in regions close to the
thermalization ones. It can be seen that within the first days
after explosion the H and E opacities differ more dramatically
than at later stages. Hence we can draw the preliminary
conclusion that the fluxes will be particularly different before
the light curve reaches the plateau. This will be illustrated in
the next section with specific models.
INFLUENCE OF THE OPACITY AVERAGING METHOD ON
SN LIGHT CURVES
The SN light curves were computed with the STELLA code
code was used to compute and average the opacities on a fre-
quency grid in the E approximation. An additional procedure
was developed for the computations in the H approximation.
So far the formula from Blinnikov  has been used only
in the CRAB code  to compute the opacity
averaged over the entire spectrum. In our case, an averaging in
narrower frequency intervals  was required, because
STELLA computes the radiative transfer on a grid of 100–1000
frequency intervals.
Using the above two opacity approximations, we computed
the light curves for two types of supernovae, SNe Ia and
SNe IIP. The SN Ia ejecta are composed mostly of metals
and their opacity is determined mainly by the spectral lines,
while the SN IIP ejecta are mostly hydrogen ones, but the
admixtures also play an important role. Therefore, it should
be understood how important the role of the ejecta expansion
opacity averaging method for each type of SN is in modeling
its radiation.
As an SN Ia model we chose the classical W7 model
constructed artificially. This is a star of mass 15 M⊙and radius
the injection of thermal energy 4 × 1050 erg at the center
followed by the formation of 0.15 M⊙radioactive nickel as a
result of its explosion. The presupernova is surrounded by an
envelope of mass 0.03 M⊙with a density ρ ∝r−2 up to a
distance of 3600 R⊙. Such an envelope helps to explain the
first maximum observed in many SNe IIP in the light curve
before it reaches the plateau.
The UBVRI light curves for both models computed in
different opacity approximations are shown in Figs. 3 and 5.
The difference between the SN Ia light curves in the visible
bands is not very large. As could be assumed from a direct
comparison of the opacities at typical 
chemical composition, temperature, and density , in
the visible range changing the approach to the opacity affects
most strongly the I band. The main differences lie in the
redistribution of radiation between the far-ultraviolet and far-
Opacity of Ejecta in Calculations of Supernova Light Curves
t = 15.0 
ρ = 10−15 
T = 4×103 
II P
t = 50.0 
ρ = 10−14 
T = 4×103 
II P
Figure 2: Same as Fig. 1, but for SN IIP ejecta. The computations were performed for the solar chemical composition. The
upper graph is plotted for more rarefied matter with a steeper velocity gradient; the lower one is plotted for denser matter with
a shallower velocity gradient, roughly corresponding to the layers generating the SN IIP radiation during the first maximum
infrared ranges . Attenuation of the hard ultraviolet
flux can affect, for example, the degree of excitation of
atomic levels and can change the pattern of spectral lines in a
complete calculation including, in particular, fluorescence.
In the case of SNe IIP, the main differences in the ap-
proaches to the opacity manifested themselves to a greater
extent at the initial phases of the light curve . Therefore, we show here only this phase, within
The question of what contribution the spectral lines make
to the energy exchange between radiation and matter and
how much these lines contribute to the equalization of their
temperatures and the establishment of equilibrium remains
open.
As can be seen from the results of our computations in
Opacity of Ejecta in Calculations of Supernova Light Curves
Figure 3: The broadband light curves for the W7  model computed in the H  and E 
approximations for the opacity. For comparison, the crosses and triangles indicate the light curves of several observed SNe Ia.
Figs. 6–8, the height and shape of the peak changes greatly
for different expansion opacity approximations and depend on
the so-called qf parameter that must efficiently describe the
fluorescence .
All of the codes to compute the light curves that use
coherent scattering in lines neglect this important effect. It
was shown already in Blinnikov et al.  that the simple
prescription of qf = 1, when all lines are purely absorbing
ones, provides good agreement of the model light curves with
SN observations and the EDDINGTON code. Thus, for the E
opacity a good reproduction for the light curves is obtained at
qf close to 1, when all lines are absorbing ones , while for the H case the parameter qf can
approach zero, when the lines are almost purely scattering
ones . The choice of an
optimal parameter qf in the H case deserves a separate study.
DISCUSSION OF RESULTS AND CONCLUSIONS
Our results show that different descriptions of the expansion
opacity have a significant influence on the observed light
curves of both SNe I and II. Comparison of STELLA with
other codes, including the Monte Carlo one , when the heuristic E approach is
used, does not yet prove that the E approach is more appro-
priate to the problem. After all, the CRAB code also shows its
applicability to different objects and the H approximation is
used there to calculate the opacity of an expanding medium.
However, it should be kept in mind that there is also a different
aspect of the problem: fluorescence and thermalization. Low
values of the line absorption parameter should be taken when
Opacity of Ejecta in Calculations of Supernova Light Curves
Figure 4: The bolometric light curves  for the SN Ia model computed in different approximations for the opacity:
E with absorbing lines  and H with absorbing lines . The color lines indicate the quasi-bolometric UBVRI light
curve , the far ultraviolet blueward of the U band , and the far-infrared radiation redward of the I band
Figure 5: The broadband light curves for the SN IIP model computed in the E  and H  approximations for the
opacity.
using the H approach. In computations on powerful computers,
in principle, no approximations like the expansion opacity
could have been made, but so far such computations can
be performed only in those situations where the flows are
monotonic, there are no shock waves, and so on.
We are grateful to V.P. Utrobin who shared the experience
of his work with an H–type approximation in the CRAB code
and to the anonymous referee for important remarks.
This study was supported by the Russian Foundation for
Basic Research .","[('opacity', 54), ('line', 52), ('light', 32), ('curve', 30), ('case', 27), ('expansion', 21), ('approximation', 20), ('ejecta', 17), ('sn', 17), ('exp', 16)]","[(('light', 'curve'), 28), (('expansion', 'opacity'), 13), (('weak', 'line'), 10), (('velocity', 'gradient'), 8), (('spectral', 'line'), 8), (('opacity', 'ejecta'), 7), (('ejecta', 'calculation'), 7), (('calculation', 'supernova'), 7), (('supernova', 'light'), 7), (('friend', 'castor'), 7)]","[(('opacity', 'ejecta', 'calculations'), 7), (('ejecta', 'calculations', 'supernova'), 7), (('calculations', 'supernova', 'light'), 7), (('supernova', 'light', 'curves'), 7), (('strong', 'weak', 'lines'), 4), (('light', 'curves', 'sn'), 4), (('friend', 'castor', 'eastman'), 3), (('castor', 'eastman', 'pinto'), 3), (('cases', 'strong', 'weak'), 3), (('sobolev', 'optical', 'depth'), 3)]"
2410.17178v1.txt,"arXiv:2410.17178v1    22 Oct 2024
Mon. Not. R. Astron. Soc. 000, 000–000 
Printed 23 October 2024
X. Hernandez1⋆, Pavel Kroupa2,3†
Released 30/10/2024
Concerning recent published studies exploring the presence or otherwise of a gravi-
tational anomaly at low accelerations in wide binary stars as observed by the Gaia
satellite, Cookson  presents an interesting case. In that study, RMS values of
binned relative internal velocities in 1D for wide binaries are compared to Newtonian
predictions for that quantity, with the author concluding that the data presented show
no indication of any inconsistency with Newtonian expectations. However, the com-
parison presented is critically ﬂawed, as the Newtonian predictions used refer to wide
binaries with mean total masses of 2.0 M⊙. This is larger than the 1.56 M⊙value
which applies to the data used in said paper. In this short note we correct the error
mentioned above and show that the data and error bars as given in Cookson 
are in fact inconsistent with Newtonian expectations. Contrary to the assertion by
Cookson, the data presented in Cookson  actually show a clear anomaly in the
low acceleration gravitational regime, this being consistent with a number of indepen-
dent studies yielding this same result. Based on the analysis by Cookson , wide
binary systems show a clear Milgromian deviation from Newtonian dynamics.
Key words:
gravitation — stars: kinematics and dynamics — binaries: general —
Solar mass wide binary stars with separations above a few
thousand au probe the low acceleration regime, a ≲a0 =
tic and cosmological scales generally attributed to an as yet
undetected dark matter component occur. As ﬁrst proposed
by Hernandez et al. , the distribution of relative in-
ternal velocities between components of wide binaries will
hence provide a critical test of gravity at the same accel-
eration regime where at much larger scales dark matter is
invoked to force an agreement between observations and gen-
eral relativity  and Newtonian gravity. If wide binary
kinematics are found to be in accordance with Newtonian
predictions, then modiﬁed gravity proposals, which imply
deviations from Newtonian dynamics in the low acceleration
regime, would be strongly disfavoured. On the other hand,
if a gravitational anomaly is found in wide binaries, a low
acceleration validity limit for classical gravity will have been
found, removing any astrophysical justiﬁcation for the dark
matter hypothesis, and evidencing the need for the devel-
opment of extensions to GR. Notice that independent con-
straints on the local density of dark matter imply a limit
of ρDM < 0.01M⊙pc−3 , thus, any boost
at even tenths of a percent level detected in the relative ve-
locities of wide binaries at separations of order 6 kAU can
not be explained through invoking dark matter as currently
envisioned.
The release of the third Gaia catalogue has, for the ﬁrst
time, made available astrometric data of suﬃcient accuracy
to undertake wide binary gravity tests. As a consequence,
over the last couple of years a series of published studies
have appeared in this ﬁeld. Given the signiﬁcant potential
relevance of this test, all such works must be examined with
care. Two independent groups using various sample selec-
tion strategies and statistical test have published numerous
analysis showing consistent results for an accurate match to
Newtonian expectations in the high acceleration region for
separations below 2000 au where Newtonian gravity, and the
X. Hernandez, P. Kroupa
most well studied modiﬁed gravity theory of MOND , coincide. These studies include careful testing of
the methods applied using synthetic samples and a diligent
treatment of observational errors, which determine the con-
ﬁdence intervals of the reported results. Interestingly, these
studies also coincide in their results for the low acceleration
regime, but ﬁnding in this region a gravitational anomaly.
This consists of relative internal velocities between the com-
ponents of wide binaries with projected separations above
tions, in consistency with long standing MOND predictions.
Examples of the above are Hernandez , Chae ,
Hernandez et al. , Chae  and Chae .
A study concluding Gaia wide binaries show a 19σ pref-
erence for the best-ﬁt Newtonian model over the best-ﬁt
MOND one also appeared, Banik et al. . This last pa-
per however, suﬀers from several major ﬂaws, as recently
described extensively in Hernandez, Chae & Aguayo-Ortiz
at the regime transition found by the previous two groups,
both models being compared yield the same results as a self-
consistency check on the entire procedure. Also, Noise-free
models are compared directly to noisy data taken as a unique
observational template, in spite of the fact that the obser-
vational errors present are comparable, and in some cases
even larger, than the velocity binning used. Ignoring the full
eﬀects of observational noise explains the exaggerated con-
ﬁdence intervals quoted by the authors, and also introduces
a bias towards a Newtonian model.
The reason for this last point is that if one starts from
a MOND reality and then adds noise, the low velocity edge
of the velocity distribution will shift towards lower values
which are no longer accessible to noise-free MOND models.
Newtonian noise-free models on the other hand, are inher-
ently shifted towards smaller velocity values, and hence can
easily match the low velocity edge of a MOND reality which
includes the presence of observational noise. The opposite
does not happen on the high velocity edge of the velocity
distribution, because there the noise-free Newtonian models
of Banik et al.  easily match the MOND plus noise ex-
tension through the inclusion of ﬂybys and other kinematic
contaminants which the authors do not remove from the
sample, but attempt to account for in the 7-dimensional ﬁts
performed, where 6 of the 7 parameters refer not to gravity,
but to the various kinematic contaminants their sample was
not cleared of originally.
More recently Cookson  appeared, henceforth
Coo24, building on the approach of Hernandez, Cookson &
Cortes , and exploring the robustness of the results to
variations in one particular detail of the sample selection of
Hernandez, Cookson & Cortes . After testing several
options, the author settles on what he considers an optimal
data set, and presents it in comparison to Newtonian pre-
dictions, to conclude no gravitational anomaly is present in
the data. As it is easy to check, the conclusion is not only
invalid, but indeed the opposite, as the data in Coo24 were
compared to a Newtonian prediction calibrated for binary
stars having a mass of 2M⊙and not the 1.56M⊙which cor-
responds to the average mass of the binaries in the Coo24
sample.
In this short paper we expose this obvious mistake, and
show the same ﬁnal data form Coo24, but compared to the
Newtonian prediction corresponding to the correct normali-
sation. This shifts the Newtonian model downwards towards
slightly smaller velocities, to reveal the same low accelera-
tion gravitational anomaly reported by both the Hernandez
and the Chae groups. As a further consistency check, the
high acceleration results of Coo24 appear consistent with
Newtonian expectations once the correct normalisation is
included, while binaries with separations below 2000 au ap-
pear below the Newtonian predictions in the original Coo24
paper, a self-consistency check which was completely missed
by the author. Indeed, the Coo24 data show the robustness
of the results of the Hernandez group to the particular detail
being explored, and hence become a valuable conﬁrmation
of the wide binary gravitational anomaly.
The following sections probe into the Coo24 study in
detail, and make explicit the points mentioned above. Sec-
tion  discusses the variations in sample selection explored
in that paper, and Section  looks into the details of the
ﬂawed comparison presented against Newtonian predictions
for binaries having diﬀerent masses from the ones included in
the study, amongst other design ﬂaws of that paper. Section
COOKSON  SAMPLE SELECTION
With the stated aim of exploring the inﬂuence of selection
criteria on the conclusions drawn from wide binary grav-
ity tests, Coo24 constructs various wide binary samples and
compares their appearances on simple diagnostic internal
separation vs. velocity diagnostic plots. The sample selec-
tion is based on what was used in Hernandez, Cookson &
Cortes , allowing for modiﬁcations on one particular
parameter. The sample selection process begins with select-
ing all DR3 sources within a certain maximum distance, in
this case 143 pc, only marginally diﬀerent from the value
of 130 pc used in the publications of the Hernandez group.
To this ﬁrst sample only very lax quality cuts are imposed,
merely to have some conﬁdence in that the sources selected
are indeed stars. Then, a ﬁrst binary candidate list is pro-
duced by sampling 0.5 pc circles on the plane of the sky
about each star in search of potential companions. In the
works by the Hernandez group, and following the ideas of
El-Badry & Rix  and El-Badry et al. , this step
is optimised by requiring that any candidate pair satisﬁes
also that the distance between stars in the pair along the
line of sight lies  within a factor of
order 2 of the separation on the plane of the sky of the pair.
This last point was not included in the Coo24 study.
Next, comes the step which is explored in the Coo24 pa-
per, the removal of ambiguous candidate pairs. This, which
has been termed degrouping, consists of the removal of all
candidate pairs which include any star which is also a mem-
ber of any other candidate pair. At this point, an error toler-
ance must be speciﬁed on the maximum parallax error below
which a Gaia source will be considered as relevant for the de-
grouping process. In Hernandez, Cookson & Cotes, a value of
to the binaries studied was close to 100pc, and maximum
internal binary separations of 0.06 pc were considered, this
implies that noisier excluded sources will have 1σ conﬁdence
Conﬁrmation of the wide binary anomaly.
Figure 1. Left: The ﬁnal ﬁgure in Cookson . Each small black point represents a single wide binary, with the x-coordinate giving
the internal projected separation of the system, and the y-coordinate the 1D relative velocity between components of the binary. Each
binary appears twice, for RA and Dec measurements. The red and green points give binned RMS values for velocities in the various
sub-samples, for RA and Dec values, respectively, with the numbers above being the number of points per bin. The blue line gives the
Newtonian prediction for the 1D RMS binned values of Jiang & Tremaine , calibrated for an assumed total binary mass of 2M⊙.
Asides from the clear anomalous velocity bin labelled ’83’, which Cookson  for some unknown reason neglects to mention as such,
the low acceleration region to the right of 3000au, 0.015 pc, shows binned values which can be said to be consistent with the blue line, as
they lie only 1σ above it, bins labelled ’53’ and ’25’. The ﬁnal point to the right is of little relevance, both on account of its poor statistical
value, and because it contains binaries in a regime dominated by tidal eﬀects and stellar encounter interactions, which all other groups
working on the ﬁeld for good reasons avoid, see text. What the author failed to notice was that in the high acceleration region to the
left of 0.01 pc, which should serve as a consistency check of the overall procedure and a signal to any errors, the binned values actually
lie below the blue line. The ﬁgure hence apparently shows a hitherto unknown high acceleration gravitational anomaly where observed
binaries rotate under a suppressed eﬀective gravity. The reason for this lies in the fact that the average binary masses in the Cookson
the Newtonian prediction for binaries having the same mass as the average one present in the Cookson  sample . The
high acceleration region is now consistent with Newtonian predictions, and the low acceleration one shows again the clear close to 20%
velocity boost reported by the Hernandez and Chae groups, as evidenced by the solid red line of the MOND prediction. The slanted red
dashed line in both panels is a hand ﬁtted demarcation between bound binaries and ﬂyby cases appearing in the original.
intervals for their distances to us along the line of sight of
with those of target stars, this is already at least 16.6 times
larger than the largest binary separations considered, and
hence refers to a source having a minimal probability of ac-
tually being a perturber of any kind. Coo24 explores the
eﬀects of relaxing this 1% maximum parallax tolerance for
the degrouping phase of the sample selection process.
Then, a series of quality cuts are applied to the data
to remove ﬂybys and cases where one or both stars in any
binary might be binaries themselves, through limits on the
allowed internal Gaia single-star solution RUWE quality in-
dex, requiring a radial velocity measurement to be available
for each star and that the diﬀerence in this quantity between
members of any binary be below 4 km s−1 and a ﬁnal ﬁl-
tering through the region of the HR diagram identiﬁed by
Belokurov et al.  and Penoyre et al.  to be least
aﬀected by unresolved binaries, which in this case would be
tertiaries. The details of the above quality cuts can be found
in Hernandez, Cookson & Cortes  or Coo24.
Coo24 repeats the entire sample selection and astrome-
try processing procedures for various values of the maximum
parallax tolerance error to be considered in the degrouping
phase, producing samples for 1, 1.4 and 2.5% tolerance er-
rors. The results are presented in plots showing the internal
projected separation of each ﬁnal binary considered against
the relative velocity between components of each binary,
both in RA and Dec. Coo24 settles for the 2.5% parallax
tolerance error during the degrouping phase as the optimal
choice for this parameter. Results appear in ﬁg. 6 of that
paper, which includes also a comparison against Newtonian
predictions from the work of Jiang & Tremaine , which
appear as a blue solid line in this ﬁgure. This line is then
compared against the RMS values of sub-samples binned
into internal separation intervals. We reproduce this ﬁgure
in the left panel of Fig. 1.
Coo24 uses this ﬁgure to conclude that Gaia wide bi-
naries show no indication of any low acceleration gravita-
tional anomaly, as most of the bins for separations larger
than 0.01pc appear broadly consistent with the Newtonian
line shown, with the exception of the one labelled ’83’, a
fact which for some reason Coo24 omits to mention. As it
is crucial to understand exactly how the blue line in ﬁg. 6
of Coo24 comes about, the following section explores this
point.
X. Hernandez, P. Kroupa
COOKSON  NEWTONIAN COMPARISON
In order to compare the points appearing in the diagnostic
plots of ﬁgs. 4-6 in Coo24 against Newtonian expectations,
the author presents binned RMS values at a number of in-
tervals. This is because the Newtonian expectation against
which the data are compared are the results of the study
by Jiang & Tremaine . This last study simulates large
populations of 50,000 wide binaries having a distribution of
ellipticities and random projection orientations, and evolved
through numerical simulations over a 10 Gyr period, in the
presence of the tidal ﬁeld of the Milky Way at the Solar Ra-
dius and encounters with molecular clouds. These binaries
are then projected on the plane of the sky and the resulting
distribution of points is ﬁnally summarised in their ﬁg. 7.
This ﬁgure gives binned RMS values of the relative 1D ve-
locity as a function of the internal projected separation on
the plane of the sky for the simulated binaries, precisely the
quantities given in ﬁgs. 4-6 of Cookson . However, ﬁg.
velocities in physical units, but in dimensionless quantities,
so that the ﬁgure can be used to infer results for binaries of
any chosen total mass.
As it is trivial to check, one of us in Hernandez et al.
assumption to derive RMS internal 1D binary velocity val-
ues in km s−1 vs. internal binary projected separations in
pc from ﬁg. 7 in Jiang & Tremaine . This line in
dimensional units ﬁrst appeared in ﬁg. 4 of Hernandez et
al. . In the interest of keeping a constant comparison
line throughout the various publications by the Hernandez
group, the line was kept constant, assuming the same 2M⊙
total binary mass. This was always made explicit and borne
in mind when comparing to data, e.g., the 12th paragraph in
section  in Hernandez, Cookson & Cortes  makes
it explicit that a total binary mass of 2M⊙was assumed to-
wards arriving at the Newtonian comparison line for RMS
velocities shown in the ﬁgures where this appears in that
paper. Also, the ﬁfth paragraph in section  in Hernandez
total binary mass assumed in producing the RMS Newto-
nian expectation from the Jiang & Tremaine  results,
with comments on the change in normalisation between the
data and the line being important included in the seventh
paragraph of section  of that paper. That this line in di-
mensional form has remained invariant from Hernandez et
al.  to Coo24 is trivial to check, as it is also to per-
form the scaling directly from ﬁg. 7 in Jiang & Tremaine
ticular 2M⊙, to arrive at the blue line shown in ﬁgs. 4-6 of
Coo24.
The average binary mass in Hernandez  was of
in Coo24, the average binary mass in that work is also of
that the comparison presented in ﬁg. 6 in Coo24 is critically
ﬂawed. Before correcting this obvious mistake, the original
ﬁg. 6 in Coo24 deserves a closer look, as shown in the left
panel of our Fig.1. The claim of no gravitational anomaly
in the 2D projected separation > 0.01 pc region, even when
comparing against Newtonian expectations for masses larger
than the ones present, is somewhat suspect, not only because
the ’83’ bin shows a clear velocity boost well above the 1σ
conﬁdence interval given in the ﬁgure for that bin, but also
because the following two bins appear above the Newtonian
line, although only at a 1σ level. The ﬁnal bin, containing
only 6 binaries is irrelevant, not only on account of the clear
lack of statistical signiﬁcance at such low occupancy num-
bers, but also because of the enormous projected separations
covered. Of the other groups having published on the topic,
in both the Chae publications and in Banik et al.  the
samples are ended at 0.15pc, Pittordis & Sutherland 
stop at 0.1 pc, and, over an abundance of caution, in the
Hernandez group publications the samples end at 0.06 pc.
It is clear that observed projected separations are only lower
limits on the present 3D separation between the components
of observed binaries, which in turn, given the elliptical or-
bits these systems present , are only
lower limits to the maximum orbital separations attained
by a given binary. Given the average interstellar separation
of close to 1 pc for stars in the Solar Neighbourhood, all the
groups listed above have chosen to stop the analysis well be-
fore the maximum projected separation of 0.5pc considered
in Coo24, to minimise the presence of nearby perturbers.
A ﬁnal important point to note regarding the original
ﬁg. 6 in Coo24, is the failure of the high acceleration con-
sistency check. The inclusion of a Newtonian region where
there is no uncertainty as to what the result should be is
intended as an internal check on the whole procedure. How-
ever, as can be checked in the left panel in our Fig. 1, the
binned RMS values of high acceleration binaries with sep-
arations below 0.01 pc actually appear as sub-Newtonian.
The original data presented in Coo24 actually show a clear
gravitational anomaly for high acceleration binaries show-
ing smaller relative velocities than Newtonian expectations.
Clearly, this should have alerted the author to the presence
of an error in the comparison, the mass normalisation mis-
match noted above.
Finally, we reproduce in the right panel of Fig.1 the
exact same data of the ﬁnal ﬁg. 6 in Coo24, but adjusting
the Newtonian prediction to match the average total binary
mass of 1.56M⊙present in the Coo24 sample. As can be
seen, the high acceleration region for separations below 0.01
pc is now accurately consistent with the Newtonian predic-
tion, with the exception of the tightest bin, where edge ef-
fects lead to a small oﬀset. The low acceleration region with
separations above 0.01 pc on the other hand, now clearly
shows a gravitational anomaly consistent with the 20% ve-
locity enhancement of MOND expectations, shown by the
solid red line. At separations larger than 0.01 pc, the bins
labelled ’132’, ’53’, ’25’, and even the highly suspect last
Newtonian prediction, and in excellent accordance with the
MOND prediction of the red line. The ’83’ bin is also incon-
sistent with Newtonian expectations, but remains anoma-
lous when compared to the MOND predictions, although by
a smaller amount than what appears in the original Fig.6 in
Coo24. Going beyond this ﬁrst qualitative comparison would
require a detailed statistical comparison against full velocity
distribution predictions, of the type presented in e.g. Chae
In the conclusions of Coo24 both the results of the Her-
nandez and the Chae groups are dismissed on the basis of
the mistaken diagnostic comparison we reproduce in the left
Conﬁrmation of the wide binary anomaly.
panel of Fig. 1, without including any detailed statistical
test of the type appearing in e.g. Hernandez et al.  or
in any of the publications of the Chae group, where careful
comparisons of the full velocity distributions against sim-
ulated full velocity distributions of Newtonian models are
performed. Indeed, the sample selection parameter explored
in Coo24 is not even of any relevance for the publications of
the Chae group, where the isolation of ﬁnally selected bina-
ries is guaranteed through completely independent mecha-
nisms. As is evident from the right panel of our Fig. 1, what
the results of Coo24 in fact show is exactly the same low ac-
celeration gravitational anomaly which has been reported in
numerous independent analysis of the Hernandez and Chae
groups, e.g. see ﬁg. A1 in Hernandez  or ﬁg. 13 in
Chae .
We have show that the study by Cookson  suﬀers from
a critical ﬂaw which renders its conclusions void. Correcting
the mistake in question, and using the data presented in that
paper, in fact reveals a clear gravitational anomaly at low ac-
celerations, with relative velocities between the components
of observed binary stars being above Newtonian expecta-
tions. Thus, the data presented in Cookson  are in fact
consistent with recent studies showing a close to 20% veloc-
ity boost for Gaia binaries with separations beyond 3000 au,
e.g. Hernandez et al. , Chae , Hernandez et al.
generic MOND predictions of decades ago.","[('binary', 60), ('newtonian', 41), ('coo', 33), ('hernandez', 30), ('velocity', 27), ('separation', 26), ('sample', 22), ('cookson', 21), ('line', 21), ('wide', 20)]","[(('wide', 'binary'), 19), (('newtonian', 'prediction'), 13), (('gravitational', 'anomaly'), 11), (('low', 'acceleration'), 10), (('newtonian', 'expectation'), 9), (('high', 'acceleration'), 9), (('projected', 'separation'), 9), (('binary', 'mass'), 9), (('hernandez', 'al'), 8), (('sample', 'selection'), 7)]","[(('hernandez', 'cookson', 'cortes'), 5), (('total', 'binary', 'mass'), 5), (('high', 'acceleration', 'region'), 4), (('binned', 'rms', 'values'), 4), (('low', 'acceleration', 'regime'), 3), (('acceleration', 'region', 'separations'), 3), (('hernandez', 'chae', 'groups'), 3), (('con', 'rmation', 'wide'), 3), (('rmation', 'wide', 'binary'), 3), (('internal', 'projected', 'separation'), 3)]"
2410.17187v1.txt,"Draft version October 23, 2024
Typeset using LATEX twocolumn style in AASTeX631
Exploring Unobscured QSOs in the Southern Hemisphere with KS4
Yongjung Kim,1 Minjin Kim,2 Myungshin Im,3, 4 Seo-Won Chang,3, 4 Mankeun Jeong,3, 4 Woowon Byun,1
Joonho Kim,5 Dohyeong Kim,6 Hyunjin Shim,7 and Hyunmi Song8
Republic of Korea
of Korea
We present a catalog of unobscured QSO candidates in the southern hemisphere from the early in-
terim data of the KMTNet Synoptic Survey of Southern Sky . The KS4 data covers ∼2500 deg2
sky area, reaching 5σ detection limits of ∼22.1–22.7 AB mag in the BV RI bands. Combining this
with available infrared photometric data from the surveys covering the southern sky, we select the
unobscured QSO candidates based on their colors and spectral energy distributions  fitting
results. The final catalog contains 72,964 unobscured QSO candidates, of which only 0.4 % are previ-
ously identified as QSOs based on spectroscopic observations. Our selection method achieves an 87 %
recovery rate for spectroscopically confirmed bright QSOs at z < 2 within the KS4 survey area. In
addition, the number count of our candidates is comparable to that of spectroscopically confirmed
QSOs from the Sloan Digital Sky Survey in the northern sky. These demonstrate that our approach
is effective in searching for unobscured QSOs in the southern sky. Future spectro-photometric surveys
covering the southern sky will enable us to discern their true nature and enhance our understanding
of QSO populations in the southern hemisphere.
Keywords: Quasars  — Photometry  — Catalogs  — Galaxies 
Quasi-stellar objects , also known as quasars,
are the brightest population of active galactic nuclei
tion of the supermassive black holes  and their
impact on host galaxies along the cosmic time. Unob-
scured  QSOs, in particular, provide a unique
opportunity to directly observe their central structure,
including the accretion disk surrounded by dense ion-
ized gas  and dusty torus . Therefore, multiwave-
Corresponding author: Minjin Kim
yjkim.ast@gmail.com; mkim.astro@gmail.com
length studies of those sources provide insights into
detailed physical properties of AGNs  and structural parameters .
Furthermore, there is a substantial interest not only
in investigating the properties of individual objects but
also in understanding the demography of QSOs. This
includes statistical analysis of QSO populations, exam-
ining their distribution in space and time , and analyzing luminos-
ity functions to trace cosmic evolution , alongside the
corresponding black hole mass functions and distribu-
tions of Eddington ratio . Unobscured QSOs ac-
count for a significant portion of the total QSO popula-
tion, making them valuable for such studies . Therefore, surveys of unobscured QSOs
are essential, as they contribute to our understanding
of both cosmic evolution and the mechanisms underly-
ing the growth of SMBHs .
Despite significant advancements in QSO surveys over
the past two decades , the explo-
ration of QSOs in the southern hemisphere remains in-
complete.
For example, Croom et al.  spectro-
scopically discovered ∼10K QSOs in a field of a limited
area  in the southern
hemisphere. Onken et al.  found 156 spectroscop-
ically identified QSOs from the All-sky BRIght Com-
plete Quasar Survey , based on the com-
bination of All-sky space missions. In addition, Yang &
Shen  utilized optical photometric data 
from the Dark Energy Survey  to
search for the photometric QSO candidates in the south-
ern hemisphere.
Some studies have been more dedicated to exploring
the particular area, the south ecliptic pole field, using
optical imaging data . This re-
gion will be extensively explored by several survey mis-
sions, such as Euclid ,
Spectro-Photometer for the History of the Universe,
Epoch of Reionization, and Ices Explorer , eROSITA , and
poral survey data in the study of AGN physical prop-
erties, it is vital to pre-select the bright AGN in the
southern hemisphere.
Using a network of three KMTNet 1.6-m telescopes
in Chile, Australia, and South Africa ,
an optical imaging survey called the KMTNet Synoptic
Survey of Southern Sky  is underway. This survey
commenced on November 29, 2019. It aims to image up
to 7000 square degrees of the sky visible from the south-
ern hemisphere  in four optical bands
depths of ∼22.1–22.7 mag. It is noteworthy that the
survey area has not been fully covered by DES. The pri-
mary science goal is to identify the optical counterparts
of gravitational wave triggers and to study kilonovae in
their early stages .
We plan to roll out Data Release 1 in late 2024, which
will be accompanied by a journal paper on the survey
bilities compared to other southern sky surveys, such as
SkyMapper Southern Survey , KS4
is expected to facilitate the discovery of more QSO can-
didates that were previously missed. The early interim
data, which covers the sky area of ∼2500 deg2 , are currently available only to internal collaborators
for research and verification. Leveraging this early data,
we aim to compile a catalog of unobscured QSO candi-
dates, serving as groundwork for future survey missions.
The QSO selection solely based on the optical col-
ors has some limitations due to contaminators, such as
early type stars or star-forming galaxies. On the other
hand, mid-infrared  colors are particularly effec-
tive in identifying QSO candidates , largely because the MIR light is emit-
ted from the hot or warm dust surrounding SMBHs, as
posited by the unified AGN model . However, since MIR wavelengths
are less affected by dust extinction, relying solely on
MIR colors can lead to a mixed sample of both unob-
scured and obscured QSOs. The integration of optical
data significantly enhances the selection of unobscured
QSOs . There-
fore, we utilize a multi-wavelength dataset, including the
KS4 data, to specifically target the identification of un-
obscured QSOs in the southern sky.
In this paper, we present the catalog of unobscured
QSO candidates in the southern hemisphere, selected
from the KS4 interim data.
The KS4 data and pho-
tometry are described in Section 2. In Section 3, we de-
scribe how the unobscured QSO candidates are selected,
and the validation of the candidate selection is given in
Section 4.
Through this paper, we adopt the canon-
ically used cosmological parameters for the standard
ΛCDM universe: H0 = 70 km s−1 Mpc−1, Ωm = 0.3
and ΩΛ = 0.7. All the magnitudes are given in the AB
system unless exceptions are noted.
The data used in this work consists of object catalogs
from BV RI stacked images for 638 KS4 tiles .
Each pre-defined tile covers a total field of 2
degree × 2 degree, matching the field-of-view of the
KMTNet camera . Observations were
made using three identical 1.6-m telescopes between late
November 2019 and January 2022. We prioritized ob-
Unobscured QSOs with KS4
Figure 1. Sky coverage map for the KS4 data, centered on
the south ecliptic pole. Areas surveyed at least four times in
the BV RI bands using dithering techniques  are marked in red.
The DES survey area is shown in
blue. The background represents a color composite from the
Digital Sky Survey 2.
servations in areas of the sky excluding the Large Mag-
ellanic Cloud, Small Magellanic Cloud, low Galactic lat-
itude regions , and the regions already cov-
ered by the DES .
Because of the alti-
tude limit of KMTNet, the Decl. < −85◦regions are
excluded as well. The basic survey strategy is straight-
forward: each tile is observed with a 120-second expo-
sure per visit, with a total combined exposure time of at
least 480 seconds. Each raw image consists of an array
of four e2v CCDs 
with a pixel scale of 0.4 arcsec. To fill the CCD gaps in
the vertical  and horizontal 
directions, we use a four-point dithering pattern with
offsets of about 4 and 7 arcmin in RA and Dec, respec-
tively. This dithering pattern results in heterogeneous
image depths across the CCD gap and edge regions of
each tile.
The data were pre-processed at the KMTNet data cen-
ter in Daejeon, Korea, including overscan and crosstalk
corrections, bias subtraction, and flat fielding. A bad
pixel mask is generated by identifying pixels that are
affected by cosmic-ray hits on the detector, crosstalks
from saturated pixels in an associated amplifier , and regions that are unusable due to in-
herent defects in the CCD. The pre-processed data then
undergoes zero-point  scaling to a uniform value
of ZP = 30 mag, eliminating spatially variable ZPs
across the field. For astrometric calibration, we utilize
the SCAMP software  to derive an accurate
World Coordinate System  solution, using Gaia
EDR3  as the reference
catalog. This method achieves a root-mean-square er-
ror of 0.026 arcsec in both RA and Dec relative to the
reference catalog WCS.
The KS4 stacking procedure involves using the SWarp
software  to combine the pre-processed im-
ages. We employ a tangential projection method and a
median combine technique to produce the final stacked
images. Additionally, the FSCALASTRO parameter is set
to VARIABLE to adjust for spatial variations in the pixel
scale of KMTNet images, ensuring accurate flux rescal-
ing throughout the field. The average seeing sizes of the
combined images are about 2.10, 2.03, 1.94, and 1.93
arcsec in the BV RI bands, respectively.
The photometric calibration of the images is per-
formed using APASS DR9  and
SkyMapper DR3  catalogs as
reference data, with the following calibration equations:
B = BAPASS −0.06 −0.27 × 
V = VAPASS + 0.02
R = rAPASS + 0.0383 −0.3718 × 
I = iSMSS + 0.011 −0.243 × 
In the above, BAPASS and VAPASS are in Vega mag-
nitudes, while the others are in the AB system. The
references for these conversion equations are as follows:
the B and V conversion equations are from Park et al.
I is derived by comparing KMTNet images taken in the
COSMOS field with the SMSS catalog of the same field.
For point sources brighter than 17 mag, the average
uncertainties of KS4 photometric calibration are 0.026,
spectively. Correspondingly, the final 5σ imaging depths
are B = 22.75, V = 22.60, R = 22.80, and I = 22.09
mag, which are comparable to those of the preceding
QSO survey with KMTNet data . The
saturation levels are 12.95, 13.07, 13.77, and 13.60 mag,
respectively.
Source detection was performed on the KS4 I-
band images with a signal-to-noise ratio criterion of
MAG AUTO SNR I> 1.
Subsequently, we extracted the
flux of sources in each band image using the dual mode
of SExtractor . We primarily
use the MAG AUTO estimates from the SExtractor, which
measure flux within an adaptively scaled aperture. This
approach is due to variations in the point spread func-
tion  depending on the source’s position in the
image.
To utilize infrared photometric data in the follow-
ing analysis, we crossmatch the KS4 catalog with ex-
ternal catalogs from the infrared surveys covering the
KS4 area: Two Micron All Sky Survey , VISTA Hemisphere Survey , and Wide-field Infrared Survey
Explorer . For each KS4 ob-
ject, we identify potential counterparts by searching for
the nearest neighbor source in external catalogs, rely-
ing solely on its coordinates. Considering the imaging
resolution of the surveys, we tested several cases and
established the matching radii for 2MASS, VHS, and
WISE as 2.0, 1.0, and 2.0 arcsecs, respectively.
The 2MASS point source catalog  provides de-
fault magnitudes for the J, H, and Ks bands , representing the optimal measurements for each
band.
Although 2MASS is an all-sky survey, its de-
tection limits are relatively shallow, with 5σ detection
limits for a point source at 17.4, 17.2, and 16.9 mag,
respectively. To enhance our depth of detection, we also
utilize J, H, and Ks magnitudes from VHS, specifically
the 2 arcsec diameter aperture magnitudes , which achieve 5σ detection
limits of 21.5, 21.2, and 20.3 mag, respectively. Note
that VHS coverage in the H-band is limited to the south
galactic cap. When a source appears in both 2MASS
and VHS, we prioritize the VHS magnitudes, resorting
to 2MASS only when VHS data is unavailable.
Among the various versions of WISE data, we use the
W1, W2, W3, and W4 magnitudes from the AllWISE
catalog . The magnitudes are mea-
sured with profile-fitting photometry  and their 5σ detection limits are 19.6,
To convert the Vega magnitudes to AB magnitudes,
the following papers and conversion factors are refer-
enced: Blanton et al.  for 2MASS JHKs bands
VHS JHKs bands , and Cutri et al.
For the final catalog, we perform the galactic extinc-
tion correction by utilizing sfdmap Python package1.
Following the default setting, we use the dust map by
Schlegel et al.  with a scaling factor of 0.86 from
Schlafly & Finkbeiner .
These corrections are
made under the assumption of RV = 3.1.
We initially eliminate unreliable sources flagged by
the KS4 data processing pipeline. These flagged sources
typically arise from spurious signals, such as crosstalk,
blended light from bright sources, and instrumental
hot/bad pixels . As mentioned in Section 2, each
KS4 field was intentionally dithered at least four times.
To ensure data homogeneity, we select objects observed
at least four times in every band, using the NDITH flags
in the KS4 catalog . Then, the effective
survey area size decreases, as shown by the red patches
in Figure 1.
Our source detection was conducted in the I-band im-
ages, hence there are numerous sources beyond the 5σ
imaging depths in the other bands, as shown in Fig-
ure 2. For reliable source selection, we apply an addi-
tional signal-to-noise ratio threshold of 5 in the I-band
of I ≲21 mag.
Note that the SNRs are simply de-
termined by taking the reciprocal of magnitude errors.
The imposition of this additional SNR criterion in the
I-band  marginally reduces the number
of sources. The B-band and WISE detections are also
important in the QSO selection below, so we give ad-
ditional criteria for them: SNRB > 2, SNRW1 > 2,
SNRW2 > 2, and SNRW3 > 2. Further imposing these
SNR criteria results in a significant reduction in the
source count , while the numbers of
bright sources  are relatively less affected
by these criteria. This is due to the remaining bright
stars after the selection. Indeed, most of the remaining
bright sources have blue MIR colors ,
implying that they are not likely to be QSOs.
We distinguish QSOs from stellar objects and inactive
galaxies by their MIR colors; QSOs exhibit redder MIR
colors due to the hot dust emissions reprocessing light
from accretion disks. We adopt the AGN selection cri-
teria based on WISE colors of X-ray-detected sources as
proposed by Mateos et al. :
Mateos et al.  defined these criteria in the Vega
magnitude system.
We applied the Vega-to-AB con-
version factors as per the AllWISE data , with a minor discrepancy  from those
used in Mateos et al. .
Note that most X-ray-
detected sources in Mateos et al.  are at z < 2,
Unobscured QSOs with KS4
SSFLAG & NDITH
Figure 2. Histograms of the reliable sources selected by SSFLAG and NDITH BVRI criteria across the BV RI bands. The forced
photometry in the I-band yields flux measurements of numerous sources beyond the 5σ imaging depths, as indicated by the
vertical dotted lines in each panel. The red histograms represent the marginal reduction in source count after applying an SNR
criterion in the I-band. The blue histograms show the decrease in numbers when applying additional SNR criteria across the
B/W1/W2/W3 bands. Note that the sudden increase in number at I ≲16 mag is due to the bright stars that remain after the
SNR cut. The shaded regions denote the ranges of saturated magnitudes.
W2 −W3
W1 −W2
Selection wedges
Type-1
Type-2
Selected Milliquas QSO 
Color/SNR-rejected  
SED-fit-rejected  
B −W3
W1 −W2
Figure 3. Color-color diagrams of the reliable sources that have SSFLAG=Null and NDITH≥4 in the KS4 catalog. The QSO
selection criteria are indicated by black lines. The blue contours map the distribution of type-1 QSOs in Lyke et al. ,
while the red contours represent the distribution of type-2 QSOs . The blue contour levels range from 0.5σ
to 3σ at 0.5σ intervals, starting from the innermost to the outermost, whereas red contour levels range from 0.5σ to 2σ with
the same increment, also moving outward from the center. The squares indicate the spectroscopically confirmed QSOs from the
Milliquas v8 catalog  within our survey area. The brown circles represent the QSOs selected by our method, while
the orange triangles and yellow diamonds are rejected ones due to their photometry and poor fitting results, respectively.
where their hot dust components are observable within
the W1-to-W3 bands. The left panel of Figure 3 shows
the WISE color-color diagram relevant to the selec-
tion wedge .
Accompanying this are the
color distributions of spectroscopically confirmed type-1
from Lyke et al.  and Reyes et al. , respec-
tively. As this selection wedge is determined by X-ray
detection, it effectively captures both type-1 and type-2
QSOs. Our objective is to refine the selection to un-
obscured QSOs; thus, we impose an additional criterion
based on the optical-MIR colors to reflect the bluer opti-
cal spectral shape of unobscured QSOs, following Byun
et al. :
Note that this criterion is converted to the AB magni-
tude system, while the previously proposed criterion is
given in the Vega magnitude system .
The right panel in Figure 3 presents the color distri-
butions incorporating B-band magnitudes. This fourth
criterion, shown as the black solid lines, more accurately
differentiates between type-1 and type-2 QSOs. Indeed,
criteria, whereas a mere 6.2 % of the type-2 QSOs from
Reyes et al.  do so. It is noteworthy that the selec-
tion of type-2 QSOs may be skewed by the small sample
size  relative to type-1 QSOs . We also note that our selection criteria are highly
effective for identifying type-1 QSOs up to z ∼2, with
a recovery rate of 95 %. The recovery rate drops signif-
icantly to 7 % at z ∼3.3, indicating that these criteria
are primarily suited for the identification of low-redshift
type-1 QSOs. This is because high-redshift QSOs ex-
hibit faint flux at the B band and may not be detected
in the W3 band. We further discuss the selection effi-
ciency of spectroscopically confirmed QSOs within the
KS4 field in Section 4.1.
Consequently, employing the above selection criteria
yields 106,443 QSO candidates over the KS4 survey area.
Because the photometric selection solely with broad-
band photometry can lead to including a large portion
of non-QSOs, we carry out the SED fitting on the pre-
selected sources in order to further perform the rigorous
classification. The detailed method is described in Son
et al.  and Byun et al. . The photometric
data obtained from KS4, 2MASS, VHS, and WISE is
used for the analysis. We adopt LePhare++2, a C++ ren-
dition of the original Fortran program LePhare , which allows us to fit
the SED with various sets of SED templates. We ini-
tially employ three AGN templates from Lyu & Rieke
in mid-infrared: normal, hot dust-deficient, and warm
dust-deficient AGNs. In addition, we consider the emis-
sion from the polar dust with extinction  on the AGN continuum. Finally, the contribution
from the host galaxies, which is modeled with 7 types of
SWIRE galaxies   is added. The flux ratio of the host to
the total flux at 1.6 µm is set to be 1 −95%. Finally,
estimation of the COSMOS survey 
and the stellar templates from Bohlin et al.  and
Pickles  are adopted. Figure 4 shows an example
of the SED fitting results.
To select the best model among the SED templates
for QSOs, inactive galaxies, and stars in describing the
observed SEDs of the QSO candidates, we employ the
Bayesian information criterion . The BIC is de-
fined as BIC = χ2 + k ln n, where k is the number of
free parameters and n is the number of the data points
in the SED. According to this criterion, a source is clas-
sified as a QSO only if the BIC for the best fit with
QSO templates is at least 10 points lower than that of
the best fit with galaxy SED templates ,
a method proven effective for QSO/galaxy classification
card heavily obscured AGNs, we impose an additional
criterion on extinction . We further refine our
selection by excluding sources whose χ2
QSO values exceed
the 97.7 %  confidence threshold.
This statistical
cutoff helps identify and remove outliers or less likely
QSO candidates based on the quality of their fit, ensur-
ing that only the most probable QSOs are retained in our
analysis.
Employing these stringent criteria enhances
the integrity and reliability of our dataset, focusing on
sources that best match the expected characteristics of
QSOs.
In the KS4 field, the final set of unobscured QSO can-
didates comprises 72,964 sources, all of which meet the
selection criteria outlined in Sections 3.1 and 3.2. We
provide a detailed multi-wavelength catalog of these can-
didates, as described in Table 1.
Unobscured QSOs with KS4
Figure 4.
Example of SED fitting results.
The yellow
squares are the photometric data, while the blue, red, and
gray lines represent the best-fit galaxy, QSO, and star mod-
els, respectively. The χ2 values of the models are marked in
the legend.
The most direct method to confirm our QSO can-
didates as real QSOs is to obtain their optical/near-
infrared spectra to identify the AGN features. However,
confirming all QSO candidates through individual spec-
tral observations is a cost-ineffective approach for con-
ducting QSO surveys in the southern sky. We, therefore,
anticipate relying on forthcoming spectro-photometric
surveys, such as 7DS and SPHEREx, for their direct
confirmation. Instead, we here explore the validity of
our QSO candidates through indirect methods.
To validate our selection, we compare our QSO candi-
dates with spectroscopically confirmed QSOs in the KS4
survey area. We utilize the Milliquas catalog v8 , which contains approximately 0.9 million type-
within our survey area. Specifically, within our survey
area, there are 484 QSOs classified as the core-dominant
type-1 QSOs in the catalog. Upon cross-matching with
our QSO candidates, we find only 325 QSOs, resulting
in a recovery rate of 67 %.
In Figure 3, we present the color distributions of the
KS4 or WISE photometry.
Approximately a quarter
of QSOs are rejected during the color-selection stage
neath the selection wedge of Mateos et al.  in the
left panel, a region where only a very minor fraction of
type-1 QSOs are expected to be located .
The remaining samples are rejected during the selection
based on the SED fitting . It is impor-
Table 1. Description of the Columns in the KS4 QSO Can-
didate Catalog.
KS4 QSO candidate designation
R.A. 
Decl. 
KS4 B-band MAG AUTO magnitude
e Bmag
Error on B-band magnitude
KS4 V -band MAG AUTO magnitude
e Vmag
Error on V -band magnitude
KS4 R-band MAG AUTO magnitude
e Rmag
Error on R-band magnitude
KS4 I-band MAG AUTO magnitude
e Imag
Error on I-band magnitude
J-band magnitude
e Jmag
Error in J-band
f Jmag†
Flag in J-band
H-band magnitude
e Hmag
Error in H-band
f Hmag†
Flag in H-band
Ks-band magnitude
e Kmag
Error in Ks-band
f Kmag†
Flag in Ks-band
WISE W1-band magnitude
e W1mag
Error on W1-band magnitude
WISE W2-band magnitude
e W2mag
Error on W2-band magnitude
WISE W3-band magnitude
e W3mag
Error on W3-band magnitude
WISE W4-band magnitude
e W4mag
Error on W4-band magnitude
Note—All the magnitudes are given in AB magnitudes.
values are indicated as −99.0. A magnitude with an error of −99.0
means an upper limit. 
tant to note that the Milliquas sample is a compilation of
spectroscopically confirmed QSOs without homogenetic
selection, which may be related to the missing popula-
tions by our method.
In Figure 5, we present the distributions of redshift
QSOs within the KS4 survey area. Since our selection
criteria are optimized for identifying low-redshift, unob-
scured QSOs, the missing fraction of QSOs increases at
higher redshifts. This fraction also increases at I ≳20
mag, where the number count of reliable sources after
applying the SNR criteria drops dramatically .
As pointed out in Section 3.1, our color-selection crite-
ria are highly effective for finding low-redshift QSOs. If
we consider the QSOs with I < 20 mag at z < 2, the
numbers of selected, color/SNR-rejected, and SED-fit-
rejected QSOs are 232, 21, and 13, respectively, resulting
in a recovery rate of 87 % . It is noteworthy
Selected Milliquas QSO
Color/SNR-rejected
SED-fit-rejected
Figure 5. Redshift  and I-band magnitude  distributions of Milliquas QSOs within the KS4 survey area. The top
and bottom panels show the stacked histograms and the fractional distributions, respectively. The brown histograms represent
the QSOs that meet our selection criteria, while the orange and yellow histograms are the rejected ones because of their
photometry and poor fitting results, respectively.
that the fraction of color-selected objects is then 92 %
for SDSS type-1 QSOs as discussed in Section 3.1. This
similarity highlights the effectiveness of our QSO selec-
tion methods for spectroscopically confirmed QSOs in
the southern sky.
As an indirect approach to validate our QSO candi-
dates, we compare their number counts per unit area
to those of spectroscopically identified QSOs from the
SDSS DR16 , spanning an area of
Furthermore, we restrict our analysis to
QSOs with a redshift lower than 2 to accommodate
the variations in number counts by redshift. The ad-
justed number counts from SDSS are represented as gray
squares in Figure 6. Note that the SDSS QSO sample is
incomplete at i′ > 19.1 mag ,
indicated by open squares in the figure.
To match the number counts of SDSS QSOs, we con-
vert KS4 magnitudes of our QSO candidates to SDSS
i′-band magnitude using the combination of transforma-
tion equations for z ≤2.1 QSOs by Jester et al. :
i′ = V −0.19 −0.9 + 0.18.
Accurately determining the effective survey area is
crucial for precise estimation of the number counts.
Considering the gaps between KMTNet CCD chips care-
fully, we introduce the Hierarchical Equal Area isoLat-
Table 2. Effective Survey Area
HEALPix levels
Note—The units of b and area are deg and
deg2, respectively. The last column is for the
number of QSO candidates.
itude Pixelation of a sphere , which divides the sphere’s surface into uniform-
sized areas according to given levels.
We count the
HEALPix pixels covering the reliable sources in the KS4
catalog .
Figure 7 shows
examples of the source-matched pixels depending on
HEALPix levels ranging from 11 to 13, corresponding
to spatial resolutions of 2.95, 0.74, and 0.18 arcmin2,
respectively. There are gaps between the CCD chips,
which are not covered by at least four dithered obser-
vations.
As the level goes higher, the polygons trace
the shape of the gap sharply, while the unoccupied re-
gions by bright stars appear more as well. However, at
higher HEALPix levels, the finer resolution results in
Unobscured QSOs with KS4
i ′ 
Ni ′ 
i ′ 
Ni ′/Ni ′, SDSS 
SDSS DR16
DES DR2
Figure 6. Differential number counts of KS4 QSO candidates in the i′-band  at |b| > 10◦. The gray squares
represent spectroscopically identified unobscured QSOs in SDSS , while the open symbols indicate incomplete
bins. Similarly, the filled and open sky-blue triangles are for the DES DR2 QSO candidates . The inset
shows the number counts of KS4 QSO candidates with varying galactic latitude limitations, compared to those of SDSS QSOs.
some pixels being excluded from the calculation due to
their location in regions with no detectable sources, even
though these regions are covered more than four times
of the survey area. Consequently, we adopt the effec-
tive area under the assumption of HEALPix level 12.
The sizes of effective survey areas according to galactic
latitude  limit are listed in Table 2.
The red circles in Figure 6 represent the differen-
tial number counts of KS4 QSO candidates located at
generally align with those observed in the SDSS QSO
sample, they are marginally lower in the complete bins
several factors:  a potentially missing population not
captured by our selection criteria,  the uncertainties
associated with the estimation of the SDSS survey area
size , and  the omission
of precise corrections for selection completeness, which
approaches unity  at i ≲19 mag. Nevertheless,
the power-law slopes of 0.85 for KS4 and 0.83 for SDSS
within the magnitude range of 16 < i′ < 19 suggest that
our selection method effectively reproduces the observed
abundance of real QSOs.
The inset compares the number counts of our candi-
dates, defined by various b limits, normalized to those
of the SDSS QSOs. This comparison shows consistent
trends across different b thresholds, further validating
the efficacy of our selection criteria. In particular, no no-
ticeable number excess between |b| > 10◦and |b| > 40◦
suggests that our method effectively excludes stellar con-
taminants.
Yang & Shen  recently published a photometric
catalog of QSO candidates from the DES Data Release
log utilizes optical-to-IR photometry via a probabilistic
approach to identify candidates.
The number counts
from this sample, represented by sky-blue triangles in
Figure 6, are about twice as high as those from our can-
didates. Note that the sample used here adheres to the
higher-purity recommendations – in Yang
shifts lower than 2. Similar to our comparison with the
SDSS sample, the noted discrepancy could stem from
the previously mentioned factors. Additionally, differ-
ences may also arise from the inclusion of host-dominant
AGNs from the Miliquas catalog in the training sample,
which our selection criteria may largely overlook. The
power-law slope between 16 < i′ < 19 is slightly flatter
at 0.78, potentially reflecting the broader inclusivity of
their targeting strategy.
Decl. 
n = 11
Decl. 
n = 12
R.A. 
Decl. 
n = 13
Figure 7. Changes in sky coverage depending on HEALPix
Levels of 11 , 12 , and 13 . Each panel
presents the effective area coverage for a given HEALPix
level, with areas from the n = 11 level superimposed to
enable direct comparison. The unoccupied regions are at-
tributable to the gaps between the KMTNet CCD chips,
representing incomplete regions not covered by at least four
dithered observations or the areas affected by bright stars.
We further emphasize the complementarity in sky cov-
erage between the KS4 QSO catalog and the DES DR2
catalog of Yang & Shen . These two catalogs cover
most of the southern sky , providing an ex-
tensive resource for QSO studies.
The Gaia mission is designed to observe bright sources
PQSO > 0.5
PGal > 0.5
PStar > 0.5
Gaia DR3 others
Not in Gaia DR3
i ′ 
Figure 8.
Stacked histogram  and fractional distri-
bution  of our QSO candidates, with classifications
based on inclusion in the Gaia DR3 dataset. The colors rep-
resent different classifications based on probability thresh-
olds: red for PQSO > 0.5, orange for PGal > 0.5, and yellow
for PStar > 0.5. Gray indicates those within Gaia DR3 that
do not meet any of the specified probability criteria, while
light-gray represents candidates not matched to Gaia DR3
sources.
probability of being a QSO  determined by the
discrete source classifier . The Combmod proba-
bility is determined from the combination of class infor-
mation from Specmod  and Allosmod
they provide the probabilities of being a galaxy 
and a star .
In Figure 8, we present the fractional distribution of
our candidates as classified by the probabilities in Gaia
DR3.
According to Gaia DR3’s source classification,
sources are deemed QSOs, galaxies, or stars based on
the highest posterior probability exceeding 0.5. Notably,
only 64 % of our candidates are likely classified as QSOs
with PQSO > 0.5 , while a significant
portion  of the remaining sources are likely clas-
sified as stars .
This classification disparity appears to contradict the
high completeness reported for QSO and star classifi-
cations using Combmod . However, it is important to note that
these probabilities were determined using only Gaia data
in optical wavelengths.
Even if an object is a point
source with PStar > 0.5, the presence of detection in the
WISE bands suggests a significant possibility that the
object might not be a star, except for very bright sources
Unobscured QSOs with KS4
Both in Quaia and this work 
Only in this work 
Only in Quaia, NDITH-rejected 
Only in Quaia, color/SNR-rejected 
Only in Quaia, SED-fit-rejected 
i ′ 
Figure 9. Stacked histogram  and fractional distribu-
tion  of QSO candidates at |b| > 10◦selected in
Quaia and this work.
The gray histogram represents the
candidates both in Quaia and this work. The red histogram
indicates the candidates only in this work. The navy, blue,
and sky-blue histograms are those rejected due to NDITH,
color/SNR criteria, and SED-fit results, respectively.
shown in Figure 4 and by the changes in the number
distribution after the SNR cut in the WISE bands due
to the inclusion of bright stars, shown in Figure 2. Fur-
thermore, cross-matching with the Milliquas catalog in
Section 4.1 reveals that 20 % of spectroscopically identi-
fied QSOs are assigned PStar > 0.5. Additionally, the
effectiveness of our approach in rejecting stellar con-
tamination, strengthened by the consistent trends in the
number counts across different b thresholds ,
suggests that not all sources with PStar > 0.5 in our
candidates are indeed stars. These raise concerns about
the reliability of using PQSO alone for accurate QSO se-
lection.
Given doubts about the low purity  of the Gaia
DR3 QSO candidates with PQSO selection , Storey-Fisher et al.  recently generated an
all-sky QSO catalog named Quaia, based on the combi-
nation of the Gaia DR3 and unWISE  colors,
yielding a more reliable set of QSO candidates. There
are 64,734 Quaia candidates located within the effective
survey area defined by HEALPix level n = 12, 2542 of
which are not identified in the KS4 catalog. This omis-
sion may arise from the marginal inconsistency between
the imaging data and HEALPix patches. Indeed, about
the effective survey area estimation. The fraction nat-
urally decreases if we introduce an effective survey area
defined by a higher HEALPix level . We also note that the screening with SSFLAG
has no effect because it excludes known sources.
In Figure 9, we present the i′-band distribution of
these Quaia candidates matched to the KS4 catalog and
our sample, considering the 38,534 candidates selected
both in Quaia and our work . On the
other hand, 23,593 Quaia candidates  are not se-
lected by our selection criteria; they are rejected due
to  the lack of the number of dithered observations
we select Quaia candidates using the KS4 HEALPix
maps, which cannot perfectly trace our survey area, es-
pecially at the CCD gaps. Unlike our method, Storey-
Fisher et al.  used only the W1 and W2 band
magnitudes from the unWISE catalog, which is deeper
than the AllWISE that we used, potentially explain-
ing the discrepancies in candidate selection due to SNR
and colors.
Indeed, the magnitude differences in W1
and W2 between unWISE3 and AllWISE for the Quaia
candidates excluded based on color/SNR exhibit larger
standard deviation 
compared to those candidates selected both in Quaia
and in our analysis .
This may also be attributed to either the MIR variabil-
ity between the two surveys  or
the confusion with nearby sources. On the other hand,
those rejected due to the poor SED-fitting results occupy
a large fraction of bright QSO candidates .
Most of them have χ2
QSO values higher than 2σ level, in-
dicating that they are unlikely to be probable QSOs or
nearby QSOs with the bright and extended host galax-
ies, in which the systematic uncertainties in the multi-
wavelength photometry can be significantly larger than
nucleus dominated objects.
The number of QSO candidates only in our work  is significantly high, predominantly consist-
ing of faint sources that are not likely to be observable
with Gaia. However, even at i′ ≲20 mag, the fraction
of these candidates remains nonnegligible; about 17 %
at 15 < i′ < 20.
As we discussed above, the selec-
tion efficiency of our method, validated by the recovery
rate of spectroscopically confirmed QSOs, and the num-
to find the matched sources in the unWISE catalog .
ber counts consistent with the other surveys, strength-
ens the fact that our QSO candidates are promising.
Therefore, spectro-photometric surveys, such as 7DS
and SPHEREx, will enable us to estimate the effective-
ness of our selection method rigorously and to constrain
the unobscured QSO population in the southern sky.
In this study, we present a catalog of unobscured
QSO candidates in the southern sky.
We mainly use
the KS4 interim data, which covers ∼2500 deg2 area
around the south ecliptic pole and achieves 5σ imaging
depths of ∼22.1–22.7 mag in the BV RI bands. Com-
bining this KS4 data with infrared photometric data
from the 2MASS , VHS , and AllWISE
for the initial selection and apply the SED fitting to re-
fine our list of plausible QSO candidates. The final cat-
alog consists of 72,964 candidates for unobscured QSOs
over an effective survey area of ∼2000 deg2. Despite
only 0.4 % of these candidates being spectroscopically
confirmed QSOs so far, the high recovery rate of 87 % for
QSOs with I < 20 mag at z < 2 proves the robustness of
our selection method. Moreover, this is also supported
by the number counts of our candidates, which are con-
sistent with those of the spectroscopically confirmed
QSOs from SDSS in the northern hemisphere. Moving
forward, upcoming spectro-photometric surveys, such as
SPHEREx and 7DS, are expected to provide valuable in-
sights into the true nature of these candidates, thereby
enhancing our understanding of QSO populations in the
southern sky.
We thank the anonymous referee for valuable sugges-
tions that greatly improved the manuscript. This work
was supported by the National Research Foundation of
Korea  grant funded by the Korean government
MI, SWC, JMK acknowledges support from the Na-
tional Research Foundation of Korea  grants,
No. 2020R1A2C3011091 and No. 2021M3F7A1084525
funded by the Ministry of Science and ICT . This
research was also supported by Basic Science Research
Program through the NRF funded by the Ministry of
Education .
This research has made use of the KMTNet system op-
erated by the Korea Astronomy and Space Science Insti-
tute  at three host sites of CTIO in Chile, SAAO
in South Africa, and SSO in Australia. Data transfer
from the host site to KASI was supported by the Korea
Research Environment Open NETwork .
This publication makes use of data products from the
Wide-field Infrared Survey Explorer, which is a joint
project of the University of California, Los Angeles, and
the Jet Propulsion Laboratory/California Institute of
Technology, and NEOWISE, which is a project of the Jet
Propulsion Laboratory/California Institute of Technol-
ogy. WISE and NEOWISE are funded by the National
Aeronautics and Space Administration.
This publication makes use of data products from the
Two Micron All Sky Survey, which is a joint project
of the University of Massachusetts and the Infrared
Processing and Analysis Center/California Institute of
Technology, funded by the National Aeronautics and
Space Administration and the National Science Foun-
dation.
The VISTA Hemisphere Survey data products served
at Astro Data Lab are based on observations collected at
the European Organisation for Astronomical Research
in the Southern Hemisphere under ESO programme
The national facility capability for SkyMapper has
been funded through ARC LIEF grant LE130100104
from the Australian Research Council, awarded to the
University of Sydney, the Australian National Univer-
sity, Swinburne University of Technology, the Univer-
sity of Queensland, the University of Western Australia,
the University of Melbourne, Curtin University of Tech-
nology, Monash University and the Australian Astro-
nomical Observatory. SkyMapper is owned and oper-
ated by The Australian National University’s Research
School of Astronomy and Astrophysics. The survey data
were processed and provided by the SkyMapper Team
at ANU. The SkyMapper node of the All-Sky Virtual
Observatory  is hosted at the National Compu-
tational Infrastructure . Development and support
of the SkyMapper node of the ASVO has been funded
in part by Astronomy Australia Limited  and the
Australian Government through the Commonwealth’s
Education Investment Fund  and National Col-
laborative Research Infrastructure Strategy ,
particularly the National eResearch Collaboration Tools
and Resources  and the Australian National
Data Service Projects .
This work has made use of data from the Euro-
pean Space Agency  mission Gaia , processed by the Gaia Data Pro-
cessing and Analysis Consortium .
for the DPAC has been provided by national institu-
tions, in particular the institutions participating in the
Gaia Multilateral Agreement.
Facilities: KMTNet, IRSA
Unobscured QSOs with KS4
Software:
astropy , LePhare++ ,","[('qsos', 79), ('qso', 65), ('band', 59), ('survey', 58), ('k', 56), ('candidate', 52), ('source', 47), ('data', 44), ('magnitude', 42), ('area', 38)]","[(('qso', 'candidate'), 30), (('survey', 'area'), 21), (('band', 'magnitude'), 19), (('unobscured', 'qsos'), 14), (('number', 'count'), 14), (('southern', 'sky'), 13), (('qsos', 'k'), 12), (('type', 'qsos'), 12), (('spectroscopically', 'confirmed'), 11), (('selection', 'criterion'), 10)]","[(('spectroscopically', 'confirmed', 'qsos'), 9), (('error', 'band', 'magnitude'), 8), (('effective', 'survey', 'area'), 7), (('unobscured', 'qso', 'candidates'), 6), (('unobscured', 'qsos', 'ks'), 6), (('catalog', 'unobscured', 'qso'), 5), (('bv', 'ri', 'bands'), 5), (('ks', 'survey', 'area'), 5), (('qsos', 'ks', 'survey'), 4), (('spectro', 'photometric', 'surveys'), 4)]"
2410.17189v1.txt,"Draft version October 23, 2024
Typeset using LATEX twocolumn style in AASTeX62
Temporal and Spectral Analysis of the Unique and Second Brightest Gamma-Ray Burst GRB 230307A: Insights
from GECAM and Fermi/GBM Observations
R. Moradi,1 C. W. Wang,1, 2 B. Zhang ,3, 4 Y. Wang ,5, 6, 7 S.-L. Xiong ,1 S.-X. Yi,1 W.-J. Tan,1, 2
M. Karlica,8 and S.-N. Zhang 1, 2
Republic of China
In this study, we present the pulse profile of the unique and the second brightest gamma-ray burst
GRB 230307A, and analyze its temporal behavior using a joint GECAM–Fermi/GBM time-resolved
spectral analysis. The utilization of GECAM data is advantageous as it successfully captured sig-
nificant data during the pile-up period of the Fermi/GBM. We investigate the evolution of its flux,
photon fluence, photon flux, peak energy, and the corresponding hardness-intensity and hardness-flux
correlations. The findings within the first 27 seconds exhibit consistent patterns reported previously,
providing valuable insights for comparing observations with predictions from the synchrotron radiation
model invoking an expanding shell. Beyond the initial 27 seconds, we observe a notable transition in
the emitted radiation, attributed to high latitude emission , influenced by the geometric prop-
erties of the shells and the relativistic Doppler effects. By modeling the data within the framework
of the large-radius internal shock model, we discuss the required parameters as well as the limitations
of the model. We conclude that a more complicated synchrotron emission model is needed to fully
describe the observational data of GRB 230307A.
Keywords: gamma-ray bursts: general — gamma-rays: general — stars: — : general —
A comprehensive understanding of the physical origins
of gamma-ray burst  pulses is crucial to unravel
the complexities of the overall GRB phenomenon and to
shed light on the nature of the inner engine and associ-
ated radiation processes .
The individual GRB pulses typically exhibit a charac-
teristic temporal evolution, characterised by a rapid rise
rmoradi@ihep.ac.cn;cwwang@ihep.ac.cn;bing.zhang@unlv.edu;
yu.wang@inaf.it;xiongsl@ihep.ac.cn
followed by an exponential decay, often referred to as
the “FRED” shape .
However, statistical investigations of the decay phase
of smooth FRED pulses within the Burst and Transient
Source Experiment  have shown that many of
these pulses do not conform to exponential behaviour
Therefore, the exploration of alternative functions, such
as power-law functions, has been proposed to explain
this decay trend .
During the decay phase, the spectral hardness of GRB
pulses demonstrates a noticeable decrease. A significant
fraction of pulses in this phase appear to satisfy spe-
arXiv:2410.17189v1    22 Oct 2024
cific relations between their temporal and spectral prop-
erties, such as the hardness-intensity correlation 
lation  .
Based on these empirical findings, and with the goal of
comprehending the radiative mechanisms responsible for
the cooling process, it is essential to concentrate on the
decay phase of GRBs’ prompt emission. Furthermore,
it is vital for theoretical models to replicate these ob-
servational results, as they can help distinguish among
various possibilities; .
In this regard, in this paper we conduct a comprehen-
sive investigation into the spectral and temporal charac-
teristics of GRB 230307A, the second brightest gamma-
ray burst  detected in over 50 years, character-
ized by its long-duration prompt emission .
The unique features of
GRB 230307A, along with the similar properties of GRB
softer temporally extended emission  and kilonova
pact stellar merger origin of GRB 230307A . The emer-
gence of an X-ray component after the prompt γ-ray
emission phase suggests a magnetar engine  ). Detailed analy-
ses of the temporal spectral behavior suggest that the
emission is likely synchrotron radiation from a Poynting-
flux-dominated jet, with evidence of mini-jets suggested
in the internal-collision-induced magnetic reconnection
and turbulence  model , even
though other possibilities have also been discussed .
The high brightness of GRB 230307A caused the
Fermi Gamma-ray Burst Monitor  to encounter pulse pile-up and data loss
during the prompt emission from T0+1 s to T0+20 s
This issue was
also observed in the case of another bright GRBs, e.g.
GRB 221009A . However, the Gravi-
tational wave high-energy Electromagnetic Counterpart
All-sky Monitor , success-
fully obtained significant data during the pile-up period
of the Fermi/GBM, showcasing its unique capability to
capture data from highly luminous GRBs. This ability
of GECAM provides valuable insights into the nature of
exceptionally bright GRBs.
Utilizing data obtained from both GECAM and
Fermi/GBM, we perform a time-resolved spectral anal-
ysis during the first 100 seconds of GRB 230307A. Our
analysis, as shown in section 2 reveals consistent be-
havior with the Hardness-Intensity Correlation 
and Hardness-Fluence Correlation  relations as
reported by Ryde & Svensson  derived from
a sample of 25 bursts.
In view that this spectral evolution behavior is con-
sistent with a general synchrotron radiation model in-
volving a large emission radius, we investigate how
the model may be consistent with such models, espe-
cially the widely discussed internal shock model .
We consider relativistic ejection as a continuous pro-
cess, with internal shocks taking the form of propagat-
ing shock waves within the outflow. We utilize a ver-
sion of the internal shock model developed by Daigne
one pair of internal shocks at a large emission radius.
This approach results in pulse shapes during the decay
phase, determined by the hydrodynamical timescale as-
sociated with the propagation of these shock waves.
We point out the limitations of this model and discuss
possible solutions to improve the model. In particular,
we echo the suggestion of Yi et al.  that the full
dataset of GRB 230307A demands a more complicated
large-radius synchrotron radiation model invoking mini-
jets, as envisaged within the framework of ICMART
model .
The paper is organized as follows: Section 2 presents
the detailed methodology of the time-resolved spec-
tral analysis applied to GRB 230307A using data from
GECAM and Fermi-GBM. In section 3, we provide a
brief overview of the internal shocks  model em-
ployed in this study.
Section 4 focuses on investigat-
ing the astrophysical model parameters required in the
IS model to explain the observed temporal and spectral
evolution of GRB 230307A and the limitations of the
model. Finally, in section 5, we present the concluding
remarks of our study.
On March 7, 2023, at 15:44:06 UT  the Gravi-
tational wave high-energy Electromagnetic Counterpart
All-sky Monitor  was triggered in-flight by
this exceptionally bright long burst and the low latency
alert reporting the GECAM discovery of this extremely
bright burst initiated a global observation campaign
The GECAM light curve shows
a roughly FRED shape with a possible precursor and
an overall duration of about 100 seconds .
The GECAM duration  of this burst is
At the same time, the Fermi Gamma-
ray Burst Monitor  detected and located GRB
a single burst in 10-1000 keV with a duration  of
approximately 35 seconds . With
a redshift estimated to be z ∼0.065 , the isotropic energy measured by GECAM-B is
Eiso = 4.8×1052 erg.
As of the detection of GRB 230707A, GECAM con-
sists of three separate satellites: GECAM-A, GECAM-
B, and GECAM-C. GECAM-A and GECAM-B were
launched in December 2020 , while
GECAM-C,
SATech-01
satellite,
launched in July 2022 .
Each GECAM satellite has two kinds of scientific in-
struments: gamma-ray detectors  and charged
particle detectors .
There are 25 GRDs on-
board GECAM-B and 12 GRDs onboard GECAM-C.
GRB 230307A was detected by both GECAM-B and
GECAM-C, and neither GECAM-B nor GECAM-C suf-
fered saturation during the event. GRD04 of GECAM-
B and GRD01 of GECAM-C are selected for spectral
analysis due to their smallest zenith angles to the burst.
These two detectors operate in two readout channels:
high gain  and low gain , which are indepen-
dent in terms of data processing, transmission, and dead
time.
For GRD04 of GECAM-B, the energy range of the HG
channel data is used from about 40 keV to 300 keV, while
the energy range of the LG channel data is used from
about 700 keV to 8000 keV. For GRD01 of GECAM-
C, only the HG channel data is used with an energy
range from 6 keV to 100 keV. It should be noted that
the response of GRD01 of GECAM-C for the 6-15 keV
range is affected by certain instrumental effects, which
may result in inaccuracies. This issue is currently being
studied. The background of GECAM-B is estimated by
fitting the data from T0-50 s to T0-5 s and T0+160 s to
T0+200 s with first order polynomials.
The background of GECAM-C is estimated by fitting
the data from T0-20 s to T0-1 s and T0+170 s to T0+600 s
with a combination of first and second-order exponential
polynomials.
It is noteworthy that GECAM features a unique and
optimized design specifically for detecting extremely
bright GRBs.
In addition to employing transmission
channels with large capacity and broad bandwidth, ad-
jacent detectors with similar pointing are read out by
independent electronic modules.
Thanks to these so-
phisticated designs, the count rate upper limit of each
GRD can reach up to 100 kcps, significantly exceeding
the count rate of GRB 230307A detected by GECAM.
In this study, we also utilize the Time-Tagged Event
one of the two scientific instruments on board the Fermi
satellite, the other being Fermi/LAT. The Fermi/GBM
instrument is equipped with two types of detectors,
namely 12 Sodium Iodide  detectors  and 2
Bismuth Germanate  detectors .
Due to the high brightness of GRB 230307A, GBM ex-
perienced pulse pile-up and data loss during the prompt
emission, resulting in the creation of Bad Time Intervals
To supplement the GECAM data when the source is
not as bright ,
we carry out a joint analysis using GBM data, using the
na detector ranging from 8 keV to 900 keV, and the b1
ranging from 0.3 MeV to 8 MeV.
The spectral analysis is conducted using the Pyxspec
software .
The exceptional
brightness of GRB 230307A enables the execution of
spectral analysis with a significantly high time resolu-
tion. The time intervals used in this study are deter-
mined based on the results of the Bayesian block anal-
ysis , with additional adjustments
made based on the fitting results.
Specifically, during the period of high brightness from
T0 to T0+15 s, the time intervals are set to 0.1 s in or-
der to achieve a more refined time-resolved spectrum.
During the period of decreased burst intensity, specifi-
cally from T0+ 15 s to T0+ 32 s, the time intervals are
adjusted to 0.5 s. This choice allows for a more refined
analysis of the dip spectrum, ensuring an adequate num-
ber of photons for accurate fitting. Following T0+32 s,
the time intervals are primarily determined based on the
Bayesian blocks analysis results. However, these inter-
vals are further adjusted multiple times to ensure opti-
mal fitting of each bin, aiming for narrow and well-fitted
bins.
Three distinct photon spectral models are chosen for
analysis: BAND model , the power law
with a high-energy exponential cutoff  model, and
the simple power law  model. In addition, we incor-
porate the uncertainties arising from cross-calibration
by introducing multiplicative factors between different
detectors. Table 1 presents the best-fit parameters ob-
tained for each time interval of GRB 230307A. The
columns include the time interval, the best-fit model,
the α parameter for the PL, CPL, or BAND function
when applicable, the β parameter for the BAND func-
tion in intervals where it is the best fit, the peak energy,
the flux, the photon flux, the CSTAT value, and the de-
grees of freedom  to assess the goodness of fit for
the best model. Additionally, we thoroughly explored
the potential need for an additional spectral component,
such as a thermal component, to fit the spectra. How-
ever, we did not find any statistically significant evidence
supporting the necessity of such an extra component.
The optimal model is determined using the Bayesian
Information Criterion . The BIC is
defined as BIC = −2lnL+klnN, where L represents the
maximum likelihood value, k denotes the number of free
parameters in the model, and N signifies the number of
data points. A model with a lower BIC value is pre-
ferred, particularly when the difference in BIC 
exceeds 10. In the best model, all parameters should be
well–constrained, and the multiplicative factors should
be approximately equal to 1.
The characteristics of the gamma-ray of the burst can
be inferred from the temporal changes in the light curve
and the spectral properties; . These observations have uncovered various sta-
tistical relationships between different measurable quan-
tities, providing valuable insights and limitations on the
physical models that govern the production of gamma-
rays .
The advent of new satellites like GECAM  has further enhanced our ability to study and un-
derstand these phenomena.
As explained in the previous subsections, we examined
the potential evolution of the spectral shape during the
decay phase of GRB 230307A, but no significant changes
were observed; see also Table. 1. We now study the tem-
poral behaviour of the energy flux FE, the photon flux
N and spectral peak energy Ep. FE is related
to N and Ep by 
FE = NEp
φ =
Z E2/Ep
E1/Ep
Ndx,
φ0 =
Z ∞
xNdx,
with N representing the spectral shape. Figure. 1
shows the joint GECAM+Fermi/GBM evolution of the
above functions, namely FE, N, Ep, φ,
as well as the photon fluence Φ. These functions are
obtained from the best fit parameters determined in Sec-
tion 2, and reported in Table. 1.
Ryde & Svensson  have shown that the decay
behavior of the photon flux, N, is a consequence of
the validity of both the Hardness-Intensity Correlation
Ep ∝e−ΦN/Φ0 ,
law during the decay phase:
N =
n ,
phase, and it serves as a reference point from which
the decay behavior of the photon flux N = N0 is
measured, and τ is a constant. Subsequently, Ryde &
Svensson  found that the distribution of n in their
sample was peaked around n = 1.
Based on our comprehensive analysis of data obtained
from the joint analysis of GECAM and Fermi/GBM, we
find that none of FE, Ep, and N can be accu-
rately described by a single decaying power law over the
entire post maximum emission period, extending from
the initial decay time td1 = 7.2 s to t ∼100 s.
We then investigate the behavior of the flux and fit it
using a broken power-law model. The best fit parame-
ters, as shown in Fig. 2 reveals a break time t = 27±2
s, with the first index α1 = 1.55 ± 0.10 and the second
index α2 = 2.98 ± 0.16. Therefore, we denote the break
time of the flux as td2 = 27 s, representing the initiation
of the second decay phase of the flux.
Within the range of td1 = 7.2 s to td2 = 27 s, we ob-
serve that FE ∝1/t, Ep ∝1/t,
N ∝1/t, and φp ∝E
Figs. 3 and 5.
These lead to HIC and HFC relations
Ep ∝N
Ep ∝e−ΦN/ ,
the peak energy Ep and the associated energy flux,
FE, believed to be intrinsic to the emission process,
is consistent with the correlations identified by Liang
et al. , Ghirlanda et al. , Lu et al. ,
Frontera et al. , and Dainotti et al. .
tp = 4.95 s
td1 = 7.2 s
td2 = 27 s
evolution, FE, in erg.cm−2.s −1. The second panel shows the photon fluence evolution, Φ, in photons.cm−2. The third
panel shows the photon flux evolution, N, in photons.cm−2.s −1. The forth panel shows the spectral peak energy evolution,
Ep, in keV. The fifth panel shows φp defined in Eq. 1. The peak of the pulse is tp = 4.95 s. The interval from td1 = 7.2
s to td2 = 27 s indicates the time interval of the decay phase studied.
Beyond td2 = 27 s, the decay behavior follows FE ∝
temporal behaviour aligns with the characteristics of
high latitude emission , which persists after the
on-axis emission from the final dissipating regions in the
relativistic outflow ceases; . Such a HLE emission is in general consistent with
the cessation of synchrotron emission in a large emis-
sion radius, such as the ICMART model  and the large-
radius internal shock model  ). The photosphere models  have a much smaller emission
radius, so that the high-latitude emission timescale is
expected to be much shorter and exhibit a rapid de-
cay , which is inconsistent with
Post-Maximum Flux: broken power-law
tp = 4.95 s
td1 = 7.2 s
td2 = 27 s
Figure 2. The post maximum flux F best–fitted with a broken power-law model. The best fit parameters indicate a break
time at 27±2 s, with the first index α1 = 1.55 ± 0.10 and the second index α2 = 2.98 ± 0.16.
the data. Furthermore, the spectral analysis presented
in Table 1 shows no evidence of statistically significant
thermal components, which are essential indicators of
photospheric emission in such analyses.
In the subsequent analysis, it is therefore crucial to in-
vestigate the decay behavior of FE, Ep, and N
within a general framework of synchrotron radiation at a
large emission radius. A practical model is the large ra-
dius internal shock model , because it has less parameters than
the ICMART model that invokes multiple minijets  and Shao & Gao  for de-
tailed modeling) and because the two models share sim-
ilar global properties in terms of FE, Ep, and N
evolutions. We want to, however, emphasize the caveats
of the IS model. Since additional rapid variability is ob-
served on top of the broad pulse profile of GRB 230307A,
in order to fully interpret the data within the IS model,
additional small-radius shocks are needed to account for
the rapid variability. The superposition between small-
radius ISs and a large-radius IS is actually disfavored
by the data of GRB 230307A . Nonethe-
less, the goal of the following investigation is to see how
good the simplest IS model can reproduce the general
behavior of this burst.
The internal shock model, introduced by Rees &
Meszaros , explains the GRBs originating from
compact stellar-mass objects. It suggests that a wind
emitted by such objects has an uneven distribution
of speed, resulting in faster sections catching up with
slower ones, generating gamma-rays through radiative
processes such as synchrotron or inverse Compton radi-
ation through internal shocks ;
Internal shocks  were initially thought to be dis-
crete shell collisions , but this led
to discrepancies with observed GRB decay rates due to
the “curvature effect.” In the multiple shells IS scenario,
the long-term overall trend of the light curve reflects the
activity history of the central engine, showing neither
shape-energy dependence nor any power-law behavior
in the luminosity trend. This random behavior is not
surprising, as the cluster of shells is randomly generated
and possesses random mass and velocity.
Within a relativistic wind characterized by a nonuni-
form Lorentz factor, internal shocks occur and transform
a fraction of the kinetic energy into radiation. Our ap-
proach involves depicting the irregular wind as a series
of relativistic shells, with the collision of two shells con-
stituting the fundamental process in this model. When
multiple shells are present, numerous collisions occur.
To delineate the temporal structure, we compute the
time sequence of these two-shell collisions and overlay
the resultant pulses from each collision.
A rapidly-moving shell 
overtakes a slower one , and the two combine to cre-
ate a single merged shell . This system mimics an
inelastic collision between two masses mr and ms. By
applying the principles of energy and momentum con-
servation, we determine the Lorentz factor of the merged
shell to be :
γm ≃
γrmr + γsms
γr + ms
Post-maximum: Ep ∝F 0.48
log10  = 4.74+0.02
r = 0.94
B = 0.48+0.00
Post-maximum: Ep ∝e−Φ/Φ0
r = 0.70
Φ0 = 559.94+2.37
Post-maximum: Ep ∝N 1.52
r = 0.72
B = 1.52+0.03
p-scaled
E−0.49
r = 0.71
B = −0.49+0.03
Figure 3. The correlations between the peak energy, Ep, and various parameters such as A: the flux, FE, N, B:
photon fluence, C: photon number, Φ, and D: φp, are explored in our analysis. The relationship between Ep and
FE aligns with the findings of previous studies by Liang et al. , Ghirlanda et al. , and Lu et al. . The
Hardness-Fluence Correlation  is illustrated panel B, while the Hardness-Intensity Correlation  is shown in the
panel C. The optimal fitting time interval ranges from td1 = 7.2 to td2 = 27 seconds. The correlation coefficients of 0.94, 0.70,
Ep with FE, N, and φp are performed using a logarithmic model: log = A + B · log, where x represents
FE, N, and φp.
Normalized count rate
IS in 6-8000 keV
Figure 4. The plot represents the normalized count rate ob-
tained from the simulation of randomized shell collisions in
the IS model versus time. 50 shells are ejected, each charac-
terized by randomly chosen initial thickness, Lorentz factor,
and mass from log-distributed random distributions.
Lorentz factors range from 100 to 1000, the mass spans from
engine follow a linear distribution, with values ranging from
Subsequently, each collision releases an internal energy
as described by:
Eint = mrc2 + msc2.
η =
L, is expressed as:
L =
h,
hc/R]
t > δte/2γ2
m.
m/δte, R represents the collision ra-
dius, and δte denotes the emission timescale at the mo-
ment when the reverse shock crosses the fast-moving
shell. This is expressed as δte = lr/c, where
lr is the width of the rapid shell, βr = 1/2,
and βrs = 1/2, being γrs the Lorentz factor of
reverse shock approximated as:
γrs ≃γm
leased in a single emission event, specifically relevant
to GRB 230307A, characterized by immediate emission
without distinct intervals between pulses. By simulating
the ejection of multiple randomized shells from the cen-
tral engine, we are able to closely monitor the properties
of each individual shell. For instance, we eject 50 shells,
with their initial thickness, Lorentz factor, and mass
chosen from random distributions in log space.
Lorentz factors vary from 100 to 1000, the mass ranges
from 1028 to 1029, and the initial thickness ranges from
ejection times are randomly selected from a linear distri-
bution, with values ranging from 0 to 20 seconds in the
rest frame of the GRB central engine. Upon collision,
two shells merge, and the merged shell parameters  are adopted as the new values for the remaining
shell.
To ensure the tracking of future collisions, the
tej,m = R/cβm, where βm = 1/2. The simu-
lation then restarts with one less shell. This process is
repeated for each collision, allowing the code to monitor
all collision/merging events for any arbitrarily designed
central engine activity .
To determine Ep, we utilize an empirical relation-
ship between the isotropic emission energy Eiso and Ep,
which has been found to be widely applicable to GRBs
time-resolved analysis of individual bursts . This relationship has also
been demonstrated to be valid in GRB 230307A, as il-
lustrated in Fig. 3. We assume the validity of this cor-
relation, represented by the equation:
Ep = 100 keV
The photon flux N in the energy channel 
is approximately given by Eq. 1:
N12 ≃
Z E2,obs/Ep,obs
E1,obs/Ep,obs
Ndx , 
where DL denotes the luminosity distance, LF  repre-
sents the superposition of all L from each collision as
shown in Eq. 10.
For GRB 230307A, the spectral shape is described by
the CPL model, defined as N = Aαexp,
where A is the normalization factor in units of ph
cm−2keV−1s−1,
is the pivot energy fixed at
Post-Maximum Flux 
tp = 4.95 s
td1 = 7.2 s
td2 = 27 s
IS model: Flux 
HLE: Flux 
Post-Maximum Ep
tp = 4.95 s
td1 = 7.2 s
td2 = 27 s
IS Model: Ep
HLE Ep
Photon Flux 
Post-Maximum N 
tp = 4.95 s
td1 = 7.2 s
td2 = 27 s
IS model: N 
HLE: N 
Figure 5. The best fit parameters of FE, Ep, and N during the decay phase of GRB 230307A are depicted in the plot.
It is evident that fromt td1 = 7.2 to td2 = 27 s, the best fit results align closely with the predictions of the internal shock model.
Beyond td2 = 27 s, the observed emission corresponds to the High-Latitude Emission  phase.
tral index, and Ec is the break energy in units of keV
listed in Table. 1. The simulation results for  are depicted in Fig. 4. The
light curve does not follow a specific trend, and the
trend changes for each set of random variables.
As outlined in the preceding subsection regarding the
multiple shells IS scenario, where a cluster of shells is
generated randomly with varying mass and velocity, the
extended light curve demonstrates neither a correlation
between shape and energy nor any power-law pattern
in luminosity trends. To address this, a different model
proposed by Daigne & Mochkovitch  consid-
ers continuous relativistic ejections, resulting in shock
waves propagating within the outflow.
This modified
approach effectively considers one single collision at a
large emission radius, which better matches the observed
data, showing slower pulse decays . The simplified model in this context by Daigne &
Mochkovitch  approximates the flow evolu-
tion, treating the wind as solid shells interacting through
collisions, while disregarding pressure waves present in
a full hydrodynamical scenario .
In our investigation of the decaying phase of the burst,
we employ a simplified pulse analytic model proposed
by Daigne & Mochkovitch . Given that our focus
is solely on the overall decaying behavior and not on
calculating variability in this particular burst , we have chosen to utilize this analytical
model in this work due to its advantageous analytical
properties.
The analytical solution involves a relativistic wind,
wherein at time t = t0 a slower shell with mass M0 and
Lorentz factor Γ0 starts decelerating a faster portion
of the flow characterized by a constant rest-frame mass
flux ˙M and Lorentz factor Γ1 > Γ0. Being t the observer
time, and γ = Γ/Γ1, with Γ and M being the current
Lorentz factor and mass of the slow shell. Due to the
accretion of rapidly moving material, the Lorentz factor
of the slow shell increases. When a mass element dM is
accreted, the Lorentz factor becomes
Γ + dΓ =
Γ1ΓΓ1dM + ΓM
ΓdM + Γ1M
dt Γϵc2 =
at large τ
comoving frame, t0 = M0/ ˙M, τ = t/t0, and initial γ0 =
Γ0/Γ1 . For decaying
part, γ is approximated by
γ ≃1 −
Now, we examine the spectral evolution during the
pulse decay and explore the underlying emission pro-
cesses. Following Daigne & Mochkovitch 
we consider that the dissipated energy in the system is
radiated solely through the synchrotron process. The
peak energy  is influenced by the Lorentz factor
electron Lorentz factor behind the shock . Classical
equipartition assumptions allow to express the magnetic
field and the characteristic electron Lorentz factor as
functions of the comoving density  and the dissipated
energy per unit mass . Specifically, for the standard
synchrotron radiation within internal shock model the
peak energy can be written as
Ep = Esyn ∝Γ B Γ2
e ∝Γρ1/2ϵ5/2 ∝5
γ7/2t
Ep ∝t−7/2. However, empirical observations obtained
from the best fit of the decay phase of GRB 230307A,
as shown in section 2, show that the observed spectral
evolution of pulses deviates from this steep power law
behavior at late times and decays as Ep ∝t−0.77.
To account for the observations, a more generalized
phenomenological expression for the peak energy is in-
troduced, given by Ep ∝Γρxϵy ∝
γ4x+y−1t2x
which at
later time becomes
Ep ∝
t2x+y ,
standard synchrotron values of 1/2 and 5/2, .
The values of x and y are crucial in characterizing the
observed spectral evolution of GRBs. For the case of
GRB 230307A, when x = y = 1/4, the observed peak
energy, Ep ∝1/t0.77 is reproduced.
In the following
subsection, we will investigate the significance of these
values for x and y.
We have so far investigated the behavior of spectral
evolution during pulse decay and presented a more ver-
satile expression for the peak energy, considering differ-
ent exponents to match empirical observations. Now,
we are going to show that this choice of parameters is
also consistent with the observed flux, and photon flux
of GRB 230307A, when considering the internal shock
model. The synthetic pulse profile depicted in Fig. 5 is
generated by decelerating a wind with fast shells with
Lorentz factor Γ1 = 400 and total power
erg.s−1 decelerated by slow shells with Lorentz factor
Γ0 = 100 as illustrated in section 3. The adopted values
are t0 = 0.6 s, x = y = 1/4. The pulse duration aligns
with the expectation of being approximately 27 s.
The post-maximum evolution of the flux at 6-8000
keV bandwidth exhibits a decay behavior of 1/t1.5, as
depicted in Fig. 5.
This decay pattern can be com-
prehended by employing Eq. 15, when combined with
the Lorentz gamma profile of Eq. 16. The behavior of
N ∝1/t0.44 within internal shock model is calculated
using Eq. 1 in conjunction with Eq. 15, which aligns
with the observation, N ∝1/t0.43, as shown in Fig. 5.
From our analysis, we have determined that the dis-
crepancies observed in the spectral evolution of GRB
shock model proposed in this study may stem from
oversimplified assumptions regarding the microphysics
within the emitting shocked regions. Additionally, these
discrepancies could be attributed to the nature of the
magnetic field, which may naturally decay with radius
or be influenced by the presence of an external magnetic
field Uhm & Zhang ; Zhou et al. .
internal-collision-induced magnetic reconnection
and turbulence  model
The current understanding of relativistic shocks does
not offer a detailed description. However, incorporat-
ing fluctuations in the fraction of accelerated electrons
based on shock intensity allows for achieving more qual-
itative and quantitative agreement between model pre-
dictions and observed spectral evolution for many GRBs
The issues raised above may be more complex than
initially thought.
Through the use of time-resolved
spectral analysis of GECAM data, Yi et al.  have
demonstrated that the broad pulse of GRB 230307A
consists of numerous rapidly variable short pulses,
rather than being caused by short pulses overlaid on top
of a slow component. This poses a significant challenge
to the internal shock models, which attribute all vari-
ability components to collisions among different shells.
Instead, they propose the ICMART model , wherein the prompt radiation is generated
by many mini-jets resulting from local magnetic recon-
nection events in a large emission zone far from the GRB
central engine. Most notably, GRB 230307A shows the
presence of a dip at t ∼18 s in all energy bands observed
by GECAM and Fermi/GBM. During the decay phase,
significant dip with its location consistent with such a
decay phase, is allowed in the ICMART model. Because
the fast pulses originate from local mini-jet events, the
broad-band emission of the fast component is related to
the dynamics of the mini-jets and is therefore aligned in
time.
The presence of a dip and numerous rapidly variable
short pulses in the prompt emission of GRB 230307A
challenges the traditional approach to understanding the
pulse behavior of these new class of GRBs and calls
for a revision. It also emphasizes the need to consider
models other than the internal shock model to explain
such peculiar behavior. Further observations of similar
GRBs with high-resolution detectors, such as GECAM,
are necessary to draw a general conclusion.
In this paper, we presented the pulse profile of
GRB 230307A and conducted a detailed analysis of
its temporal behavior using a combined GECAM–
Fermi/GBM time-resolved spectral analysis. The inclu-
sion of GECAM data is particularly advantageous as it
effectively captured crucial information during the pile-
up period of the Fermi/GBM , showcasing its unique ability to gather
data from highly luminous GRBs.
This capability is
especially valuable in enhancing our understanding of
highly luminous GRBs, such as GRB 230307A and GRB
The analysis performed in this study is essential in val-
idating the existence of the observed hardness-intensity
correlation  and hardness-flux correlation .
These correlations are pivotal observational relations in
GRBs, as they can aid in distinguishing among different
theoretical models.
The proposed multiple shells IS scenario, as outlined
by Kobayashi et al. , displays a light curve trend
that mirrors the historical activity of the central engine.
This trend does not exhibit a dependence on shape-
energy or adhere to a power-law behavior in luminos-
ity.
This stochastic behavior is expected, considering
the random generation of the shell cluster, which pos-
sesses varied mass and velocity parameters. Achieving
precise alignment of all elements in this model is chal-
lenging due to its inherent randomness, necessitating a
highly specialized design to ensure accurate representa-
tion of the system.
We have observed that the decay phase of GRB
emission model,
in particular,
within the context
of the large-radius internal shock model , which posits continuous rel-
ativistic ejections causing shock waves to propagate
within the outflow.
However, the parameter values
x = 1/4 and y = 1/4 are lower than the expected val-
ues x = 1/2 and y = 5/2 for standard synchrotron
radiation.
This deviation suggests in the internal shock frame-
work a potential scenario where the correlation be-
tween the magnetic field, outflow Lorentz factor, elec-
tron Lorentz factor, and the dissipated energy is less pro-
nounced. As highlighted by Boˇsnjak & Daigne ,
uncertainties persist in understanding the microphysics
involved in shocked material, which could be attributed
to oversimplified assumptions about the microphysics
within the emitting shocked regions .
On the other hand, this choice of parameters
is consistent with a more general model proposed by
Uhm & Zhang , which involves a Poynting-flux-
dominated jet dissipating magnetic energy at a distance
from the engine.
In this model, the magnetic field
strength decreases with radius as the emission region
expands, and rapid bulk acceleration occurs during the
production of prompt γ-rays. Alternatively, it indicates
the possible need to incorporate additional elements,
such as a combination of a decaying shock-generated
magnetic field and a non-decaying background magnetic
field, to provide a better fit to the observations compared
to the standard internal shocks model .
On the other hand, Yi et al.  demonstrated that
the broad pulse of GRB 230307A is actually composed of
numerous rapidly variable short pulses, rather than be-
ing the result of the superposition of many short pulses
on top of a slow component. This finding aligns with the
concept of many mini-jets arising from local magnetic
reconnection events in a large emission zone far from
the GRB central engine, as proposed in the ICMART
model. This challenges the internal shock models, which
attribute all variability components to collisions among
different shells. Consequently, it is not surprising that
the internal shock model for this GRB requires ad hoc
assumptions in order to provide an acceptable pulse ex-
planation.
An area requiring further investigation in the internal
shock model is the reliability of the initial Lorentz fac-
tor profile. For example, it is becoming increasingly ac-
cepted that the kinetic energy of ejecta in GRBs is pow-
ered by the rotational energy of the central engine. How-
ever, the utilization of rotational energy-powered ejecta
results in a reduction of the overall rotational energy of
the central engine, consequently leading to a smaller ini-
tial kinetic energy for the remaining ejecta. As a conse-
quence, this affects the temporal decrease of the Lorentz
gamma factor of the ejecta , raising concerns
about the validity of the initial profile employed in the
internal shock model, which highly relies on the decel-
eration of rapidly moving materials by slower materials
within a relativistic wind.","[('model', 78), ('grb', 47), ('gecam', 42), ('energy', 40), ('shock', 37), ('shell', 35), ('emission', 35), ('ep', 35), ('time', 33), ('pulse', 32)]","[(('internal', 'shock'), 25), (('shock', 'model'), 20), (('lorentz', 'factor'), 19), (('decay', 'phase'), 13), (('gamma', 'ray'), 12), (('fermi', 'gbm'), 12), (('peak', 'energy'), 11), (('power', 'law'), 11), (('post', 'maximum'), 10), (('photon', 'flux'), 9)]","[(('internal', 'shock', 'model'), 15), (('gecam', 'fermi', 'gbm'), 8), (('gamma', 'ray', 'burst'), 7), (('hardness', 'intensity', 'correlation'), 5), (('gecam', 'gecam', 'gecam'), 5), (('tp', 'td', 'td'), 5), (('time', 'resolved', 'spectral'), 4), (('high', 'latitude', 'emission'), 4), (('large', 'emission', 'radius'), 4), (('decay', 'phase', 'grb'), 4)]"
2410.17232v1.txt,"MNRAS 000, 1–13 
Preprint 23 October 2024
Compiled using MNRAS LATEX style file v3.0
Quantifying the Impact of the Si/O Interface in CCSN Explosions Using
the Force Explosion Condition
Luca Boccioli,1★
Mariam Gogilashvili,2
Jeremiah Murphy3
and Evan P. O’Connor4
Accepted XXX. Received YYY; in original form ZZZ
The explosion mechanism of a core-collapse supernova is a complex interplay between neutrino heating and cooling , the gravitational potential, and the ram pressure of the infalling material. To analyze the
post-bounce phase of a supernova, one can use the generalized Force Explosion Condition , which succinctly formalizes
the interplay among these four phenomena in an analytical condition, consistent with realistic simulations. In this paper, we
use the FEC+ to study the post-bounce phase of 341 spherically symmetric simulations, where convection is included through
a time-dependent mixing length approach. We find that the accretion of the Si/O interface through the expanding shock can
significantly change the outcome of the supernova by driving the FEC+ above the explosion threshold. We systematically explore
this by  artificially smoothing the pre-supernova density profile, and  artificially varying the mixing length. In both cases,
we find that large-enough density contrasts at the Si/O interface lead to successful shock revival only if the FEC+ is already close
to the explosion threshold. Furthermore, we find that the accretion of the Si/O interface has a substantial effect on the critical
condition for supernova explosions, contributing between 5% and 15%, depending on how pronounced the density contrast at
the interface is. Earlier studies showed that convection affects the critical condition by 25–30%, which demonstrates that the
accretion of the Si/O interface through the shock can play a nearly comparable role in influencing shock dynamics.
Key words: keyword1 – keyword2 – keyword3
The explosion mechanism of a core-collapse supernova  has
been the topic of many decades of research . Theoretical, observational, and computational efforts
have shed light onto this complex phenomenon, and the theoretical
understanding of the explosion of a CCSN has drastically improved in
the last decade, thanks to the rapid improvement of supercomputers
and, therefore, of detailed simulations .
Nonetheless, the detailed mechanism of the CCSN explosion is
still a matter of active research. Understanding the physical phe-
nomena that lead to an explosion has important consequences in a
variety of astrophysical environments. Knowing which stars explode
and which ones fail determines the distribution of compact objects in
the Universe . It also determines the thermody-
namic conditions that lead to the nucleosynthesis of heavy elements,
which in turn determines the chemical enrichment of the interstellar
medium . Finally, understand-
ing the physical phenomena responsible for triggering the explosion
can unveil what are the pre-collapse features that are important for
the explosion or the failed shock revival .
The first breakthrough in studying the CCSN explosion mecha-
nism was made by Colgate & White , who suggested that
neutrinos were responsible for carrying energy from the deep in-
terior of the central proto-neutron star  to the vicinity of the
shock. Later, Bethe & Wilson  refined this picture by introduc-
ing the so-called ""delayed neutrino mechanism"". In their simulations,
neutrinos take a few hundred milliseconds after the bounce to trans-
fer enough energy to revive the shock. During this period, the shock
stalls at a nearly constant radius. However, with more advanced mod-
els for describing the state of matter at supranuclear densities and
interactions between neutrinos and matter , the explosion could not be obtained in spherical sym-
metry anymore. The reason is the lack of multi-dimensional effects,
and state-of-the-art multidimensional simulations indeed show self-
consistent explosions . In particular, in the last two decades neutrino-driven convec-
tion has been shown to play a crucial role for the explosion dynamics
the development of parametric models to include neutrino driven
convection and turbulent dissipation effects in spherically symmet-
ric, 1D simulations. These parametric models are known as 1D+
simulations and over the years, they were able to produce successful
explosions .
The advantage of such models is that they are computationally very
affordable, and therefore allow for systematic studies of the explosion
mechanism itself. One of the first attempts at deriving an explosion
condition was the seminal work of Burrows & Goshy , who
derived a critical luminosity condition based on the idea that, given
a mass accretion rate, there is a maximum neutrino luminosity above
which a stalled shock solution is not realizable, and therefore an
explosion can develop. Similar studies were later carried out over the
years , oftentimes guided
by the results of multi-dimensional simulations.
Recently, a few studies 
have pointed out that the magnitude of the density drop located near
the Si/O interface can be used to predict the outcome of the explo-
sion. Even before these studies, the idea that the accretion of the Si/O
interface onto the shock could facilitate the explosion was well es-
tablished . Because of the sudden change in composition,
there is usually a large density drop at the Si/O interface that, when
accreted through the shock, leads to a sudden drop in ram pressure,
facilitating a rapid shock expansion that, under the right conditions,
can turn into a runaway explosion.
In this paper, we will analyze how the accretion onto the shock
of the density drop, which typically occurs near the Si/O interface,
affects the explosion. In particular, this manuscript is structured as
follows: in Section 2.1 we describe the FEC+, in Section 2.2 we
discuss the accretion of the Si/O interface through the shock, and
in Section 2.3 we describe our numerical setup. Then, in Section 3
we analyze the explosion of 341 1D+ simulations, and in Section
magnitude of the density drop at the Si/O interface change the FEC+
and therefore the explosion. Finally, we discuss the results in Section
Over the years many have suggested explosion criteria for success-
ful CCSN explosions . Over
three decades ago, Burrows & Goshy  pointed out that the
stalled shock phase is a steady-state, boundary-value problem. They
parameterized this problem by the neutrino luminosity, 𝐿𝜈, and mass
accretion rate, ¤𝑀, and they found a critical curve in these parameters,
below which stalled shock solutions exist and above this critical curve
there are no steady-state solutions. They suggested that the solutions
above this critical curve are most likely explosive.
Inspired by the insight of Burrows & Goshy , Murphy &
Dolence  utilized semi-analytic techniques to propose an in-
tegral condition for supernova explosions. This work suggests that
the dimensional integral of the momentum equation, denoted as Ψ,
is helpful for determining the existence of stalled-shock solutions.
The relationship between the shock velocity and the parameter Ψ
indicates that a stalled solution exists if Ψ = 0. This force explosion
condition is consistent with critical neutrino luminosity conditions
and one-dimensional simulations and implies that more factors than
just neutrino luminosity and mass accretion rate influence explosion
dynamics. They show that the integral explosion condition corre-
sponds to a critical hypersurface where the physical dimensions are
𝐿𝜈, 𝑀NS, 𝑅NS, and ¤𝑀.
Gogilashvili & Murphy  derived an analytic model for
the force explosion condition, beginning with fundamental hydro-
dynamic equations and identifying that the explosion condition de-
pends on two dimensionless parameters instead of four dimensional
parameters. The force explosion condition  is expressed as:
power normalized by accretion power, and ˜𝜅= 𝜅¤𝑀/
𝐺𝑀NS𝑅NS is
a dimensionless neutrino opacity. The coefficients 𝑎and 𝑏mostly
depend upon the density profile of the material behind the shock and
can be estimated analytically to be 𝑎∼0.05 and 𝑏∼0.51. It is
crucial to emphasize that the theory provides an analytic functional
form of the FEC. While the coefficients can be estimated analytically,
these estimates might not be accurate due to the approximations and
assumptions of the model and thus might need proper calibration
with CCSN simulations.
The validity of the FEC has been checked through three ap-
proaches. Gogilashvili & Murphy  first, compared the FEC
with the semi-analytic condition of Burrows & Goshy , leading
to a numerical fit for 𝑎= 0.06 and 𝑏= 0.38, which closely matched
their analytical estimates. Second, they demonstrated that the FEC
accurately predicts explosion conditions in one-dimensional light-
bulb simulations. In follow-up research, Gogilashvili et al. 
adapted the FEC for simulations using actual neutrino transport,
such as those conducted with the GR1D code . Their tests confirmed that the FEC reliably pre-
dicts explosion conditions in spherically symmetric supernova sim-
ulations, highlighting its robustness as a diagnostic tool.
Motivated by the accuracy of the spherical force explosion condi-
tion, Gogilashvili et al.  generalized the FEC to incorporate
multi-dimensional effects such as neutrino-driven convection and
turbulent dissipation in a simple model. This generalized condition,
termed FEC+, is expressed as
𝑊𝑏𝑅NS/, representing the dimensionless buoyant driv-
ing, and ˜𝑅𝑟𝑟= 𝑅𝑟𝑟𝑅NS/, representing the dimensionless
Reynolds stress. These additional terms 
reduce the net neutrino heating required for explosion by 26%, a re-
duction consistent with published studies . This finding high-
lights the significance of multi-dimensional effects in supernova ex-
plosion dynamics. Consequently, the FEC+ offers a more compre-
hensive and accurate framework for describing the explodability of
multi-dimensional simulations, enhancing our understanding of the
underlying physical processes.
As mentioned above, in the simple spherically symmetric case the
coefficients in Eq.  can be fitted using a simple steady-state model,
as derived by . However, for the more
complicated model that includes convection ), a more accurate calibration is needed, and the same holds for
applying the FEC+ to multi-dimensional simulations. For the present
paper, we therefore explicitly calculate the coefficients 𝑎and 𝑐in Eq.
MNRAS 000, 1–13 
Impact of Si/O interface on CCSN explosions
et al. . As a consequence, there is as yet no a priori derivation
of the threshold value, i.e. the coefficient 𝑏. Instead, we empirically
determine the threshold 𝑏by analyzing hundreds of 1D+ simulations
in the next section. In practice, the threshold is found to be around
The presence of density discontinuities in the pre-SN progenitor can
play a significant role in the explosion . In particular, the den-
sity discontinuity located near the Si/O interface is responsible for
triggering the explosion in the vast majority of progenitors. However,
as we will quantitatively show in the remainder of this paper, the ac-
cretion of this interface onto the shock triggers the explosion only if
the star is already close to exploding . This requires multi-dimensional effects and
is therefore only possible in 2D, 3D, or 1D+ simulations. Specifi-
cally, Wang et al.  and Boccioli et al.  found that one
can build a criterion based on the density drop at the Si/O interface
that can successfully predict the explodability of > 90% progenitors.
In particular, both studies independently showed that when the den-
sity drop is such that 𝛿𝜌2
Si/O/𝜌2
Si/O > 0.08 
sion will ensue. However, for high compactness progenitors, which
are more common at low metallicities, Boccioli & Fragione 
showed that, even in the absence of this density drop, a successful
shock revival would still occur, hinting that the explosion mechanism
for these high-compactness stars is slightly different.
The Si/O interface usually occurs in correspondence with a pocket
of oxygen inside the silicon-sulfur shell, which is why it should more
precisely be referred to as a Si/O interface. For the sake of brevity,
we will however refer to it simply as a Si/O interface in the remainder
of this paper. Usually, the shock reaches this interface a few hundred
milliseconds  after bounce, i.e. during the
stalled shock phase. The sudden drop in density causes the ram
pressure of the infalling material to suddenly decrease, and the shock
is therefore able to expand to larger radii. Under favorable conditions,
this expansion turns into a runaway explosion, causing a successful
shock revival. This has been shown by several multi-dimensional
simulations in the past few years , where a successful shock
revival occurs right after the shock reaches the Si/O interface. Notice
that, especially for 2D simulations, there are instances where the Si/O
interface is accreted at times between 200−400 ms after bounce and
the explosion instead is triggered much later, at times between 700 ms
and 1 s . Moreover,
both in 2D and 3D simulations, different nuclear equations of state
can lead to earlier or later explosions, depending on the details of the
microphysics .
This suggests that, after the interface is accreted onto the shock, if
the change in ram pressure is large enough to disrupt the quasi-steady
state, then an explosion will occur. Otherwise, the perturbation might
be large enough to move the standing accretion shock to larger radii,
but not large enough to completely disrupt the quasi-steady state. In
that case, the shock will stall at larger radii and, eventually, it will
slowly recede and the explosion will fail . In the remainder of this paper, we will quan-
titatively test this hypothesis using the generalized force explosion
condition  described in Section 2.1.
To analyze how the accretion of the Si/O interface affects the ex-
plosion, we employed a series of simulations performed with the
open-source code GR1D ,
modified as described in Boccioli et al.  to include the effect
of neutrino driven convection via the Reynolds decomposition model
based on time-dependent mixing-length theory STIR . The equation of state for all simulations is the SFHo equa-
tion of state for nuclear matter , and neutrino opacities from Bruenn , with
weak-magnetism and recoil corrections from Horowitz , and
the virial correction for neutrino-nucleon scattering from Horowitz
et al. . Neutrino-electron scattering opacities follow the in-
elastic treatment of . The neutrino transport solves the
two-moment equations and uses the analytic M1 closure scheme to
relate the radiation pressure to the radiation energy density and mo-
mentum . The neutrino spectrum is resolved with
The spatial resolution was the same for all of the simulated pro-
genitors . We adopted a grid of 700 zones
linearly spaced up to 20 km, with a resolution of 300 m, and then
logarithmically spaced out to 15,000 km. The decision of using the
same spatial grid for all progenitors was motivated by the fact that
the extra heating from STIR has a weak dependence on the spatial
resolution, as shown in Boccioli et al. , particularly behind the
shock.
The STIR model  is based on a Reynolds decom-
position of the Euler equations, and it relies on a mixing-length-like
closure that relates higher-order turbulent correlations to lower-order
turbulent correlations. The STIR model is a local algebraic model
for which the equations are closed using algebraic relationships at a
local level. Other models explored different closures .
The main modification that STIR introduces is to the internal
energy equation, although extra diffusive terms must also be added
to the equations describing the evolution of the electron fraction
and the neutrino energy. STIR evolves the internal energy and the
turbulent energy separately, and therefore the evolution equation of
the total energy is the combination of those two ):
BVΛmix.
turb, and 𝑃turb = 𝜌𝑣2
turb. In the above equations,
𝑒includes the contributions from both internal and kinetic energy.
Notice that GR1D solves the general relativistic version of the above
equation, but we show the Newtonian version for simplicity. The
quantities 𝜔BV and Λmix are the Brunt-Väisälä frequency and the
MNRAS 000, 1–13 
L. Boccioli et al.
mixing length, respectively, defined as:
BV =
𝑔−𝑣𝜕𝑣
𝜌𝑔,
corrections, for simplicity. However, it should be highlighted that
one should use the total energy 𝜌 in the expression of the
Brunt-Väisälä frequency, as shown in Eq. , rather than simply 𝜌.
This can change the magnitude of the Brunt-Väisälä by more than
the main parameter of the model: 𝛼MLT. A larger value of 𝛼MLT
increases the magnitude of the mixing length, which as can be seen
from Eq.  corresponds to introducing a larger source of turbulent
energy, and therefore stronger convection and subsequent explosion.
The extra terms due to STIR are the last two terms on the RHS of
eq. 3, and the last two terms on the LHS. The terms on the LHS, i.e.
the diffusive terms, have a smaller  effect
on the overall dynamics compared to the terms on the RHS . If one applies the FEC+ to this model , then one finds that ˜𝑊𝑏in eq.  corresponds to the
last term on the RHS of eq. , whereas ⟨˜𝑅𝑟𝑟⟩corresponds to the
advection of 𝑃turb in eq. . Notice that the shear term ), is quite small, and therefore ignored in
the expression of the FEC+.
One of the primary aims of this manuscript is to use FEC+ to un-
derstand why the Si/O interface facilitates an explosion for most
ger explosions for other progenitors. The simulations for this study
are described in Boccioli & Fragione . In particular, Figure 1
shows the evolution of the shock radius and the FEC+ for 341 GR1D+
simulations of KEPLER progenitors from Sukhbold et al.  and
Woosley et al. , with masses ranging from 9 𝑀⊙to 120 M⊙
and metallicities of 𝑧= 0, 𝑧= 10−4𝑧⊙, and 𝑧= 𝑧⊙, where 𝑧⊙indi-
cates solar metallicity. The solid-green curves show simulations that
eventually explode, and the black curves shows simulations that fail
to explode. The most striking feature in the FEC+ plot 
is that there is a clear separation between failed and successful ex-
plosions for a value of FEC+ around 0.28–0.3. This is yet another
example showing that the FEC+ can be used as an explosion diag-
nostic. In this manuscript, we will use the FEC+ to diagnose why the
Si/O interface helps facilitate explosions sometimes but not always.
For the present analysis, we analyze the post-bounce phase of the
aforementioned 341 simulations. The role of the Si/O interface can
be divided into three categories: 1) the accretion of the Si/O interface
through the shock immediately triggers a vigorous explosion, 2) the
accretion of the interface triggers outward movement of the shock,
the shock stalls again, retreats a little, and then explodes, 3) the
accretion of the interface triggers outward progression of the shock,
which subsequently stalls again and does not explode.
Generally, what determines which category a specific progenitor
belongs to is how large the density drop at the Si/O interface is, and
how late the interface is accreted. We consider a ""large"" interface
one characterized by 𝛿𝜌2
Si/O/𝜌2
Si/O > 0.08, consistent with the ex-
plodability criterion derived independently by Boccioli et al. 
and Wang et al. , as summarized in Section 2.2. Moreover,
in some high compactness  cases, explosions occur regardless
of the accretion of the density drop. The compactness is defined as
𝜉M =
𝑀/𝑀⊙
𝑅/1000 km,
Therefore, one can identify 6 different scenarios, shown by select
simulations in Figure 2, that describe the post-bounce evolution of
both exploding and non-exploding models:
plosion during the stalled phase independently of the presence of a
large density drop in the pre-SN profile. This constitutes ∼26% of
all the analyzed simulations, including the 40 M⊙progenitor shown
in Figure 2.
accretion of a large density drop during the stalled-shock phase.
This constitutes ∼45% of all the analyzed simulations, including the
after the accretion of a large density drop before the stalled-shock
phase. This constitutes ∼2 −3% of all the analyzed simulations,
including the 19.5 M⊙progenitor shown in Figure 2.
a large  density drop before the stalled-shock
phase. This constitutes ∼4 −5% of all the analyzed simulations,
including the 15.5 M⊙progenitor shown in Figure 2. Notice that
these progenitors are very similar to the ones from the previous
category but have smaller density drops .
small density drop during the stalled-shock phase. This constitutes
progenitor shown in Figure 2.
either a small or large density drop after the stalled-shock phase.
This constitutes ∼15% of all the analyzed simulations, including the
The remaining ∼3 −4% of the analyzed simulations are outliers.
For example, four progenitors with large compactness do not explode,
whereas 10 progenitors with large Si/O interfaces accreted towards
the beginning or the end of the stalled-shock phase do not explode.
It is worth noting here that while these outliers do not conform
to the Si/O interface condition for explosion, all simulations are
consistent with the FEC+ explosion condition, i.e. the FEC+ never
crosses the threshold. Perhaps the most interesting outlier is the
drop very late  but instead leads
to a successful explosion. We include this progenitor in Figure 2 to
highlight the unusual time evolution of the FEC+ and shock radius.
Two more progenitors, i.e. the 23.4 and 23.2 𝑀⊙with 𝑧= 10−4𝑧⊙
have a similar pre-SN structure and post-bounce evolution. However,
these progenitors have high compactness , and the Si/O
interface is accreted a bit earlier. Therefore their shock radius and
FEC+ time evolution are not quite as unusual as they are for the
MNRAS 000, 1–13 
Impact of Si/O interface on CCSN explosions
Time post bounce 
Shock Radius 
Time post bounce 
FEC+
progenitors for which the explosion has not been launched. There is a clear bifurcation in FEC+, and empirically it can be seen that progenitors who reach a
threshold around ∼0.28–0.3 explode. Those that do not reach this threshold do not explode.
Time post bounce 
Shock Radius 
Time post bounce 
FEC+
Enclosed Mass 
Each progenitor is an example of a different explosive and non-explosive scenario, as discussed in the text . Moreover, we also show the 24.2 𝑀⊙
which does not fall in any of those scenarios, and it also has a very peculiar time evolution of the FEC+, which quickly dips similarly to what happens in failed
SNe, before suddenly increasing after the accretion of the Si/O interface, which however does not seem to affect the FEC+. In the left and middle panels, circles
and diamonds mark the times before and after the accretion of the Si/O interface, respectively. See Section 4.1 for a description of how these times are defined.
The right panel shows the pre-SN density profiles for all of the progenitors, zoomed in near the Si/O interface. Circles mark the inner edge of the Si/O interface
compactness, have steeper density profiles.
threshold when the explosion sets in, whereas it clearly is for the 24.2
𝑀⊙progenitor.
Since the FEC+ represents a clear threshold for explosion, and
we know exactly which components contribute to its evolution, it is
also a useful diagnostic to understand why the accretion of the Si/O
interface initiates explosion when it does. In the two leftmost panels
of Figure 2, we indicated the time before and after the accretion of
the Si/O interface  with circles
and diamonds. We define 𝑡start
accr as the time when the inner side of
the Si/O interface is accreted through the shock. A more detailed
description of how we define 𝑡end
accr is given later in Section 4.1. In
the following, we analyze the FEC+ evolution for each of the above
scenarios, showcased in Figure 2.
The 40 M⊙progenitor accretes the Si/O interface significantly
after the explosion has started. Therefore, the accretion of the Si/O
interface does not play a role whatsoever in the explosion of this
progenitor. This is confirmed by the smooth evolution of the FEC+,
without any rapid increase caused by the accretion of the Si/O inter-
face. What distinguishes this progenitor is its large pre-SN compact-
ness 𝜉2.0 = 0.75 that causes the neutrino heating to be very large,
which drives the FEC+ to go above the threshold and trigger the
explosion. A more thorough analysis of the role of compactness in
triggering the explosion is currently underway and beyond the scope
of this paper.
The 20 M⊙progenitor has a large density drop at the Si/O interface,
and therefore the shock revival is more pronounced than, for example,
MNRAS 000, 1–13 
L. Boccioli et al.
in the case of the 12.5 M⊙progenitor. This translates into a large
increase of the FEC+ that is pushed above the threshold, indicating
that an explosion occurs.
The 12.5 M⊙has a small density drop which is reflected in a small
increase in the FEC+. However, this increase is not large enough
to push the FEC+ above the threshold, and therefore the explosion
eventually fails.
The 18.2 M⊙has a large density drop, which is however accreted at
ended. As can be seen from the left panel of Figure 2 the shock radius
quickly recedes at very small radii after ∼250-300 ms. Therefore,
even though a large interface is accreted at late times, the shock
cannot be revived anymore. In multi-dimensional simulations, where
not only neutrino-driven convection but also other instabilities and
asymmetries can develop, the stalled-shock phase might be longer
and/or more asymmetric, which could change the outcome of the
simulation.
The 19.5 M⊙progenitor shows a unique behavior. A very large
density drop is accreted early in the evolution. However, since the
FEC+ is still small when the accretion occurs, it does not increase
enough to go above the threshold. Therefore, the explosion is not
triggered after the accretion of the interface, and in principle, one
would expect the explosion to fail. However, since this early accretion
causes the shock to stall at larger radii than any of the other progen-
itors, the FEC+ at around 150 −200 ms is the largest among the
progenitors shown. Therefore, between 200 and 300 ms, the FEC+
is barely below the threshold. Eventually, at around 600-700 ms, the
FEC+ finally goes above the threshold causing a  late ex-
plosion. One can speculate about what the cause of the explosion
is. For most progenitors, it is the accretion of the Si/O interface that
directly launches the explosion by reducing the ram pressure on the
shock. For this specific progenitor ,
the explosion happens much later than the accretion.
Nonetheless, the early accretion of the Si/O interface still plays a
crucial role in causing the explosion. This can be seen by comparing
the shock radius and FEC+ evolution for the 19.5 M⊙and the 15.5 M⊙
progenitors. These have almost identical pre-SN density  structures, as seen in the third panel of Figure 2, with
the important difference that the 15.5 M⊙progenitor has a smaller
density drop at the Si/O interface, and also a Si shell that is 0.125 M⊙
thicker, causing the interface to be accreted at a slightly later time.
The smaller density drop causes the shock radius and the FEC+ to
have a smaller increase, and therefore the FEC+ during the stalled-
shock phase  is smaller for the 15.5
M⊙progenitor. This is the reason why the 15.5 M⊙progenitor fails
whereas the 19.5 M⊙progenitor explodes at a much later time.
Another interesting feature of these two progenitors is that, since
they have a very similar post-bounce evolution, the shock trajectory
of the 15.5 M⊙progenitor would look pretty much exactly like the
one for the 19.5 M⊙progenitor if one were to use a larger value
of 𝛼MLT. This shows that most CCSNe are in general very close to
explosion, and small changes in the physics can lead to qualitatively
drastically different outcomes. In the next section, we will investigate
how changes in the strength of 𝜈-driven convection  and in the density drop at the Si/O interface can change the
shock and the evolution of the FEC+, leading to a successful or
failed explosion. It is important to remark that also changes in the
nuclear equation of state, neutrino opacities, asymmetries in the pre-
SN progenitor, beyond standard model physics, collective neutrino
oscillations, and more in general any physical input of the simulation,
can lead to similar differences in the outcome of the supernova.
Since numerical simulations are involved, not only changes to the
physics but also changes to the numerical setup and algorithms can
lead to different outcomes. For example, as mentioned in the previous
section, STIR has a weak dependence on resolution. Therefore, not
only changes to 𝛼MLT, but also changes to the numerical resolution
can lead to an explosion for the higher resolution simulation and
a failed supernova for the lower resolution simulation. This can be
generalized to the case of multi-dimensional simulations, where nu-
merical resolution is well-known to create different rates of turbulent
dissipation , which can
therefore affect the explodability.
To summarize, we qualitatively observed three types of explosions:
pansion and drives the FEC+ above the threshold, hence causing the
explosion;  the ones triggered indirectly by the accretion of the
Si/O interface , where the accretion of
this layer causes a significant shock expansion and pushes the FEC+
very close to the threshold until it eventually goes above and triggers
the explosion;  the ones triggered by a rapid increase of the FEC+,
which occurs in high-compactness progenitors such as the 40 M⊙in
Figure 2, not caused by accretions of any significant density drops,
but simply a very strong neutrino heating.
Most importantly, in all cases analyzed the explosion occurs after
the FEC+ goes above ∼0.28–0.3, despite the qualitative difference
in how the FEC+ crosses this threshold, which shows the robustness
and flexibility of the FEC+ as a diagnostic tool to determine the onset
of explosion .
CONVECTION AND ACCRETION OF THE SI/O
As shown in the previous section, the accretion of the Si/O inter-
face can often lead to the explosion of a CCSN. In this section, we
use the FEC+ to quantify in detail how the post-accretion expansion
of the shock depends on both the strength of 𝜈-driven convection
and the density drop at the Si/O interface. In addition to being a
condition for explosion, the FEC+ also quantifies a distance from
explosion. Therefore, it is useful as a quantitative measure of how
various physics affect the explosion outcome. It should be highlighted
that, in a more realistic 3D simulation, other multi-dimensional ef-
fects are also at play, and therefore the post-bounce phase is more
complicated. However, the simplicity of 1D+ simulations enables
systematic study of the interplay between the accretion of the Si/O
interface and parametrized 𝜈-driven convection.
To analyze the interplay between  neutrino-driven
convection and the accretion of the Si/O interface, one can analyze
how the FEC+ changes as a function of 𝛼MLT. Therefore, we simu-
lated the collapse and subsequent explosion of a 20 𝑀⊙from Farmer
et al.  for different values of 𝛼MLT. This progenitor is partic-
ularly interesting as it is characterized by a large density drop at the
Si/O interface. The shock evolution and the FEC+ as a function of
time after bounce are shown in Figure 3. As in Figure 1, one can see
that there is a threshold for the FEC+ around 0.28–0.3 that separates
explosions from failed SNe. As expected, simulations with small val-
ues of 𝛼MLT are further away from the threshold, and fail to explode.
MNRAS 000, 1–13 
Impact of Si/O interface on CCSN explosions
Time post bounce 
Shock Radius 
Time post bounce 
FEC+
Figure 3. The top  panel shows the evolution of the shock radius
was simulated using GR1D+ for different values of 𝛼MLT, shown in the colorbar
on the right. Before calculating the value of the FEC+, each quantity in eq. 
has been smoothed with a Savitzky-Golay filter in a 10 ms window. The filled
dots show the time before the accretion of the Si/O interface, whereas the
filled diamonds show the time after the accretion of the Si/O interface. All of
the simulations with 𝛼MLT ≥1.495 successfully explode, and the accretion
of the Si/O interface leads to a larger increase of the FEC+ for larger values of
𝛼MLT. To avoid cluttering, we only show exploding simulations with 𝛼MLT
spaced by 0.01, but a finer grid is shown in Figures 4 and 5.
Simulations with large values of 𝛼MLT cross the critical threshold
and lead to a successful explosion.
For the accretion of the Si/O interface to be sufficient to launch the
explosion, the FEC+ has to be already close to the threshold. This
is shown in the top panel of Figure 4, where the x-axis is the value
of the FEC+ at 𝑡= 𝑡start
accr  and the y-axis
is the value of the FEC+ at 𝑡= 𝑡end
accr .
Simulations with a value of FEC+ at 𝑡= 𝑡start
accr below ∼0.24 do not
explode, because once they accrete the interface the change in FEC+
is not enough to push it above threshold.
As can be seen in the bottom panel of Figure 3, the FEC+ peaks
right after the accretion of the Si/O interface, before settling down
to a more stable value. The reason behind this is that, as seen in
the top panel, the accretion of the Si/O interface causes a sudden,
extremely rapid expansion of the shock. In this transient phase, steady
state is briefly disrupted and since the FEC+ is derived assuming a
stalled shock solution, it cannot properly describe such rapid shock
expansion, hence explaining the sudden peak in the FEC+.
To avoid the unexplained peak of the FEC+, we define the 𝑡end
accr to
be the time for which the transient phase has passed 1, corresponding
to the diamonds in Figures 3.
All of the exploding simulations in Figure 3 are characterized by
a value of the FEC+ at 𝑡= 𝑡end
accr above 0.28–0.3.
The bottom panel of Figure 4 shows that the values of the FEC+
at 𝑡= 𝑡start
accr and 𝑡= 𝑡end
accr are related to the shock radii before and after
the accretion, as one would expect. Moreover, both panels show a
linear correlation between FEC+ and shock radius before and after
accretion, and both quantities also positively correlate with 𝛼MLT.
The correlations for the FEC+ show a bit more scatter, although
given the noise present in the FEC+ as a function of time  it is not surprising.
A similar correlation can be seen between ΔFEC+ 
and the value of FEC+  at 𝑡start
accr , as
shown in Figure 5. The quantities ΔFEC+ and Δ𝑅shock are defined
as the differences between the values of FEC+ and shock radius
after and before the accretion of the Si/O interface. Larger 𝛼MLT, i.e.
stronger 𝜈-driven convection, leads to the shock stalling at a larger
radius, causing the FEC+ to be larger, and eventually the accretion
of the Si/O interface causes a proportionally larger increase in FEC+
and shock radius.
Quantitatively, we observe that during the accretion, the Si/O in-
terface increases the FEC+ by about 0.03–0.04  for 𝛼MLT around the best-value of 1.51. This increase is about 10%
of the FEC+ threshold, and it is about one-third of the overall effect
of convection, which aids the explosion condition by about 25–30%
To summarize, for a given density profile with a given Si/O in-
terface, increasing 𝛼MLT will cause the shock to stall at larger radii,
increasing the overall FEC+. When the Si/O interface is accreted,
the shock expansion and the increase in FEC+ will also be larger. In
particular, this increase is directly proportional to both 𝛼MLT and the
radius of the shock  at 𝑡start
accr.
For a successful explosion to ensue, the FEC+ must be already
close enough to the threshold  when
the Si/O interface is accreted.
Another conclusion that can be drawn from the present analysis
is that accurately simulating the effects of the convection is crucial
since it contributes to determining the time evolution of the shock.
This can significantly change the outcomes of the explosion because,
as we have shown, the shock position before the accretion of the Si/O
interface is strictly related to the effect of the Si/O interface accretion
on the explosion condition.
In some 2D simulations, it has been shown that neutrino-driven
convection is not properly captured, due to the inverse turbulent
cascade caused by the imposed axisymmetry .
However, other studies have instead shown through detailed analysis
of neutrino-driven convection that the turbulent dissipation in 2D
simulations is compatible with the one found in 3D , and therefore the outcomes of the sim-
ulations should be the same. Moreover, 3D simulations can suffer
from finite-resolution effects, and low resolution might artificially
favor the explosion .
Given the uncertainties and discrepancies among different works,
more systematic studies and careful investigation of neutrino-driven
accr as the time 𝑡> 𝑡start
accr at which d2FEC+
the second negative peak, which means a negative change in slope of the
FEC+, indicating the end of the transient period, when the FEC+ reaches a
occurs in correspondence with the peak in the transient phase of the FEC+
MNRAS 000, 1–13 
L. Boccioli et al.
FEC+
FEC+
Failed Explosions
Failed Explosions
Figure 4. The top panel shows the values of the FEC+ at 𝑡= 𝑡start
accr  and
at 𝑡= 𝑡end
accr  the accretion of the Si/O interface through the shock. The
times before and after accretion are shown as circles and diamonds in Figure
defined, see the text. Only simulations with 𝛼MLT ≥1.48 are shown, since in
other cases the FEC+ and shock radius do not show any significant ""jumps"".
Simulations below the horizontal bar yield failed explosions, and indeed only
when the FEC+ at 𝑡= 𝑡end
accr has crossed the threshold an explosion ensues.
The bottom panel is the same as the top one but for shock radius instead of
FEC+.
convection effects are necessary to improve future multi-dimensional
simulations.
Another way to observe the interplay between 
neutrino-driven convection and the accretion of the Si/O interface
is to study how the explosion and the FEC+ change when only the
Si/O interface changes, with everything else being the same. There-
fore, we artificially smoothed the density drop at the Si/O interface of
two different progenitors. The first two panels of Figures 6 and 7 show
the smoothing of a 20𝑀⊙MESA progenitor  and
a 21𝑀⊙KEPLER progenitor , respectively.
The ""degree of smoothing"" shown in the colorbar of those figures is
represented by the quantity Δ𝑅/Δ𝑅orig, where Δ𝑅is the radial extent
of the Si/O interface, and Δ𝑅orig refers to the original progenitor
without any smoothing. Therefore, a more pronounced smoothing
corresponds to a larger Δ𝑅where the density profile falls off less
steeply.
After smoothing the density profile, we calculated the new pressure
by imposing hydrostatic equilibrium, whereas we did not change the
electron fraction 𝑌e, which only changes by ≲1% anyway. Finally,
FEC+
Figure 5. The top panel shows on the y-axis the increase in FEC+ due to
the accretion of the Si/O interface. This quantity, i.e. ΔFEC+, is simply the
difference between the value of the FEC+ at 𝑡= 𝑡end
accr and at 𝑡start
accr . The x-axis
is the FEC+ at 𝑡= 𝑡start
accr , the same as for Figure 4. The times before and after
accretion are shown as circles and diamonds in Figure 3. For a description of
how exactly the times before and after accretion are defined, see the text. Only
simulations with 𝛼MLT ≥1.48 are shown, since in other cases the FEC+ and
shock radius do not show any significant ""jumps"". The increase in FEC+ is
proportional to 𝛼MLT and to the value of the FEC+ at 𝑡start
accr , and for exploding
progenitors with 𝛼MLT around 1.51 it is between 0.03 and 0.04, which is
roughly 10% of the FEC+ threshold. The bottom panel is the same as the top
one but for shock radius instead of FEC+.
with the new density, pressure, and electron fraction, we calculated
the rest of the thermodynamic quantities by inverting the equation of
state. We did not change the collapse velocities. Notice that, with this
procedure, we are effectively adding mass outside the Si/O interface,
since the density of the smoothed profiles is larger. However, this
corresponds to a maximum of 0.02 𝑀⊙and 0.025 𝑀⊙for the 20𝑀⊙
MESA progenitor and the 21𝑀⊙KEPLER progenitor, respectively.
The artificially added mass is therefore small enough that it will not
perturb the evolution in any way other than the change in the density
jump at the Si/O interface.
One caveat that should be mentioned is that in principle this is not
self-consistent since the Si/O interface corresponds to a change in
composition. Therefore, one should actively change the composition
at the Si/O interface to obtain a fully consistent thermodynamic
profile, which would require a self-consistent simulation. However,
this is quite challenging and would potentially modify the entire
thermodynamic profile, and not only the Si/O interface. Therefore,
we adopted this simpler approach of directly modifying the density
profile, which is the most important quantity responsible for changing
MNRAS 000, 1–13 
Impact of Si/O interface on CCSN explosions
Time post bounce 
Time post bounce 
Shock Radius 
Time post bounce 
FEC+
Figure 6. This Figure refers to simulations of a 20 𝑀⊙MESA progenitor  whose density at the Si/O interface has been smoothed according
to the procedure outlined in the text. Larger values of Δ𝑅/Δ𝑅orig correspond to a higher degree of smoothing. The first panel shows the pre-collapse density
profile in the vicinity of the Si/O interface. The second panel shows the average mass accretion rate in the gain region as a function of time after bounce. The
third panel shows the evolution of the shock radius, and the last panel shows the FEC+. The shaded region is drawn between 𝑡start
accr and 𝑡end
accr, i.e. the time when
the inner side of the Si/O interface is accreted and the time after the accretion and transient phase of the FEC+ has ended, as defined in Section 4.1.
the ram pressure and, consequently, the effect of the accretion of the
Si/O interface through the shock.
The results of the simulations for the 20 𝑀⊙MESA progenitor
from Farmer et al.  and for the 21 𝑀⊙KEPLER progenitor
from Sukhbold et al.  are shown in Figures 6 and 7, respec-
tively. These two progenitors were chosen because of their large
interfaces and the number of computational zones within the inter-
face, allowing for a more accurate smoothing procedure. Moreover,
the accretion of the Si/O interface occurs at ∼0.22 s after bounce
for the 20 𝑀⊙MESA progenitor, and at 0.1 s post bounce for the
accretion changes the impact of the interface on the explosion.
The second panel of Figures 6 & 7 shows the average mass ac-
cretion rate in the gain region and, as expected, a smoother density
profile at the Si/O interface corresponds to a shallower drop in mass
accretion rate, which therefore prevents the shock from expanding.
The third panels of those figures show the shock evolution for progen-
itors with different degrees of smoothing. The original progenitors
explode and as the smoothing increases the shock expansion caused
by the accretion of the Si/O interface becomes smaller, until even-
tually, it is not large enough to trigger explosion. This can be seen
by looking at the last panel, which shows that for the unmodified
progenitor, the FEC+ goes above the threshold as a consequence of
the accretion of the Si/O interface, resulting in an explosion. As the
smoothing increases, the increase is smaller and smaller until the
FEC+ is not pushed above the threshold, and therefore the star does
not explode.
The 20 𝑀⊙MESA progenitor and the 21 𝑀⊙KEPLER progeni-
tor show the same general behavior concerning smoothing, and the
threshold of the FEC+ is at around 0.28–0.3, consistent with the one
found in the simulations discussed in sections 3 and 4.1. Moreover,
the 21 𝑀⊙KEPLER progenitor shows an early accretion of the Si/O
interface, much like the 19.5 𝑀⊙progenitor shown in Figure 2, the
difference being that the Si/O interface for the 21 𝑀⊙progenitor has
a larger 𝛿𝜌/𝜌, and therefore explodes early. However, as the degree
of smoothing increases, one can see that for Δ𝑅/Δ𝑅orig ≈10  the 21 𝑀⊙progenitor has a very similar
behavior to the 19.5 𝑀⊙progenitor. An early accretion of a relatively
large Si/O interface leads to the shock stalling at a large radius, and
the FEC+ being pushed just below threshold. Eventually, the FEC+
slowly increases and the star explodes a few hundred milliseconds
later. If one increases the smoothing further, the star does not explode.
This confirms that the explosion scenario  in Section 3, i.e.
where the explosion is indirectly triggered by an early accretion of a
large Si/O interface followed by a late explosion, is indeed an ""edge
case"". The accretion of the Si/O interface needs to be quite early, and
the density drop should not be too small, like in the case of the 15.5
𝑀⊙progenitor in Figure 2. It should also not be too large, like in the
case of the 21 𝑀⊙progenitor studied here, which eventually behaves
like the 19.5 𝑀⊙progenitor in Figure 2 only when the density drop
is artificially smoothed.
This study shows how important it is to properly calculate the
Si/O interface. This is no easy feat, given the complexity of stellar
evolution calculations. For one these shells may or may not be a
consequence of how 1D stellar evolution models handle the bound-
aries between stable layers and convectively unstable layers, which
are inherently 3D, and even in simple 1D calculations, shell mergers
might occur depending on how the convective boundaries evolve.
In addition, it is well known that reduced burning networks, which
decrease the computational cost and facilitate convergence, can be
quite unreliable at the late stages of post-main sequence evolution of
massive stars .
FEC+
In Section 4.1 we showed how the change of the FEC+ induced by
the accretion of the Si/O interface is proportional to the strength of
neutrino-driven convection, and also to the value of the FEC+ itself
right before the accretion. In this section, we quantify how the Si/O
interface impacts the explosion by calculating the changes in the
FEC+ caused by the accretion of the Si/O interface across the 341
progenitors analyzed in section 3.
First, one has to distinguish among the three major outcomes,
displayed in Figure 8 with three different color schemes: 1) failed
explosions shown as black dots; 2) high-compactness progenitors
that usually explode before the accretion of the Si/O interface, shown
as purple dots; 3) explosions caused by the accretion of the Si/O
interface, shown as colored dots and crosses.
First, we focus our analysis on how the value of the FEC+ at
𝑡= 𝑡start
accr is related to the value of the FEC+ at 𝑡= 𝑡end
accr . We define the time before  accretion as the time
when the inner  side of the Si/O interface, i.e. the left 
edge of the rectangles in the right panel of Figure 2 is accreted
through the shock. This definition accurately captures the change
MNRAS 000, 1–13 
L. Boccioli et al.
Time post bounce 
Time post bounce 
Shock Radius 
Time post bounce 
FEC+
Figure 7. Same as Figure 6, but for a 21 𝑀⊙KEPLER progenitor 
in FEC+ for failed explosions  and high-compactness
progenitors . However, one needs to be more careful
when the explosion occurs after the accretion of the Si/O interface
after accretion"" as the time after which the transient phase has ended,
as explained in Section 4.1.
For the failed explosions  one expects scatter around
the bisector. The reason is that when the Si/O interface is small  it will not modify much the FEC+, and
therefore the FEC+ before and after the Si/O interface is accreted
through the shock should not be very different. The only exception
is when the accretion happens during a rapid shock expansion , typically at early  times after bounce. By the time the
outer edge of the Si/O interface is accreted, the FEC+ has there-
fore changed significantly. This explains the points above and below
the bisector, for which the accretion occurs at early and late times,
respectively.
For the high-compactness progenitors, shown as purple dots, one
expects the FEC+ at 𝑡= 𝑡start
accr to be already above the threshold
since, in most cases, the explosion sets in before the accretion of the
Si/O interface. This is indeed confirmed by the fact that most of the
purple dots are located at values of FEC+ at 𝑡= 𝑡start
accr larger than 0.3,
indicating that the explosion has already set in. Note that there are
three isolated cases in which accretion of the Si/O interface happens
very early in the evolution while the shock is still expanding and has
not yet reached the stalled shock phase. In these cases, while the early
accretion of the Si/O interface changes the FEC+, the main change
is due to the expansion of the shock.
Finally, the exploding progenitors with 𝜉2.0 < 0.5 are the re-
maining dots and crosses, color-coded based on the time when the
accretion of the Si/O interface occurs. For these, the Si/O interface
triggers an explosion, and the FEC+ quantifies why. Before the ac-
cretion of the Si/O interface, FEC+ is below the threshold, and after
the accretion of the Si/O interface FEC+ is at or above the threshold.
The progenitors shown as crosses are characterized by a very early
accretion of the Si/O interface  and are therefore exceptions that will be
discussed later in the section.
Similar conclusions can be drawn from the middle panel, in which
ΔFEC+  is plotted against the value of the FEC+ at 𝑡= 𝑡start
accr. The
correlation between these two qunatities is quite evident. The rea-
son is that, if the accretion occurs early in the post-bounce phase,
the shock is still expanding, and therefore the FEC+ is also increas-
ing. Therefore, one expects larger ΔFEC+ since not only does the
accretion increases the FEC+, but the secular shock expansion also
contributes to the increase. This is confirmed by the fact that sim-
ulations with early accretions  also exhibit larger
ΔFEC+. As mentioned above, the reason is that if the accretion of the
Si/O interface occurs early enough in the post-bounce phase, then the
FEC+ increases not only because of the accretion but also because of
the secular shock expansion. Notice that this discussion is not valid
for failed explosions and high-compactness progenitors.
In the last panel, we show ΔFEC+ as a function of 𝛿𝜌2
Si/O/𝜌2
Si/O,
where 𝜌2
Si/O is the density on the inner side of the Si/O interface,
and 𝛿𝜌Si/O is the density drop from the inner to the outer side of
the interface. Examples of this are given in the rightmost panel of
Figure 2. One would naively expect larger density drops to cause
larger ΔFEC+, and indeed that trend is generally visible. However, it
is also polluted by a large vertical scatter, which is due to early time
accretions, for which not only the accretion of the Si/O interface, but
also secular shock expansion contribute to ΔFEC+. This is confirmed
by the fact that, given the same 𝛿𝜌2
Si/O/𝜌2
Si/O, simulations where
the Si/O interface is accreted at early times lead to larger ΔFEC+.
Therefore, if one takes into account the contribution to the FEC+
by secular shock expansion, one can find a much tighter dependence
of ΔFEC+ on 𝛿𝜌2
Si/O/𝜌2
Si/O. By only selecting progenitors that have
a relatively similar accretion time of the Si/O interface  defined in Section 3, for which the
early accretion of the Si/O interface does not push the FEC+ above
the threshold, but instead pushes it very close to it, facilitating the oc-
currence of an explosion a few hundred milliseconds later. Therefore
one expects the FEC+at 𝑡= 𝑡end
accr to be below the threshold, and the
ΔFEC+ to be very large since the earlier the accretion, the faster the
shock expansion, and therefore the larger the FEC+ increase. This
is indeed confirmed by the left and middle panels in Figure 8. Most
of the simulations shown as rainbow-colored dots instead belong to
scenario 2) defined in Section 3. These are cases where the accretion
of a large Si/O interface occurs during the stalled shock phase, pushes
the FEC+ above the threshold, and therefore causes the explosion.
As seen in the left panel of Figure 8, for some of them the FEC+ at
𝑡= 𝑡end
accr is however below the threshold, and 𝑡accr > 0.1 s. These can
be thought of as intermediate cases between scenarios 2) and 3). The
accretion of the Si/O interface occurs at the very beginning of the
stalled shock phase.
MNRAS 000, 1–13 
Impact of Si/O interface on CCSN explosions
FEC+ before accretion
FEC+ after accretion
ξ2.0 > 0.5
Failed explosion
FEC+ before accretion
Si/O/ρ2
Si/O
taccr of the Si/O interface
Figure 8. The left panel shows the values of the FEC+ at 𝑡= 𝑡start
accr and at 𝑡= 𝑡end
accr. The FEC+ threshold is shown as a shaded region between 0.28 and 0.3
on both the x and y-axis. The middle and right panels share the y-axis, which shows ΔFEC+, defined as the difference between the FEC+ at 𝑡= 𝑡end
accr and at
𝑡= 𝑡start
accr . Finally, the right panel has 𝛿𝜌2
Si/O/𝜌2
Si/O on the x-axis, where 𝛿𝜌represents the density jump at the Si/O interface in the pre-SN progenitors, and
𝜌Si/O is the density at the inner edge of the interface. The color of the dots highlights three scenarios: 1) the simulation fails to explode even after accreting the
Si/O interface ; 2) the simulation explodes even before the Si/O interface accretes ; 3) the Si/O interface triggers an explosion . Crosses indicate progenitors with 𝑡accr < 0.1 s.
Q10%
Q25%
Q75%
Q90%
𝑡accr > 0.12
𝑡accr > 0.15
𝑡accr > 0.18
𝑡accr > 0.2
FEC+ > 0.2
FEC+ > 0.22
FEC+ > 0.24
FEC+ > 0.26
Table 1. Statistical measures of ΔFEC+ for different samples, which include all exploding progenitors with 𝜉2.0 < 0.5, and further restricted based on the
condition described in the first column. The second column shows the size of the sample , and the remaining
columns are the mean, median, and 10%, 25%, 75%, and 90% quantiles. The percentage in parenthesis is calculated by dividing the number by the FEC+
threshold , and it therefore shows how much the accretion of the Si/O interface contributes to the overall explosion condition. The first row includes
all exploding progenitors with 𝜉2.0 < 0.5. The next four rows restrict the population further based on the accretion time of the Si/O interface. The last four rows
restrict the population further based on the value of the FEC+ right before the accretion of the Si/O interface. The second to last row represents the most reliable
population .
explosion condition
In Section 4.1 we already compared the magnitude of ΔFEC+ with
the overall explosion threshold of 0.28–0.3. However, that was done
only for one progenitor, and we found that ΔFEC+ is about 10% of the
explosion condition. With the much larger sample of 341 progenitors,
we can perform the same exercise and estimate the average impact
that the Si/O interface has on the explosion condition. This is not
a straightforward task since, as explained above, the secular shock
expansion can sometimes overlap with the increase in FEC+ due
to the Si/O interface accretion. Therefore, we exclude all of the
high-compactness progenitors from the subsequent analysis, since as
explained above the vast majority accreted the Si/O interface after
the explosion has started. Then, we limit our sample to exploding
progenitors that accrete the Si/O interface during the stalled-shock
phase, when changes to the FEC+ can be attributed exclusively to the
accretion of the interface, rather than to secular shock expansion. This
can be achieved by excluding all progenitors for which the accretion
of the interface occurs before a certain time or, alternatively, by
excluding all progenitors for which the FEC+ at 𝑡= 𝑡start
accr is below
a certain value. Depending on the chosen condition, the sample will
be different.
In Table 1 we show how different cuts on 𝑡accr and the value of the
FEC+ at 𝑡= 𝑡start
accr affect the sample size. Then, for the progenitors
that satisfy those conditions, we calculate the median, mean, and se-
lected quantiles of ΔFEC+. We find the conditions 𝑡accr > 0.18 s and
FEC+ > 0.24 to lead to almost the same sample. We also consider
them to be the best cuts since they include as many simulations as
possible, without however including cases where significant secular
shock expansion is occurring alongside the accretion of the Si/O in-
terface. Notice that the condition that the FEC+ at 𝑡= 𝑡start
accr should
be > 0.24 is the same found in Section 4.1 in order for the 20𝑀⊙
MESA progenitor to explode. For both samples, the median ΔFEC+
is ∼0.035, which is in line with what was found for the 20𝑀⊙
MESA progenitor, and it is ∼10% of the explosion condition, to be
compared with the 25–30% effect of convection . The first and third quartiles are ∼0.023 and ∼0.046, which
correspond to ∼8% and ∼15% of the explosion condition, and more
than 90% of the simulations have ΔFEC+ > 0.015, corresponding
MNRAS 000, 1–13 
L. Boccioli et al.
to ∼5% of the explosion condition. This shows that the accretion
of the Si/O interface has a crucial impact on the explosion of pro-
genitors with compactness 𝜉2.0 < 0.5. This impact can be quantified
to be between 5% and 15% of the overall explosion condition, or
equivalently between 20% and 50% of the effect of convection.
In this paper, we analyzed the post-bounce phase of 341 1D+ simula-
tions using the generalized Force Explosion Condition . We
found that when the FEC+ goes beyond the threshold, empirically
found to be around 0.28–0.3, an explosion ensues. We identified three
scenarios where an explosion is achieved:  High-compactness pro-
genitors tend to always explode, often before accreting the Si/O in-
terface;  lower-compactness progenitors, with 𝜉2.0 ≲0.5, achieve
an explosion if a large enough density drop 
is accreted through the shock during the stalled-shock phase;  a
progenitor with 𝜉2.0 ≲0.5 achieves an explosion significantly after
the accretion of a large density drop before the stalled-shock phase.
For failed explosions, we identified three scenarios: 4) a progenitor
with 𝜉2.0 ≲0.5 accretes a large density drop before the stalled-
shock phase, although smaller than in scenario 3), and therefore
not sufficient to cause an explosion; 5) a progenitor with 𝜉2.0 ≲
stalled-shock phase; 6) a progenitor with 𝜉2.0 ≲0.5 accretes either a
small or large density drop late in the stalled-shock phase when the
shock cannot be revived anymore.
In all cases, the explosions ensue only when the FEC+ crosses
the threshold of 0.28–0.3, showing that the FEC+ is a robust tool to
describe the explosion condition.
We also studied how the FEC+  varies
depending on the strength of 𝜈-driven convection . We again showed that the explosion occurs
only when the FEC+ goes above the threshold of 0.28–0.3 empirically
derived in Section 3, which confirms the robustness of the FEC+ as
an explosion condition. We showed that the value of the FEC+ at
𝑡= 𝑡start
accr, as well as the change in FEC+ caused by the accretion
of the Si/O interface, are positively correlated. They also positively
correlate with 𝛼MLT, the radius of the stalled shock and the increase in
shock radius caused by the accretion of the interface. This confirms
the key role that convection has in aiding the explosion, which is
extremely important since it has been shown that, for example, three-
dimensional asymmetries in the pre-SN progenitor 
can significantly change the strength of convection that develops
in the post-bounce phase. Moreover, it is still unclear if and how
numerical resolution 
or the dimensionality of the problem  can artificially alter the efficiency of convection.
Moreover, we show that the change in the FEC+ due to the accre-
tion of the Si/O interface becomes progressively smaller by artifi-
cially smoothing the density drop at the Si/O interface. Eventually,
for a large enough smoothing parameter, the FEC+ drops below the
threshold causing the originally successful explosion to fail. Once
again, we verified that the threshold in these simulations is also at
around 0.28–0.3. We conclude that for progenitors with low to inter-
mediate compactness , the presence of a large density
drop at the Si/O interface is crucial to determine whether or not an
explosion occurs.
Finally, we analyzed how the change in FEC+ is related to the value
of the FEC+ right before accretion of the Si/O interface through the
shock, and also to the density drop at the Si/O interface 𝛿𝜌2
Si/O/𝜌2
Si/O.
We concluded that for the accretion of the Si/O interface to lead
to successful explosions, the FEC+ should already be close to the
explosion threshold, depending on how large the density drop at the
Si/O interface is. We then quantified the effect of the Si/O interface
accretion by analyzing the exploding progenitors with 𝜉2.0 ≲0.5.
The analysis for progenitors accreting the interface early  is complicated by the fact that the FEC+
changes not only because of the shock expansion caused by the ac-
cretion of the Si/O interface, but also because of a secular shock
expansion. By focusing only on progenitors where the secular shock
expansion is negligible, we find that the accretion of the Si/O inter-
face can contribute between 5 % and 15% to the overall explosion
condition, i.e. between 20% and 50% of the overall effect of convec-
tion, which has been estimated to decrease the explosion condition
by about 25–30 % .
The FEC+ is a powerful tool to analyze the post-bounce phase
of the CCSN, and can be also applied to multi-dimensional simu-
lations, where however more complex phenomena and geometries
are at play, and therefore more careful analysis is required. These
convection and the accretion of the Si/O interface can be crucial for
the explosion, and therefore future CCSN simulations should ensure
that convection is properly resolved and carefully analyzed.
Moreover, it is important to utilize  pre-
collapse models whose late stages of evolution have been carefully
calculated. Since the density drop at the Si/O interface plays such
a crucial role in the explosion, contributing to roughly 10 % of the
overall explosion condition, detailed stellar evolution models are re-
quired in order to accurately simulate the last few months of the life
of massive stars, where shell Si-burning becomes important and can
drastically modify the density discontinuity at the interface with the
oxygen shell.","[('fec', 191), ('si', 168), ('interface', 160), ('explosion', 158), ('shock', 121), ('accretion', 116), ('progenitor', 98), ('simulation', 69), ('density', 64), ('time', 51)]","[(('si', 'interface'), 139), (('accretion', 'si'), 64), (('density', 'drop'), 39), (('explosion', 'condition'), 28), (('post', 'bounce'), 25), (('stalled', 'shock'), 24), (('shock', 'radius'), 22), (('start', 'accr'), 22), (('shock', 'phase'), 19), (('large', 'density'), 18)]","[(('accretion', 'si', 'interface'), 57), (('stalled', 'shock', 'phase'), 19), (('large', 'density', 'drop'), 17), (('fec', 'start', 'accr'), 16), (('density', 'drop', 'si'), 15), (('drop', 'si', 'interface'), 15), (('time', 'post', 'bounce'), 12), (('si', 'interface', 'accreted'), 10), (('post', 'bounce', 'phase'), 9), (('si', 'interface', 'shock'), 9)]"
2410.17252v1.txt,"Draft version October 23, 2024
Typeset using LATEX twocolumn style in AASTeX62
Cosmic Ray Mediated Thermal Fronts in the Warm-Hot Circumgalactic Medium
Hanjue Zhu ,1 Ellen G. Zweibel,2, 3 and Nickolay Y. Gnedin4, 5, 1
Submitted to ApJ
We investigate the 1D plane-parallel front connecting the warm  and hot  phases of
the circumgalactic medium , focusing on the influence of cosmic rays  in shaping these
transition layers. We find that cosmic rays dictate the thermal balance while other fluxes  adjust to compensate. We compute column density ratios
for selected transition temperature ions and compare them with observational data. While most of
our models fail to reproduce the observations, a few are successful, although we make no claims for
their uniqueness. Some of the discrepancies may indicate challenges in capturing the profiles in cooler,
photoionized regions, as has been suggested for by previous efforts to model thermal transition layers.
The circumgalactic medium , a gaseous halo
enveloping galactic disks, acts as both a reservoir for
star formation and a repository for galactic outflows and
feedback processes. Its multiphase structure, character-
ized by regions of varying temperatures and densities,
retains the imprints of the galaxy’s formation and evo-
lutionary history. Investigating the CGM is crucial for
understanding the baryon cycle in galaxies, as it reg-
ulates the flow of gas between the galaxy and the in-
tergalactic medium, thereby influencing star formation
rates and the effectiveness of feedback processes .
Recent observational and theoretical studies have sig-
nificantly advanced our understanding of the CGM. Ob-
servations using metal absorption lines, such as those
from O VI, C IV, and Mg II, have unveiled the complex
multiphase nature of the CGM, highlighting substantial
variations in temperature, density, and ionization states
across different regions . Discrepancies between observed line
ratios and model predictions indicate the necessity for
models beyond collisional ionization equilibrium 
Corresponding author: Hanjue Zhu 
hanjuezhu@uchicago.edu
and photoionization equilibrium  to accurately in-
terpret CGM properties. Efforts to address these dis-
crepancies include considering radiative cooling flows
that incorporate gas dynamics and self-photoionization,
turbulent mixing layers where Kelvin-Helmholtz insta-
bilities create intermediate-temperature regions around
clouds, conductive interfaces where cool clouds evapo-
rate and hot gas condenses at the surface due to heat
conduction, and ionized gas behind radiative shocks po-
tentially produced by strong galactic winds .
An important aspect of the CGM is the influence of
cosmic rays .
CRs provide additional pressure
support, affect thermal balance, and may drive galac-
tic winds, all of which shape the overall structure and
evolution of the CGM. Their ability to propagate over
vast distances and recoup their losses by extracting en-
ergy from ambient shocks and turbulence allows CRs to
impact regions far from their origin, influencing the ther-
mal and dynamical properties of the CGM on a larger
scale . How-
ever, in the absence of observational evidence for cosmic
rays in the CGM, we can only speculate upon their in-
fluence. And because direct detection of cosmic rays in
diffuse environments such as the CGM is challenging, we
arXiv:2410.17252v1    22 Oct 2024
Zhu, Zweibel, Gnedin
must resort to observational predictions based on mod-
eling the thermal gas itself.
The transfer of momentum and energy between cos-
mic rays and thermal gas is mediated by magnetic field
structures on a scale of order the cosmic ray gyroradius.
These small-scale magnetic structures could be part of
an extrinsic turbulent cascade, but can also be driven by
the cosmic rays themselves via the so-called streaming
instability . The latter case is
known as cosmic ray self confinement, and we will as-
sume throughout this paper that it holds. An interesting
consequence of the theory is that it breaks down if the
projections of the cosmic ray pressure gradient and ther-
mal plasma density gradient along the background mag-
netic field oppose each other. In this case - as predicted
from analytical arguments by Skilling  and shown
first in a simulation by Wiener et al. , scattering
disappears, the cosmic rays decouple from the gas, and
their pressure becomes constant. Such transport bottle-
necks should be common in the CGM, with its clumpy
density structure, provided the clumps are magnetically
connected to the Galactic disk, where most cosmic rays
presumably originate.
Bottlenecks were studied in a series of papers of in-
creasing generality in Wiener et al. ; Bus-
tard & Zweibel ; Tsung et al. . These works
showed that the large CR pressure gradient accelerates
cool clouds upward, imparting significant momentum
and causing changes in gas density and temperature pro-
files.
Additionally, CRs were found to pressurize the
cold gas, reducing its density and heating the cloud’s
surface, leading to broader transition layers and altering
ion abundances. These findings underscore the signifi-
cant role of CRs in shaping the thermal and dynamical
properties of the CGM. Due to the disparity in scale
between the assumed cloud size and the front thickness,
the transition region has only been modeled previously
as a 1D structure in a steady state .
The transition layers between the cool and hot phases,
characterized by a density contrast typically around a
factor of 100 as temperatures shift from 104 K to 106 K,
are particularly affected by CRs. Models of these tran-
sition layers are crucial not only for explaining observed
line ratios but as was shown in ,
may also be indicators of clouds dominated by cosmic
ray pressure. This dominance would signify that CRs
are a significant component of the galactic structure, af-
fecting both the local environment of the CGM and the
broader galactic ecosystem; it would also be evidence for
the theory of cosmic ray self confinement.
In this study, we refine the 1D plane-parallel symmet-
ric model presented by Wiener et al.  by incor-
porating thermal conduction and gas flow in the energy
equation in addition to cosmic ray heating and radiative
cooling. These additions aim to broaden the range of
cosmic ray-mediated front models and provide a more
comprehensive understanding of the CGM’s structure
and dynamics.
The plan of this paper is as follows. In §§2.1, we briefly
review some well-known front models. In §§2.2 we for-
mulate the problem, with a separate discussion of the
boundary conditions in §§2.3. Section 3 gives detailed
discussions of two types of front, static  and evap-
orative .
We show the effects of changing the
background magnetic field and cosmic ray pressure in
the cloud in §4. In §5 we compute the column densities
of several transition temperature ions across our range
of front models and compare their ratios to the ratios
derived by Wakker et al.  from absorption line
spectroscopy. Section 6 summarizes and discusses our
main conclusions. Appendix A is a short discussion of
condensation fronts. Appendix B shows some results on
how the column densities of transition temperature ions
depend on domain over which we integrate.
Past work on fronts without cosmic rays provides valu-
able context for understanding the distinct role of CRs
in gas dynamics. Cowie & McKee  investigated
evaporative fronts in the interstellar medium, where
thermal conduction from a surrounding hot medium
drives the continuous evaporation of cooler gas clouds.
Their model established a steady-state balance between
thermal conduction and enthalpy flux as the heating and
cooling terms, respectively, with mass being lost from
the cool phase to the hot phase. They also demonstrated
how the geometry of the system influences the stability
and evolution of these fronts. Inoue et al.  later
examined phase transition layers connecting two ther-
mal equilibrium phases, a cold neutral medium cooled
by C II and a warm neutral medium cooled by Lyα,
with heating provided by photoelectrons from dust. Be-
cause both studies solved for temperature assuming en-
ergy balance and heat transfer by classical conduction1
they were treated as two-point boundary value prob-
lems.
In Cowie & McKee , the temperature of
the hot medium was chosen based on observations and
the cloud temperature was approximated by zero. In-
does not follow Fick’s Law, but that is immaterial for our purposes.
Cosmic Ray Mediated Thermal Fronts
oue et al.  set the boundary temperatures to their
equilibrium values.
In Wiener et al. , thermal conduction and en-
thalpy flux were dropped, leaving collisionless cosmic
ray heating supplemented by a base heating rate per
particle to balance radiative cooling. Without heat con-
duction, the energy equation was reduced to first or-
der. The models were integrated outward for 100 pc, at
which point T had typically increased from its chosen
value of 104 K at the cloud boundary to a few 106 K.
Over this temperature range it was not possible to find
a reasonable high-temperature thermal equilibrium, so
the model was solved as an initial value problem and the
temperature was still increasing at the 100 pc cutoff. It
was checked a posteriori that the fronts found from the
model were too thick for heat transport by thermal con-
duction to play a significant role. We expand on this
issue in §3.
The evolution equations governing a thermal gas de-
scribe the conservation of mass, momentum, and energy.
In 1D, they are:
v −κT
respectively; vA is the Alfv´en velocity B/√4πρ; Pg and
Eg denote the thermal pressure and total energy density
of the gas, with
Eg =
γg −1 + 1
κT stands for the thermal conduction coefficient, and ρL
is the net cooling function. For simplicity, we assume a
fully ionized Hydrogen plasma and an ideal gas equation
of state, and take γg = 5/3 throughout.
We use a net cooling function ρL ≡n2Λ −nΓ. We
adopt Γ = 1×10−26 erg/s, consistent with the value used
in . Because of the sharp cooling drop-
off near T = 104 K, this choice of Γ allows for thermal
equilibrium at both T ∼104 K and T ∼106 K. We note
that this value of Γ is higher than the lower limit set by
Coulomb scattering of cosmic rays in thermal plasma
we use
Λ = 1.1×10−21×10Θ)erg cm3 s−1, 
Θ = 0.4x −3 +
ex+0.08 + e−1.5 .
in Imada & Zweibel  and modified in Wiener et al.
licity cooling rates from Wiersma et al. , where
they used the Cloudy photoionization code to calculate
cooling functions for gas with varying metallicities, tem-
peratures and densities under a UV/X-ray background
at z = 0.
In the fluid approximation, cosmic rays are governed
by the energy equation :
resents the CR energy flux:
Fc = γcPc −κc
The influence of cosmic rays is manifested through
the cosmic ray pressure  gradient term in the mo-
mentum equation and the heating term in the energy
equation.
As shown in Wiener et al. ; Zweibel
the transfer of cosmic ray energy to Alfv´en waves due
to the streaming instability together with the transfer of
wave energy to the thermal background, as must occur
in a steady state. The pressure gradient force represents
momentum transfer between the cosmic rays and the
magnetic field fluctuations, and occurs whether cosmic
rays are self confined or confined by extrinsic turbulence.
We seek steady state solutions of Equations  - 
under the further simplifying assumptions that cosmic
ray diffusion is negligible and v/vA ≪1. That is, we
set all time derivatives to zero, drop κc, and wherever
v + vA appears, we replace it by vA. This leads to the
following three conserved quantities:
ρv = constant,
ρv2 + Pg + Pc = constant,
Pc/ργc/2 = constant.
Zhu, Zweibel, Gnedin
Fiducial Value
np,0
Pg,0
Pc,0
Pg,0
Table 1. Parameters values adopted in this paper.
From equations 8 we obtain the following relationship
between number density  and temperature :
ρv2 +nkBT +Pc,02/3 = ρ0v2
where the “0” subscripts denote initial values. This al-
lows us to treat T as a function of n. We then recast
eqn.  as a second order ordinary differential equation
tial value problem to determine the spatial profile of the
variables. The boundary values are established within
the warm ionized medium, as detailed in Section 2.3.
In the warm ionized medium on the left boundary
number density  of 0.1 cm−3. The initial temper-
ature  is established at 104 K, leading to an initial
gas pressure  of 1.38 × 10−13 erg/cm3.
These boundary conditions are chosen to represent typi-
cal conditions in the warm ionized medium of the CGM.
In this environment, the thermal conduction coefficient
is given by κT = 5.6 × 10−7T 5/2 erg/s/K/cm .
In our fiducial case, we adopt a constant magnetic
field strength B = 3 µG and assume the field is per-
pendicular to the plane of the front. This choice aligns
with observations of the ∼µG magnetic field strength
in the CGM .
For the evaporative solutions, we set the gas velocity
at the boundary  to 400 m/s, neglecting the effects of
any relative motion between the cloud and the ambient
medium. The positive value of the mass flux corresponds
to the warm medium evaporating into the hot medium.
We also explore the scenario with a static front, where
v = 0. We show in Appendix A that a condensation
front cannot form under these conditions. In our setup,
we have vA ≫v, which satisfies the condition for the
validity of Equation 3. In our fiducial case, we assume
Pc,0 = 2Pg,0 ), but the
initial ratio of CR pressure to gas pressure is a parameter
we vary; we introduce the parameter α ≡Pc,0/Pg,0 + 1.
We list all relevant parameters in Table 1.
We proceed by solving the energy equation , reformulated as a second-order initial value
problem for n, imposing two boundary conditions: n0 =
plore the effect of the gradient  on our outcomes.
defer discussions of how dn
dx is selected in a natural envi-
ronment to future works, except for a few remarks in §6.
In Wiener et al.  the front models are generated
by integrating a first order ODE, and dn
through the polytropic relationship between Pc and n,
along with the requirement that cosmic ray heating bal-
ances radiative cooling.
In this section, we first examine the simpler static
front case  to isolate and better under-
stand the interaction between thermal conduction, cos-
mic ray heating, and radiative cooling. We numerically
solve the energy equation 
density, temperature, and pressure profiles as a func-
tion of distance in Figure 1.
Following Wiener et al.
Cosmic Ray Mediated Thermal Fronts
Number Density
dx |0 = 10
dx |0 = 10
dx |0 = 10
dx |0 = 10
Pressure/kB
Figure 1. Number density , temperature , and
pressure  profiles for a static front system, where
the velocity v = 0 and only thermal conduction, cosmic ray
heating, and radiative cooling interact. The profiles are dif-
ferentiated by the initial gradients of number density, dn
across a range from 10−14 to 10−17 cm−4.
We emphasize
that except for the initial gradients, all profiles originate from
the same initial conditions. The top panel shows the num-
ber density profiles, where steeper initial gradients lead to
a more rapid decline in number density from the origin. In
contrast, the less steep initial gradients see a substantial and
sharp decrease around ∼1 pc, indicating a delayed transi-
tion. The middle panel shows the corresponding temperature
profiles. The different profiles underscore the significant im-
pact of the initial gradient on the structure of the front. The
bottom panel illustrates the pressure profiles; while the ini-
tial cosmic ray pressure is twice that of the gas pressure, it
gradually decreases as the density drops.
tinguished by their initial gradients of number density,
the models are initialized with cosmic ray pressure dom-
inant , and all become gas pressure domi-
nated at large distances, typically by about an order of
magnitude. This is due to the polytropic relationship
between Pc and ρ, whereas the decrease in ρ is compen-
sated by an increase in T. As shown in Figure 1, steeper
initial gradients , which
imply stronger influences from thermal conduction and
cosmic ray heating, resulting in a noticeable and imme-
diate decrease in number density from the origin.
contrast, profiles with less steep initial gradients , only show a significant decrease in
number density at ∼1 pc, indicating a delayed transi-
tion. Regardless of the initial gradient, the number den-
sity always decreases to of order 10−3 cm−3 at x = 100
pc, with the steepest gradient reaching ∼1×10−3 cm−3
and the least steep gradient dropping to ∼3 × 10−3
cm−3. The same behavior is observed in the temper-
ature profiles in the middle panel of Figure 1. Lower
cm−4 one; we display one profile here for clarity. The
bottom panel presents the pressure profiles for all four
In Figure 2, we show the contributions of each flux
component as a function of distance.
In the steepest
initial gradient case, cosmic ray heating is consistently
offset by conductive cooling. When dn
cosmic ray heating decreases to be smaller than radia-
tive cooling, radiative cooling is then countered by CR
heating and conductive heating. This balance contin-
ues all the way to the boundary at x = 100 pc. It is
not surprising that all these models require conductive
cooling near the cloud, since the gradient required to
balance cosmic ray heating with radiative cooling is of
order 10−21 cm−4, much smaller than any of the models
presented here.
Comparing our static front results with previous work
highlights the influence of cosmic rays on the energy
balance.
In Cowie & McKee , without cosmic
rays, conductive heating balances radiative cooling at
the front. In contrast, when cosmic rays are included,
as in our model and Wiener et al. , cosmic ray
heating becomes the dominant process balancing radia-
tive cooling. Nonetheless, thermal conduction remains
significant, as strong cosmic ray heating requires con-
ductive cooling to dissipate the excess energy. Our re-
sult highlights the crucial role of thermal conduction in
maintaining thermal equilibrium within the system.
Zhu, Zweibel, Gnedin
dx |0 = 10
dx |0 = 10
dx |0 = 10
dx |0 = 10
Flux Gradient 
Figure 2.
Contributions of each flux component as a function of distance in the static case, in solid and dashed lines in
erg cm−3s−1. This figure delineates the varying dynamics between different flux components under different initial number
density gradients. For smaller initial number density gradients, cosmic ray heating first is balanced by conductive cooling,
until it decreases to the level of radiative cooling, which is when conduction becomes less dominant heating term. For larger
initial gradients, the transition from conductive cooling to heating occurs over a more extended distance . The
interplay between these components is critical to understanding the thermal dynamics of the system.
In this section, we incorporate an evaporative flow into
the energy equation:
γg −1Pgv −κT
the resulting number density, temperature, and pres-
sure profiles in Figure 3.
For comparison, the static
front cases are shown with dashed lines. Notably, the
profiles of the evaporation fronts begin to deviate from
those of the static case beyond 0.1 pc, with these devi-
ations amplifying with increasing distance. Specifically,
the number density in the evaporation front scenario
drops more significantly compared to the static case,
which corresponds to a greater increase in temperature.
The pressure profiles show that as we move away from
the origin, the cosmic ray  and gas pressure profiles
evolve similarly to the static front case. However, after
the point where Pc = Pg, the difference between Pc and
Pg becomes greater compared to the static front sce-
nario, indicating a sharper transition between the roles
of CR pressure and gas pressure. The presence of flow
introduces dynamical pressure, which is absent in the
static case.
In the evaporation scenario, the pressure
profile shows a continuous decrease in the sum of CR and
gas pressures, accompanied by an increase in dynamical
pressure. However, the dynamical pressure remains rel-
atively small, accounting for only about 3% of the total
pressure at x = 100 pc, emphasizing the dominant role
of CR and gas pressures in shaping the dynamics of the
layer.
We also present the contributions of each flux com-
ponent as a function of distance in Figure 4. For less
steep initial gradients , we
identify three distinct regions.
Near the origin, cos-
mic ray  heating is the dominant process and is
balanced by conductive cooling. Further from the ori-
gin, there is a transition region where CR heating de-
creases to levels comparable to radiative cooling, caus-
ing conduction to shift from cooling to heating. This
transition indicates a regime where CR heating is bal-
anced by radiative cooling. Beyond this point, as ra-
diative cooling reaches its maximum and subsequently
declines, conductive heating and enthalpy flux cooling
become the dominant processes, similar to the Cowie &
McKee evaporation fronts. For steeper initial gradients
nificantly. Strong CR heating prevents radiative cooling
from fully balancing the thermal energy input, leading to
the absence of a region where radiative cooling balances
CR heating and thereby precluding the formation of re-
gions with flat number density and temperature profiles.
Cosmic Ray Mediated Thermal Fronts
These results underscore the complex interplay between
CR heating, radiative cooling, and conduction in regu-
lating the thermal dynamics of the system.
In this section, we examine the effects of chang-
ing physical parameters, specifically the magnetic field
strength and the ratio between cosmic ray pressure
and gas pressure at the x = 0 boundary, denoted as
α ≡Pc,0/Pg,0 + 1.
The magnetic field strength ap-
pears only as a multiplier in the cosmic ray heating term.
Meanwhile, the ratio α sets the initial cosmic ray pres-
sure and also determines the system’s total pressure,
with a higher α indicating increased initial CR pressure
and total pressure.
We investigate how variations in
these parameters impact the dynamics and structure of
the layer.
In this subsection, we discuss the results for α = 10.
With this higher value, at the x = 100 pc boundary,
the number density does not drop as low, reaching only
temperature profile is smaller, however.
Additionally,
the role of dynamical pressure across the layer becomes
much less significant.
At the x = 100 pc boundary,
dynamical pressure constitutes only 0.8% of the total
pressure, in contrast to the α = 3 case, where dynami-
cal pressure at the same boundary is comparable to CR
pressure.
Examining the component plot  reveals the
impact of α. For dn
ing at the origin extends the distance needed for conduc-
tion to change from cooling to heating. This difference
is illustrated in the top two panels of Figure 6. We also
observe that for dn
sity gradient, and hence the temperature gradient, are
steeper than their α = 3 counterparts . Additionally, after the peak in radiative
cooling, the α = 10 scenarios require a longer distance
for radiative cooling and CR heating to fall to levels
similar to enthalpy flux and conduction. This extended
distance is reflected in the more gradual changes in the
number density and temperature profiles in this region.
The component plots demonstrate how the strength of
CR heating at the origin impacts the system’s thermal
structure and the spatial extent of physical processes.
These observations are similar to the α = 3 case, where
the presence of CR heating leads to the adjustment of
other terms in the energy equations to counteract CR
heating.
Number Density
dx |0 = 10
dx |0 = 10
dx |0 = 10
dx |0 = 10
Pressure/kB
Figure 3. Number density, temperature, and pressure pro-
files in the fiducial, evaporative front case. The dashed, semi-
transparent lines show the flux magnitude for the static sce-
nario.
The top panel shows the number density profiles,
where steeper initial gradients lead to a more rapid decline
in number density from the origin. In contrast, the less steep
initial gradients see a substantial and sharp decrease around
shows the corresponding temperature profiles.
The differ-
ent profiles underscore the significant impact of the initial
gradient on the structure of the front.
The bottom panel
illustrates the pressure profiles; while the initial cosmic ray
pressure is twice that of the gas pressure, it gradually de-
creases as the density drops.
Zhu, Zweibel, Gnedin
dx |0 = 10
dx |0 = 10
dx |0 = 10
dx |0 = 10
Flux Gradient 
Figure 4. Contributions of each flux component as a function of distance in the fiducial case. The thin, semi-transparent
lines show the flux magnitude for the static scenario. For initial gradients less steep than 10−16 cm−4, three distinct phases are
identified: close to the origin, cosmic ray heating dominates and is counterbalanced by conductive cooling; at an intermediate
distance, a transition to conductive heating occurs as cosmic ray heating balances radiative cooling; and beyond, where conduc-
tive heating dominates against declining radiative cooling. For steeper initial gradients , CR heating is so
strong that radiative cooling cannot balance it, precluding the formation of regions with flat number density and temperature
profiles.
We now investigate the impact of a heightened mag-
netic field strength, B, set to 30 µG. Note that when
only B is adjusted, the total pressure of the system re-
mains the same; only the cosmic ray heating term in-
creases by a factor of 10. With this stronger magnetic
field, achieving number density and temperature profiles
as steep as in previous cases requires steeper initial num-
ber density gradients. This change is clearly shown in
Figure 7. A key difference between the B = 30µG result
and the α = 10 result is that, with constant total pres-
sure, the temperature does not reach ∼106 K, as it does
in the fiducial case and in the α = 10 case. Similarly,
the enhanced magnetic field reduces the relative impor-
tance of dynamical pressure compared to the trend in
the fiducial case .
Turning to Figure 8, we see that a tenfold increase
in the magnetic field strength causes CR heating to de-
crease more rapidly, aligning more quickly with radia-
tive cooling levels. For all initial density gradients, by
x = 100 pc, the relatively smaller decrease in number
density compared to the fiducial case means that en-
thalpy flux cooling and conductive heating are no longer
the dominant processes. Instead, CR heating and radia-
tive cooling become dominant at the x = 100 pc bound-
ary. This sustained dominance of CR heating and ra-
diative cooling at larger scales is a direct outcome of the
stronger magnetic field’s influence on the system.
It may seem paradoxical that increasing B, and with
it vA, increases the heating rate but decreases T far
from the cloud. We can understand this by observing
that as B increases, so does the range over which cosmic
ray heating is balanced by conductive cooling. This in
turn requires that T cannot increase too quickly. For
example, it can readily be shown that if T has a power
law dependence on position, T ∝xp, conductive cooling
requires p < 2/7. This accounts for the relatively slow
increase in T in the high magnetic field models.
We use the ionization fractions Xi from Gnat &
Sternberg  and the relative abundances AX from
Asplund et al.  to compute the column densities of
specific ions. The column density for each ion is defined
by NX = AX
Xin dx, where n is the number den-
sity, and the integral is across the front, up to a distance
of 100 pc. We discuss this choice of integration bound-
ary in Appendix B. We then compare our calculated
line ratios with measurements from high-velocity clouds
et al. . Discrepancies between our theoretical line
Cosmic Ray Mediated Thermal Fronts
Number Density
dx |0 = 10
dx |0 = 10
dx |0 = 10
dx |0 = 10
Pressure/kB
Figure 5. Profiles for number density , temperature , and pressure  for the evaporation
scenario with a high cosmic ray pressure to gas pressure ratio at the x = 0 boundary . In the top panels, the solid lines
represent α = 10, while the dashed semi-transparent lines depict the profiles for α = 3. The top panels illustrate the impact of
α on the number density and temperature profiles, showing that the number density reaches only ∼10−2 cm−3 at x = 100 pc
for dn
highest and lowest initial density gradients in blue and red, respectively, indicating that the transition from cosmic ray pressure
dominated to gas pressure dominated persists at higher α, while dynamical pressure constitutes only 0.8% of the total pressure
at the x = 100 pc boundary, in contrast to the α = 3 case where it is comparable to CR pressure.
ratios and the observational data could highlight poten-
tial shortcomings in the current models.
We compute the column densities for CIV, OVI, and
SiIV, and derive the diagnostic line ratios SiIV/CIV,
CIV/OVI, and NV/OVI. These ratios serve as key in-
dicators for understanding the ionization processes and
thermal structure within the interstellar medium. The
SiIV/CIV ratio probes cooler, photoionized gas with
temperatures in the range of 104 −105 K, primarily
indicating regions where UV radiation dominates the
ionization processes . In contrast, the CIV/OVI ratio spans the
transition between photoionized and hotter, collisionally
ionized gas, typically found at temperatures between
fying multiphase structures within the ISM, often trac-
ing shock-heated gas in more dynamic regions . Finally, the NV/OVI ratio
is sensitive to collisionally ionized gas in the warm-hot
ionized medium , making it a useful diagnostic
for gas in intermediate ionization states and the pres-
ence of shock-heated or disturbed gas .
We show in the left panel of Figure 9 our model-
dependent SiIV/CIV ratio versus CIV/OVI ratio in
logarithmic scale, while in the right panel we present
Zhu, Zweibel, Gnedin
dx |0 = 10
dx |0 = 10
dx |0 = 10
dx |0 = 10
Flux Gradient 
Figure 6. Flux component contributions in the evaporation scenario with α = 10, plotted over distance. Thin, semi-transparent
lines indicate the α = 3 fiducial case for comparison. The top two panels illustrate that for dn
heating at the origin extends the distance needed for conduction to change from cooling to heating. The bottom two panels
show that for
α = 3 counterparts. The component plots demonstrate the impact of CR heating on the front structure and the spatial extent
of physical processes.
NV/OVI ratio versus CIV/OVI ratio, overplotted with
observational data from Wakker et al. . The black
circular data points with error bars represent the obser-
vational measurements, and our model predictions are
superimposed as colored geometric shapes.
Addition-
ally, we include a CR heating balancing radiative cool-
ing model, as discussed in Wiener et al. , shown
in orange.
Our results reveal that most of the models lie well
outside the observational range in the SiIV/CIV versus
CIV/OVI parameter space. The highest magnetic field
sistent with the observations; however, these models si-
multaneously predict CIV/OVI ratios that are too high
compared to the observational data. In contrast, mod-
els with lower magnetic field values produce SiIV/CIV
ratios that are significantly lower than those observed in
the HVCs.
Similarly, in the right panel, the high B value models
systematically overestimate the NV/OVI ratio, placing
them outside the range of observational constraints. For
the other models, those with the largest dn
mate the CIV/OVI ratio, suggesting that the structure
and dynamics of the gas in these models may be in-
sufficient to fully capture the physical processes driving
the ionization conditions. However, the range of mod-
els that overlap the data plotted in  space is quite large, suggesting that this is not a par-
ticularly sensitive diagnostic of conditions in the front,
including the initial value of the density gradient.
A prima facie interpretation of these results suggests
that if a model lies within the observational range in
the NV/OVI versus CIV/OVI parameter space but fails
to reproduce the correct SiIV/CIV ratio, it implies that
the model is better at predicting the ionization profile for
gas in the 105−106 K temperature range than for cooler
gas at 104 −105 K. This also indicates that the model
more accurately captures conditions dominated by col-
lisional ionization rather than photoionization. These
discrepancies highlight potential limitations in the cur-
rent model framework, particularly regarding the role
of magnetic fields and cosmic ray heating in shaping
the ionization state of the ISM and CGM. In fact, the
line ratios are not well fit by previous models which do
not include cosmic rays ). Our α = 1 models, as shown in Figure 9, align
with this finding.
It should not be overlooked, however, that some of our
models do successfully reproduce both pairs of line ra-
tios. As might be expected from the forgoing discussion,
these are the models with intermediate field strength;
B = 20µG for α = 3. As we already noted, the fits are
relatively insensitive to the initial density gradient dn
which sets the initial cosmic ray heating rate and which
Cosmic Ray Mediated Thermal Fronts
Number Density
dx |0 = 10
dx |0 = 10
dx |0 = 10
dx |0 = 10
B = 3 G
Pressure/kB
B = 30 G
B = 3 G
Figure 7. Number density , temperature , and pressure  profiles for the evaporation scenario with
a magnetic field strength of 30 µG and a cosmic ray to gas pressure ratio  of 3. In the top panels, the solid lines represent the
current scenario, while the thin lines show the profiles with a magnetic field strength of 3 µG. The profiles indicate that with
a stronger magnetic field, dynamical pressure is reduced. Additionally, achieving similarly steep profiles as seen with a weaker
magnetic field requires steeper initial number density gradients.
we regard as the least well-determined parameter in our
models.
In this paper, we explored the evolution of the tran-
sition layer between warm  and hot  gas
phases in the circumgalactic medium by integrating the
energy equation  as an initial value prob-
lem. We showed that cosmic ray heating, thermal con-
duction, radiative cooling, enthalpy flux, and dynamical
pressure collectively shape the temperature and density
profiles of the transition layer. At large distances from
the cloud, we found that conduction is the main heat
source, and that it can be balanced by an outward en-
thalpy flux, as in the models of Cowie & McKee .
We also found that the temperature and density pro-
files across the front are quite sensitive to the value we
adopt for the initial density gradient. When the gradient
is sufficiently small, radiative cooling suffices to balance
cosmic ray heating as assumed in the front models of
Wiener et al. .
For sufficiently large gradients,
cosmic ray heating is balanced by conductive cooling.
In view of this sensitivity, we have to ask what controls
this ratio in a natural environment. While addressing
this question is beyond the scope of this paper, we spec-
ulate that the answer lies either in the front formation
process or in the stability of the front to small pertur-
bations. These possibilities will be examined in future
work.
Our findings emphasize the dominant role that cosmic
rays can play in regulating the structure of the transition
Zhu, Zweibel, Gnedin
dx |0 = 10
dx |0 = 10
dx |0 = 10
dx |0 = 10
Flux Gradient 
Figure 8. Flux component contributions in the evaporation scenario with α = 3 and a magnetic field strength of 30 µG.
Solid lines represent the heightened magnetic field scenario, while thin, semi-transparent lines depict the flux contributions for
a magnetic field strength of 3 µG. The plot shows a more rapid decline in CR heating, aligning more quickly with radiative
cooling levels. Additionally, beyond the cooling function peak, CR heating and radiative cooling remain the dominant processes
due to the smaller decline in number density.
B=3 G, =3
B=3 G, =10
B=20 G, =3
B=30 G, =3
dx |0 = 10
dx |0 = 10
dx |0 = 10
dx |0 = 10
CR heating + radiative cooling
Figure 9. Left panel: SiIV/CIV versus CIV/OVI ratios in logarithmic scale, compared with observational data from Wakker
et al.  . Right panel: NV/OVI versus CIV/OVI ratios, similarly overlaid with observational
data. Colored shapes denote our model predictions, with variations in magnetic field strengths and cosmic ray parameters. The
orange shapes represent models where CR heating balances radiative cooling, following Wiener et al. . Both panels reveal
discrepancies between the models and observations, particularly in the SiIV/CIV versus CIV/OVI space, where no models fit
within the observed range. High magnetic field models match SiIV/CIV but overestimate CIV/OVI, while lower field models
miss SiIV/CIV but align with CIV/OVI and NV/OVI. This suggests that in general, the models are more accurate at higher
temperatures but struggle with cooler, photoionized conditions. Note that models with intermediate field strength, B = 20µG,
do overlap the date on both plots, although we make no claim to their uniqueness.
layer between the warm and hot gas phases of the CGM.
The addition of CR heating in these models dictates the
thermal balance, with the other flux terms adjusting
accordingly to balance CR heating.
These effects are
sensitive to the magnetic field strength and the ratio of
CR to gas pressure  at the warm gas boundary.
Our analysis of ion column density ratios shows that
while our model predictions align with some observa-
Cosmic Ray Mediated Thermal Fronts
tional data, and some represent successful fits, discrep-
ancies persist.
Only the highest magnetic field mod-
els reproduce the observed SiIV/CIV ratios, but they
overestimate the CIV/OVI ratios.
Conversely, other
models underestimate the SiIV/CIV ratios, although
some fall within the observed range for CIV/OVI and
NV/OVI. The agreement with CIV/OVI and NV/OVI,
but not with SiIV/CIV, suggests that these models bet-
ter represent ionization conditions at higher tempera-
tures , where collisional ionization dom-
inates, but struggle to capture the profiles in cooler,
photoionized regions. These discrepancies indicate that
while CR heating significantly influences the ionization
structure of the CGM, additional processes such as tur-
bulent mixing, shocks, or self-ionization may be neces-
sary to fully explain the observed line ratios in lower
temperature regimes.
The need to include such ef-
fects would be particularly compelling if magnetic field
strengths as high as 20µG  could be ruled
out.
In addition to the influence of cosmic rays, our study
highlights the crucial role of thermal conduction in
maintaining energy balance within the transition layers.
Thermal conduction dissipates excess heat from cosmic
ray interactions, preventing runaway heating and stabi-
lizing the temperature profiles.
Overall, our findings reinforce the notion that the
CGM is a complex and dynamic environment where a
variety of physical processes, e.g. cosmic ray heating,
radiative cooling, thermal conduction, and gas flows, in-
teract to shape its structure and ionization state. Future
work will focus on investigating the stability of these
transition layers, particularly in the presence of cos-
mic ray pressure, magnetic fields, and thermal conduc-
tion. Studying the onset and development of instabili-
ties within these fronts will be crucial for understanding
their behavior.
We are happy to acknowledge useful discussions with
Roark Habegger, Tsuyoshi Inoue, Andrey Kravtsov,
Peng Oh, Brant Tan, Robert Benjamin, and Aaron
Tran.
This work was supported in part by the NSF
grant AST 2010189 and in part by grant NSF PHY-
Grant No. 2919.02 to the Kavli Institute for Theoretical
Physics . HZ thanks the Department of Astron-
omy at the University of Wisconsin-Madison for their
hospitality. This manuscript has also been co-authored
by Fermi Research Alliance, LLC under Contract No.
DE-AC02-07CH11359 with the United States Depart-
ment of Energy.
A. CONDENSATION FRONT
From the momentum conservation equation  in the steady-state case , we have:
pressure.
Differentiating the equation, we have:
dρ = 2
dρ > 0. Therefore, a monotonically decreasing density ρ corresponds to a
monotonically decreasing pressure P.
This relationship implies that the flow cannot proceed from the hot medium to the cold medium. Specifically, in a
scenario where the density is monotonically decreasing as the gas transitions from the cold to the hot medium, the
corresponding pressure must also decrease. Consequently, gas flow is driven in the direction of decreasing pressure,
which means it cannot move from the hot medium to the cold medium.
B. SENSITIVITY OF LINE RATIOS TO INTEGRATION CUTOFFS
The line ratios we presented are sensitive to the cutoff point chosen for the integration over the transition layer.
In Figure 10, we plot the line ratios , log, and log) as functions of temperature
where the integration is cut off, with the shaded regions representing the range of observational data. The vertical
lines indicate the temperature at x = 100 pc for the two models: the red vertical line corresponds to the temperature
for the model with dn
Zhu, Zweibel, Gnedin
As seen in Figure 10, the choice of temperature cutoff has a significant, nontrivial effect on the calculated line ratios.
Wiener et al.  used a cutoff at 2 × 106 K, which aligns more closely with the converged values in the higher-
temperature regime, compared to cutting off at 100 pc for the red model. However, while extending the integration
to higher temperatures may yield more stable line ratios, the physical thickness of the transition layers becomes an
important consideration. The models suggest that line ratios converge at higher temperatures, but the temperature at
x = 100 pc remains well below this threshold for both models. This highlights a key issue: while we can theoretically
integrate to very high temperatures, the question remains whether the transition layers are actually that thick in reality.
Currently, we lack observational probes that can resolve the full extent of these layers. This uncertainty underscores
the need for careful definition of the integration boundaries and comparison across models and observations.
dx |0 = 10
dx |0 = 10
Figure 10.
Line ratios log, log, and log as a function of the temperature at which the
integration is cut off. The shaded regions represent the range of observational data for each line ratio. The vertical red and
blue lines correspond to the temperatures at x = 100 pc for the models with dn","[('ray', 75), ('cosmic', 70), ('pressure', 69), ('heating', 66), ('density', 61), ('model', 58), ('cooling', 55), ('temperature', 52), ('gas', 46), ('front', 44)]","[(('cosmic', 'ray'), 70), (('number', 'density'), 31), (('magnetic', 'field'), 28), (('dx', 'dx'), 28), (('radiative', 'cooling'), 27), (('ray', 'heating'), 24), (('cr', 'heating'), 23), (('initial', 'gradient'), 18), (('civ', 'ovi'), 17), (('gas', 'pressure'), 15)]","[(('cosmic', 'ray', 'heating'), 22), (('dx', 'dx', 'dx'), 18), (('number', 'density', 'temperature'), 11), (('cosmic', 'ray', 'pressure'), 10), (('magnetic', 'field', 'strength'), 10), (('cosmic', 'ray', 'mediated'), 8), (('heating', 'radiative', 'cooling'), 8), (('ray', 'mediated', 'thermal'), 7), (('mediated', 'thermal', 'fronts'), 7), (('zhu', 'zweibel', 'gnedin'), 7)]"
