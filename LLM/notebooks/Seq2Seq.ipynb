{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import fitz\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "from cleantext import clean\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import json\n",
    "import TextAnalysis as TA\n",
    "from generate_config import load_model_config, get_generation_parameters, save_model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models to benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\".\")\n",
    "CONTENT_DIR = BASE_DIR / \"content\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "SUMMARIES_DIR = BASE_DIR / \"summaries\"\n",
    "MODEL_CONFIGS_DIR = BASE_DIR / \"model_configs\"  # New config directory\n",
    "DEPRECATED_CONFIGS_DIR = MODEL_CONFIGS_DIR / \"deprecated\"  # Directory for old configs\n",
    "\n",
    "# Verify directory structure exists\n",
    "if not CONTENT_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Content directory not found at {CONTENT_DIR}. Please create it and add PDF files.\")\n",
    "\n",
    "if not MODELS_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Models directory not found at {MODELS_DIR}. Please create it and add model directories.\")\n",
    "\n",
    "# Find all model directories\n",
    "MODELS_TO_BENCHMARK = [str(d) for d in MODELS_DIR.iterdir() if d.is_dir()]\n",
    "if not MODELS_TO_BENCHMARK:\n",
    "    raise FileNotFoundError(f\"No model directories found in {MODELS_DIR}\")\n",
    "\n",
    "logging.info(f\"Found {len(MODELS_TO_BENCHMARK)} models to benchmark: {[Path(m).name for m in MODELS_TO_BENCHMARK]}\")\n",
    "\n",
    "# Create summaries directory, model-specific subdirectories, and config directory\n",
    "SUMMARIES_DIR.mkdir(exist_ok=True)\n",
    "MODEL_CONFIGS_DIR.mkdir(exist_ok=True)\n",
    "DEPRECATED_CONFIGS_DIR.mkdir(exist_ok=True)  # Create deprecated directory\n",
    "for model_path in MODELS_TO_BENCHMARK:\n",
    "    model_name = Path(model_path).name\n",
    "    (SUMMARIES_DIR / model_name).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(filepath='stopwords.txt'):\n",
    "    \"\"\"\n",
    "    Load stopwords from file or create default if doesn't exist.\n",
    "    Returns set of stopwords.\n",
    "    \"\"\"\n",
    "    default_stopwords = {\n",
    "        'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',\n",
    "        'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',\n",
    "        'to', 'was', 'were', 'will', 'with', 'the', 'this', 'but', 'they',\n",
    "        'have', 'had', 'what', 'when', 'where', 'who', 'which', 'why', 'how'\n",
    "    }\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        logging.info(f\"Stopwords file not found at {filepath}. Creating default stopwords file.\")\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(sorted(default_stopwords)))\n",
    "        return default_stopwords\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        stopwords = {word.strip() for word in f.readlines() if word.strip()}\n",
    "    \n",
    "    logging.info(f\"Loaded {len(stopwords)} stopwords from {filepath}\")\n",
    "    return stopwords\n",
    "\n",
    "# Create/load stopwords\n",
    "stopwords = load_stopwords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \"\"\"Applies a series of cleaning steps to the text.\"\"\"\n",
    "    keywords = [\"REFERENCES\", \"ACKNOWLEDGEMENTS\", \"References\", \"Acknowledgements\"]\n",
    "    \n",
    "    text = find_and_remove_references(text, keywords)\n",
    "    text = remove_text_in_brackets(text)\n",
    "    text = remove_lines_starting_with_number_or_symbol(text)\n",
    "    text = remove_lines_with_one_word(text)\n",
    "    text = remove_empty_lines(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def find_and_remove_references(text, keywords):\n",
    "    \"\"\"Finds and removes text after the first occurrence of any of the given keywords.\"\"\"\n",
    "    earliest_position = float('inf')\n",
    "    for keyword in keywords:\n",
    "        position = text.find(keyword)\n",
    "        if position != -1:\n",
    "            earliest_position = min(position, earliest_position)\n",
    "    \n",
    "    if earliest_position != float('inf'):\n",
    "        text = text[:earliest_position]\n",
    "    return text\n",
    "\n",
    "def remove_text_in_brackets(text):\n",
    "    \"\"\"Removes text enclosed in parentheses or square brackets.\"\"\"\n",
    "    pattern = r'\\([^)]*\\)|\\[[^\\]]*\\]'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def remove_lines_starting_with_number_or_symbol(text):\n",
    "    \"\"\"Removes lines starting with a number or symbol.\"\"\"\n",
    "    pattern = r'^[\\d\\W].*$'\n",
    "    return re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "\n",
    "def remove_lines_with_one_word(text):\n",
    "    \"\"\"Removes lines containing only one word.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    pattern = r'^\\s*\\w+\\s*$'\n",
    "    filtered_lines = [line for line in lines if not re.match(pattern, line)]\n",
    "    return '\\n'.join(filtered_lines)\n",
    "\n",
    "def remove_empty_lines(text):\n",
    "    \"\"\"Removes empty lines.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    non_empty_lines = [line for line in lines if line.strip() != '']\n",
    "    return '\\n'.join(non_empty_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_process_text(pdf_path, output_path):\n",
    "    \"\"\"Extracts text from a PDF file, cleans it, and saves it to a text file.\"\"\"\n",
    "    pdf_doc = fitz.open(pdf_path)\n",
    "    text = ''\n",
    "    for page in pdf_doc:\n",
    "        text += page.get_text()\n",
    "\n",
    "    cleaned_text = process_text(text)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(cleaned_text)\n",
    "    return output_path\n",
    "\n",
    "def process_pdf(pdf_filename):\n",
    "    \"\"\"Processes a single PDF file, extracts and cleans text.\"\"\"\n",
    "    text_filename = pdf_filename.with_suffix('.txt')  # Use with_suffix to change the extension\n",
    "    cleaned_text = extract_and_process_text(pdf_filename, text_filename)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summaries(summaries, output_dir, filename):\n",
    "    \"\"\"\n",
    "    Save summaries to a JSON file in the specified directory and return the summaries text.\n",
    "    \n",
    "    Args:\n",
    "        summaries: List of summary dictionaries\n",
    "        output_dir: Path to output directory\n",
    "        filename: Name of the original file\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (output_path, concatenated_summaries_text)\n",
    "    \"\"\"\n",
    "    output_path = output_dir / f\"{Path(filename).stem}_summary.json\"\n",
    "    \n",
    "    # Convert torch tensors to lists for JSON serialization\n",
    "    serializable_summaries = []\n",
    "    concatenated_summaries = []\n",
    "    \n",
    "    for summary in summaries:\n",
    "        serializable_summary = {\n",
    "            \"input_tokens\": summary[\"input_tokens\"].tolist(),\n",
    "            \"summary\": summary[\"summary\"]\n",
    "        }\n",
    "        serializable_summaries.append(serializable_summary)\n",
    "        concatenated_summaries.append(summary[\"summary\"][0])  # Extract the summary text\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_summaries, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Join all summaries with a separator\n",
    "    full_summary = \" \".join(concatenated_summaries)\n",
    "    return output_path, full_summary\n",
    "\n",
    "def summarize_with_model(model_path, text_filenames, device, settings):\n",
    "    \"\"\"\n",
    "    Generate summaries using a specific model.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the model\n",
    "        text_filenames: List of text files to summarize\n",
    "        device: torch device\n",
    "        settings: Dictionary of generation settings\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping filenames to summary texts\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading model from {model_path}\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    model_name = Path(model_path).name\n",
    "    output_dir = SUMMARIES_DIR / model_name\n",
    "    \n",
    "    summaries = {}\n",
    "    for file_path in text_filenames:\n",
    "        logging.info(f\"Processing {file_path} with model {model_name}\")\n",
    "        \n",
    "        with open(file_path, 'r', errors='ignore') as f:\n",
    "            raw_text = f.read()\n",
    "        \n",
    "        clean_text = clean(raw_text)\n",
    "        _summaries = summarize_via_tokenbatches(\n",
    "            clean_text, model, tokenizer, device, settings['token_batch_length'], settings['batch_stride'], **settings['parameters']\n",
    "        )\n",
    "        \n",
    "        # Save summaries and get the concatenated text\n",
    "        _, summary_text = save_summaries(_summaries, output_dir, file_path)\n",
    "        # Store with the original PDF filename as key\n",
    "        pdf_name = Path(file_path).stem + '.pdf'\n",
    "        summaries[pdf_name] = summary_text\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "def summarize(ids, mask, model, tokenizer, device, **kwargs):\n",
    "    \"\"\"Generate summary using the model.\"\"\"\n",
    "    ids = ids[None, :]\n",
    "    mask = mask[None, :]\n",
    "\n",
    "    input_ids = ids.to(device)\n",
    "    attention_mask = mask.to(device)\n",
    "    global_attention_mask = torch.zeros_like(attention_mask)\n",
    "    global_attention_mask[:, 0] = 1\n",
    "\n",
    "    summary_pred_ids = model.generate(\n",
    "        ids,\n",
    "        attention_mask=mask,\n",
    "        global_attention_mask=global_attention_mask,\n",
    "        return_dict_in_generate=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    summary = tokenizer.batch_decode(\n",
    "        summary_pred_ids.sequences,\n",
    "        skip_special_tokens=True,\n",
    "        remove_invalid_values=True,\n",
    "    )\n",
    "\n",
    "    return summary\n",
    "\n",
    "def summarize_via_tokenbatches(input_text, model, tokenizer, device, token_batch_length, batch_stride, **kwargs):\n",
    "    \"\"\"Process text in batches for summarization.\"\"\"\n",
    "    encoded_input = tokenizer(\n",
    "        input_text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=token_batch_length,\n",
    "        stride=batch_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    in_id_arr = encoded_input.input_ids.to(device)\n",
    "    att_arr = encoded_input.attention_mask.to(device)\n",
    "    gen_summaries = []\n",
    "\n",
    "    with tqdm(total=len(in_id_arr)) as pbar:\n",
    "        for _id, _mask in zip(in_id_arr, att_arr):\n",
    "            result = summarize(ids=_id, mask=_mask, model=model, tokenizer=tokenizer, device=device, **kwargs)\n",
    "            _sum = {\"input_tokens\": _id, \"summary\": result}\n",
    "            gen_summaries.append(_sum)\n",
    "            pbar.update()\n",
    "\n",
    "    return gen_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_summaries(df, stopwords):\n",
    "    \"\"\"Calculate n-grams for summaries.\"\"\"\n",
    "    for _, row in df.iterrows():\n",
    "        summary = row.get('Summary', '')\n",
    "        if pd.isna(summary) or summary == '':\n",
    "            empty_result = [('', 0)] * 10\n",
    "            results = {\n",
    "                'Top_Words': empty_result,\n",
    "                'Top_Bigrams': empty_result,\n",
    "                'Top_Trigrams': empty_result\n",
    "            }\n",
    "        else:\n",
    "            results = {\n",
    "                'Top_Words': TA.topwords(summary, stopwords),\n",
    "                'Top_Bigrams': TA.topbigrams(summary, stopwords),\n",
    "                'Top_Trigrams': TA.toptrigrams(summary, stopwords)\n",
    "            }\n",
    "        \n",
    "        for key, value in results.items():\n",
    "            df.at[_, key] = value\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Process PDF files to .txt\n",
    "    logging.info(\"Processing PDF files...\")\n",
    "    pdf_filenames = list(CONTENT_DIR.glob('*.pdf'))\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        executor.map(process_pdf, pdf_filenames)\n",
    "\n",
    "    # Initialize list to store DataFrames\n",
    "    all_model_summaries_dfs = []\n",
    "\n",
    "    # Initialize dictionary to store model names\n",
    "    all_model_names = {}\n",
    "\n",
    "    # Iterate through models and generate summaries\n",
    "    for model_path in MODELS_TO_BENCHMARK:\n",
    "        model_name = Path(model_path).name\n",
    "        model_config_path = MODEL_CONFIGS_DIR / f\"{model_name}_config.json\"\n",
    "        \n",
    "        # Load config if it exists, generate a new one if it doesn't\n",
    "        if model_config_path.exists():\n",
    "            model_config = load_model_config(model_config_path)\n",
    "        else:\n",
    "            save_model_config(model_path, model_name)\n",
    "            model_config = load_model_config(model_config_path)\n",
    "        \n",
    "        logging.info(f\"Starting summarization with model: {model_path}\")\n",
    "        model_summaries = summarize_with_model(\n",
    "            model_path,\n",
    "            list(CONTENT_DIR.glob('*.txt')),\n",
    "            device,\n",
    "            model_config\n",
    "        )\n",
    "        \n",
    "        # Create DataFrame for current model's summaries\n",
    "        model_df = pd.DataFrame({\n",
    "            'Document': [Path(f).name for f in pdf_filenames],\n",
    "            'Model': [model_name] * len(pdf_filenames),\n",
    "            'Summary': [model_summaries[Path(f).name] for f in pdf_filenames]  # Use just the filename\n",
    "        })\n",
    "        \n",
    "        # Add model DataFrame to the list\n",
    "        all_model_summaries_dfs.append(model_df)\n",
    "        all_model_names[model_name] = model_config\n",
    "\n",
    "    # Concatenate DataFrames from all models\n",
    "    df = pd.concat(all_model_summaries_dfs, ignore_index=True)\n",
    "\n",
    "    # Create processing summary\n",
    "    summary_report = {\n",
    "        \"models_processed\": list(all_model_names.keys()),\n",
    "        \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(SUMMARIES_DIR / \"processing_summary.json\", 'w') as f:\n",
    "        json.dump(summary_report, f, indent=2)\n",
    "\n",
    "    process_summaries(df, \"stopwords.txt\")\n",
    "\n",
    "    # Save DataFrame to CSV file\n",
    "    df.to_csv(SUMMARIES_DIR / \"{model_name}_summaries.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
