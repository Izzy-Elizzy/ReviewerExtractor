{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ads  # Assumed to be the ADS library for astronomical data\n",
    "import operator\n",
    "import re\n",
    "import nltk\n",
    "from nltk import ngrams, word_tokenize, bigrams, trigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "import fnmatch\n",
    "import requests\n",
    "import io\n",
    "from PyPDF2 import PdfReader\n",
    "from TextAnalysis import stopword_loader, count_words, topwords, topbigrams, toptrigrams  # Custom module\n",
    "from typing import List, Union\n",
    "import logging\n",
    "\n",
    "\"\"\"\n",
    "Module: main_pipeline.py\n",
    "\n",
    "This module implements the main pipeline for processing a CSV file. It retrieves scientific papers based on \n",
    "arXiv identifiers from the CSV, generates summaries for each paper using either a seq2seq or causal language \n",
    "model (depending on configuration), concatenates the summaries for entries with multiple identifiers, performs \n",
    "n-gram analysis on the combined summaries, and writes the results to a new CSV file.  The summarization step \n",
    "relies on separate modules (`seq2seq.py` and `casualLM.py`).\n",
    "\"\"\"\n",
    "\n",
    "# Configure logging (good practice for larger applications)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Function: clean_text\n",
    "\n",
    "    Cleans and preprocesses the extracted text from a scientific paper.  Removes reference and acknowledgement \n",
    "    sections, text within brackets, and lines containing only single words or starting with numbers/symbols.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw text extracted from a scientific paper.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned and preprocessed text.\n",
    "    \"\"\"\n",
    "    logger.debug(\"Cleaning text...\")\n",
    "    keywords = [\"REFERENCES\", \"ACKNOWLEDGEMENTS\", \"References\", \"Acknowledgements\"]\n",
    "    for keyword in keywords:\n",
    "        pos = text.find(keyword)\n",
    "        if pos != -1:\n",
    "            text = text[:pos]\n",
    "    \n",
    "    text = re.sub(r'\\([^)]*\\)|\\[[^\\]]*\\]', '', text)  # Remove bracketed content\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = [\n",
    "        line.strip() \n",
    "        for line in lines \n",
    "        if line.strip() and not re.match(r'^\\s*\\w+\\s*$', line) and not re.match(r'^[\\d\\W].*$', line)\n",
    "    ]\n",
    "    return ' '.join(cleaned_lines)\n",
    "\n",
    "\n",
    "def get_arxiv_text(arxiv_id: str) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Function: get_arxiv_text\n",
    "\n",
    "    Downloads a PDF from arXiv given its ID, extracts the text, and cleans it using the clean_text function.\n",
    "    Handles potential errors during download and PDF processing.\n",
    "\n",
    "    Args:\n",
    "        arxiv_id (str): The arXiv ID (e.g., \"arXiv:1234.5678\").\n",
    "\n",
    "    Returns:\n",
    "        Union[str, None]: The cleaned text extracted from the PDF, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Downloading and processing arXiv ID: {arxiv_id}\")\n",
    "    arxiv_id = arxiv_id.split('arXiv:')[-1]\n",
    "    url = f'https://arxiv.org/pdf/{arxiv_id}.pdf'\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        pdf = PdfReader(io.BytesIO(response.content))\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        return clean_text(text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error downloading PDF {url}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing PDF {url}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_summary(arxiv_id: str) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Function: get_summary (Simplified)\n",
    "\n",
    "    This function is a placeholder. In a real application, this would likely call a summarization function, \n",
    "    potentially using a language model as in the previous examples.  For this example, it simply calls \n",
    "    `get_arxiv_text`.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        arxiv_id (str): The arXiv ID.\n",
    "\n",
    "    Returns:\n",
    "        Union[str, None]: The summary text, or None if an error occurs.  Currently just returns the raw text.\n",
    "    \"\"\"\n",
    "    return get_arxiv_text(arxiv_id)\n",
    "\n",
    "\n",
    "def process_csv(csv_filepath: str, stopwords_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function: process_csv\n",
    "\n",
    "    The main function that processes a CSV file, downloads papers, generates summaries (using a placeholder \n",
    "    function), concatenates the summaries, performs n-gram analysis, and adds the results as new columns to the \n",
    "    DataFrame.  Handles various error conditions.\n",
    "\n",
    "    Args:\n",
    "        csv_filepath (str): Path to the input CSV file.  Must contain an 'Identifier' column with a list of \n",
    "                            arXiv IDs in string format (e.g., \"['arXiv:1234.5678', 'arXiv:9876.5432']\").\n",
    "        stopwords_path (str): Path to the stopwords file for n-gram analysis.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The processed DataFrame with added columns for summaries and n-grams.  Returns None if \n",
    "                      errors occur during processing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filepath)\n",
    "        df['summaries'] = \"\"\n",
    "        df['topwords'] = \"\"\n",
    "        df['topbigrams'] = \"\"\n",
    "        df['toptrigrams'] = \"\"\n",
    "        stopwords = stopword_loader(stopwords_path) # Assumed function from TextAnalysis\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            arxiv_ids_str = row['Identifier']\n",
    "            try:\n",
    "                arxiv_ids = eval(arxiv_ids_str) # This is risky, use a safer method if possible\n",
    "                all_summaries = []\n",
    "                for arxiv_id in arxiv_ids:\n",
    "                    summary = get_summary(arxiv_id)\n",
    "                    if summary:\n",
    "                        all_summaries.append(summary)\n",
    "                    else:\n",
    "                        logger.warning(f\"Failed to get summary for {arxiv_id}\")\n",
    "\n",
    "                combined_summary = ' '.join(all_summaries)\n",
    "\n",
    "                if combined_summary:\n",
    "                    top10words = topwords(combined_summary, stopwords)\n",
    "                    top10bigrams = topbigrams(combined_summary, stopwords)\n",
    "                    top10trigrams = toptrigrams(combined_summary, stopwords)\n",
    "                    df.loc[index, ['topwords', 'topbigrams', 'toptrigrams', 'summaries']] = [top10words, top10bigrams, top10trigrams, combined_summary]\n",
    "                else:\n",
    "                    df.loc[index, ['summaries', 'topwords', 'topbigrams', 'toptrigrams']] = [None] * 4\n",
    "            except (SyntaxError, NameError, TypeError) as e:\n",
    "                logger.error(f\"Error processing row {index}: {e}\", exc_info=True)\n",
    "                df.loc[index, ['summaries', 'topwords', 'topbigrams', 'toptrigrams']] = [None] * 4\n",
    "\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"CSV file not found: {csv_filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "    csv_file = 'small_identifier_sample.csv'  # Replace with your CSV file\n",
    "    stopwords_file = 'stopwords.txt' # Path to your stopword file\n",
    "\n",
    "    processed_df = process_csv(csv_file, stopwords_file)\n",
    "\n",
    "    if processed_df is not None:\n",
    "        output_filename = 'combined_output.csv'\n",
    "        processed_df.to_csv(output_filename, index=False)\n",
    "        logger.info(f\"Results saved to {output_filename}\")\n",
    "    else:\n",
    "        logger.error(\"CSV processing failed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
