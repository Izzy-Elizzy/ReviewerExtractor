{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import fitz\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from cleantext import clean\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import json\n",
    "import TextAnalysis as TA\n",
    "from generate_config import load_model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models to benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\".\")\n",
    "CONTENT_DIR = BASE_DIR / \"content\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "SUMMARIES_DIR = BASE_DIR / \"summaries\"\n",
    "\n",
    "# Verify directory structure exists\n",
    "if not CONTENT_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Content directory not found at {CONTENT_DIR}. Please create it and add PDF files.\")\n",
    "\n",
    "if not MODELS_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Models directory not found at {MODELS_DIR}. Please create it and add model directories.\")\n",
    "\n",
    "# Find all model directories\n",
    "MODELS_TO_BENCHMARK = [str(d) for d in MODELS_DIR.iterdir() if d.is_dir()]\n",
    "if not MODELS_TO_BENCHMARK:\n",
    "    raise FileNotFoundError(f\"No model directories found in {MODELS_DIR}\")\n",
    "\n",
    "logging.info(f\"Found {len(MODELS_TO_BENCHMARK)} models to benchmark: {[Path(m).name for m in MODELS_TO_BENCHMARK]}\")\n",
    "\n",
    "# Create summaries directory and model-specific subdirectories\n",
    "SUMMARIES_DIR.mkdir(exist_ok=True)\n",
    "for model_path in MODELS_TO_BENCHMARK:\n",
    "    model_name = Path(model_path).name\n",
    "    (SUMMARIES_DIR / model_name).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(filepath='stopwords.txt'):\n",
    "    \"\"\"\n",
    "    Load stopwords from file or create default if doesn't exist.\n",
    "    Returns set of stopwords.\n",
    "    \"\"\"\n",
    "    default_stopwords = {\n",
    "        'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',\n",
    "        'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',\n",
    "        'to', 'was', 'were', 'will', 'with', 'the', 'this', 'but', 'they',\n",
    "        'have', 'had', 'what', 'when', 'where', 'who', 'which', 'why', 'how'\n",
    "    }\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        logging.info(f\"Stopwords file not found at {filepath}. Creating default stopwords file.\")\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(sorted(default_stopwords)))\n",
    "        return default_stopwords\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        stopwords = {word.strip() for word in f.readlines() if word.strip()}\n",
    "    \n",
    "    logging.info(f\"Loaded {len(stopwords)} stopwords from {filepath}\")\n",
    "    return stopwords\n",
    "\n",
    "# Create/load stopwords\n",
    "stopwords = load_stopwords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \"\"\"Applies a series of cleaning steps to the text.\"\"\"\n",
    "    keywords = [\"REFERENCES\", \"ACKNOWLEDGEMENTS\", \"References\", \"Acknowledgements\"]\n",
    "    \n",
    "    text = find_and_remove_references(text, keywords)\n",
    "    text = remove_text_in_brackets(text)\n",
    "    text = remove_lines_starting_with_number_or_symbol(text)\n",
    "    text = remove_lines_with_one_word(text)\n",
    "    text = remove_empty_lines(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def find_and_remove_references(text, keywords):\n",
    "    \"\"\"Finds and removes text after the first occurrence of any of the given keywords.\"\"\"\n",
    "    earliest_position = float('inf')\n",
    "    for keyword in keywords:\n",
    "        position = text.find(keyword)\n",
    "        if position != -1:\n",
    "            earliest_position = min(position, earliest_position)\n",
    "    \n",
    "    if earliest_position != float('inf'):\n",
    "        text = text[:earliest_position]\n",
    "    return text\n",
    "\n",
    "def remove_text_in_brackets(text):\n",
    "    \"\"\"Removes text enclosed in parentheses or square brackets.\"\"\"\n",
    "    pattern = r'\\([^)]*\\)|\\[[^\\]]*\\]'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def remove_lines_starting_with_number_or_symbol(text):\n",
    "    \"\"\"Removes lines starting with a number or symbol.\"\"\"\n",
    "    pattern = r'^[\\d\\W].*$'\n",
    "    return re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "\n",
    "def remove_lines_with_one_word(text):\n",
    "    \"\"\"Removes lines containing only one word.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    pattern = r'^\\s*\\w+\\s*$'\n",
    "    filtered_lines = [line for line in lines if not re.match(pattern, line)]\n",
    "    return '\\n'.join(filtered_lines)\n",
    "\n",
    "def remove_empty_lines(text):\n",
    "    \"\"\"Removes empty lines.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    non_empty_lines = [line for line in lines if line.strip() != '']\n",
    "    return '\\n'.join(non_empty_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_process_text(pdf_path, output_path):\n",
    "    \"\"\"Extracts text from a PDF file, cleans it, and saves it to a text file.\"\"\"\n",
    "    pdf_doc = fitz.open(pdf_path)\n",
    "    text = ''\n",
    "    for page in pdf_doc:\n",
    "        text += page.get_text()\n",
    "\n",
    "    cleaned_text = process_text(text)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(cleaned_text)\n",
    "    return output_path\n",
    "\n",
    "def process_pdf(pdf_filename):\n",
    "    \"\"\"Processes a single PDF file, extracts and cleans text.\"\"\"\n",
    "    text_filename = pdf_filename.with_suffix('.txt')  # Use with_suffix to change the extension\n",
    "    cleaned_text = extract_and_process_text(pdf_filename, text_filename)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summaries(summaries, output_dir, filename):\n",
    "    \"\"\"\n",
    "    Save summaries to a JSON file in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        summaries: List of summary dictionaries\n",
    "        output_dir: Path to output directory\n",
    "        filename: Name of the original file\n",
    "    \"\"\"\n",
    "    output_path = output_dir / f\"{Path(filename).stem}_summary.json\"\n",
    "    \n",
    "    # Convert torch tensors to lists for JSON serialization\n",
    "    serializable_summaries = []\n",
    "    for summary in summaries:\n",
    "        serializable_summary = {\n",
    "            \"input_tokens\": summary[\"input_tokens\"].tolist(),\n",
    "            \"summary\": summary[\"summary\"]\n",
    "        }\n",
    "        serializable_summaries.append(serializable_summary)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_summaries, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def summarize_with_model(model_path, text_filenames, device, token_batch_length, batch_stride, settings):\n",
    "    \"\"\"\n",
    "    Generate summaries using a specific model.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the model\n",
    "        text_filenames: List of text files to summarize\n",
    "        device: torch device\n",
    "        token_batch_length: Maximum token length for processing\n",
    "        batch_stride: Stride length for processing\n",
    "        settings: Dictionary of generation settings\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping filenames to summaries\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading model from {model_path}\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    model_name = Path(model_path).name\n",
    "    output_dir = SUMMARIES_DIR / model_name\n",
    "    \n",
    "    summaries = {}\n",
    "    for file_path in text_filenames:\n",
    "        logging.info(f\"Processing {file_path} with model {model_name}\")\n",
    "        \n",
    "        with open(file_path, 'r', errors='ignore') as f:\n",
    "            raw_text = f.read()\n",
    "        \n",
    "        clean_text = clean(raw_text)\n",
    "        _summaries = summarize_via_tokenbatches(\n",
    "            clean_text, model, tokenizer, device, \n",
    "            token_batch_length, batch_stride, **settings\n",
    "        )\n",
    "        \n",
    "        # Save summaries for this file\n",
    "        summary_path = save_summaries(_summaries, output_dir, file_path)\n",
    "        summaries[file_path] = summary_path\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "def summarize(ids, mask, model, tokenizer, device, **kwargs):\n",
    "    \"\"\"Generate summary using the model.\"\"\"\n",
    "    ids = ids[None, :]\n",
    "    mask = mask[None, :]\n",
    "\n",
    "    input_ids = ids.to(device)\n",
    "    attention_mask = mask.to(device)\n",
    "    global_attention_mask = torch.zeros_like(attention_mask)\n",
    "    global_attention_mask[:, 0] = 1\n",
    "\n",
    "    summary_pred_ids = model.generate(\n",
    "        ids,\n",
    "        attention_mask=mask,\n",
    "        global_attention_mask=global_attention_mask,\n",
    "        return_dict_in_generate=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    summary = tokenizer.batch_decode(\n",
    "        summary_pred_ids.sequences,\n",
    "        skip_special_tokens=True,\n",
    "        remove_invalid_values=True,\n",
    "    )\n",
    "\n",
    "    return summary\n",
    "\n",
    "def summarize_via_tokenbatches(input_text, model, tokenizer, device, token_batch_length, batch_stride, **kwargs):\n",
    "    \"\"\"Process text in batches for summarization.\"\"\"\n",
    "    encoded_input = tokenizer(\n",
    "        input_text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=token_batch_length,\n",
    "        stride=batch_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    in_id_arr = encoded_input.input_ids.to(device)\n",
    "    att_arr = encoded_input.attention_mask.to(device)\n",
    "    gen_summaries = []\n",
    "\n",
    "    with tqdm(total=len(in_id_arr)) as pbar:\n",
    "        for _id, _mask in zip(in_id_arr, att_arr):\n",
    "            result = summarize(ids=_id, mask=_mask, model=model, tokenizer=tokenizer, device=device, **kwargs)\n",
    "            _sum = {\"input_tokens\": _id, \"summary\": result}\n",
    "            gen_summaries.append(_sum)\n",
    "            pbar.update()\n",
    "\n",
    "    return gen_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_summaries(df, stopwords_path):\n",
    "    \"\"\"Process summaries and calculate n-grams using TextAnalysis module.\"\"\"\n",
    "    # Load stopwords at the start of processing\n",
    "    stopwords = load_stopwords(stopwords_path)\n",
    "    \n",
    "    results = {\n",
    "        'Summaries Top 10 Words': [],\n",
    "        'Summaries Top 10 Bigrams': [],\n",
    "        'Summaries Top 10 Trigrams': []\n",
    "    }\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        summaries = row.get('Summaries', '')\n",
    "        if pd.isna(summaries) or summaries == '':\n",
    "            empty_result = [('', 0)] * 10\n",
    "            for key in results:\n",
    "                results[key].append(empty_result)\n",
    "            continue\n",
    "            \n",
    "        results['Summaries Top 10 Words'].append(TA.topwords(summaries, stopwords))\n",
    "        results['Summaries Top 10 Bigrams'].append(TA.topbigrams(summaries, stopwords))\n",
    "        results['Summaries Top 10 Trigrams'].append(TA.toptrigrams(summaries, stopwords))\n",
    "    \n",
    "    # Add results to DataFrame\n",
    "    for key in results:\n",
    "        df[key] = results[key]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load configurations for each model\n",
    "    all_model_summaries = {}\n",
    "    for model_path in MODELS_TO_BENCHMARK:\n",
    "        model_config = load_model_config(model_path)\n",
    "        if not model_config:\n",
    "            logging.warning(f\"No configuration found for {model_path}, using default settings\")\n",
    "            settings = {\n",
    "                'min_length': 5,\n",
    "                'max_length': int(3072 // 8),\n",
    "                'no_repeat_ngram_size': 7,\n",
    "                'encoder_no_repeat_ngram_size': 7,\n",
    "                'repetition_penalty': 3.7,\n",
    "                'num_beams': 12,\n",
    "                'length_penalty': 0.5,\n",
    "                'early_stopping': True,\n",
    "                'do_sample': False\n",
    "            }\n",
    "        else:\n",
    "            settings = model_config['parameters']\n",
    "        \n",
    "        logging.info(f\"Starting summarization with model: {model_path}\")\n",
    "        model_summaries = summarize_with_model(\n",
    "            model_path,\n",
    "            text_filenames,\n",
    "            device,\n",
    "            settings\n",
    "        )\n",
    "        all_model_summaries[Path(model_path).name] = model_summaries\n",
    "    \n",
    "    # Create processing summary\n",
    "    summary_report = {\n",
    "        \"models_processed\": list(all_model_summaries.keys()),\n",
    "        \"files_processed\": len(text_filenames),\n",
    "        \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(SUMMARIES_DIR / \"processing_summary.json\", 'w') as f:\n",
    "        json.dump(summary_report, f, indent=2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depracted Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import concurrent.futures\n",
    "# import fitz\n",
    "# import logging\n",
    "# from pathlib import Path\n",
    "\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from cleantext import clean\n",
    "# # from datasets import load_dataset\n",
    "# # from nltk.corpus import stopwords\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# from tqdm.auto import tqdm\n",
    "# import re\n",
    "\n",
    "# # Assuming you have a TextAnalysis module\n",
    "# import TextAnalysis as TA\n",
    "\n",
    "# # Configure logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# # --- Helper Functions ---\n",
    "\n",
    "# def load_stopwords(filepath='stopwords.txt'):\n",
    "#     \"\"\"Load stopwords from local file.\"\"\"\n",
    "#     try:\n",
    "#         with open(filepath, 'r', encoding='utf-8') as f:\n",
    "#             return {line.strip() for line in f}\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Warning: {filepath} not found. Using empty stopwords set.\")\n",
    "#         return set()\n",
    "\n",
    "\n",
    "# def remove_text_in_brackets(text):\n",
    "#   \"\"\"Removes text enclosed in parentheses or square brackets.\"\"\"\n",
    "#   pattern = r'\\([^)]*\\)|\\[[^\\]]*\\]'\n",
    "#   return re.sub(pattern, '', text)\n",
    "\n",
    "# def remove_lines_starting_with_number_or_symbol(text):\n",
    "#   \"\"\"Removes lines starting with a number or symbol.\"\"\"\n",
    "#   pattern = r'^[\\d\\W].*$'\n",
    "#   return re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "\n",
    "# def remove_lines_with_one_word(text):\n",
    "#   \"\"\"Removes lines containing only one word.\"\"\"\n",
    "#   lines = text.split('\\n')\n",
    "#   pattern = r'^\\s*\\w+\\s*$'\n",
    "#   filtered_lines = [line for line in lines if not re.match(pattern, line)]\n",
    "#   return '\\n'.join(filtered_lines)\n",
    "\n",
    "# def remove_empty_lines(text):\n",
    "#   \"\"\"Removes empty lines.\"\"\"\n",
    "#   lines = text.split('\\n')\n",
    "#   non_empty_lines = [line for line in lines if line.strip() != '']\n",
    "#   return '\\n'.join(non_empty_lines)\n",
    "\n",
    "# def find_and_remove_references(text, keywords):\n",
    "#   \"\"\"Finds and removes text after the first occurrence of any of the given keywords.\"\"\"\n",
    "#   earliest_position = -1\n",
    "#   for keyword in keywords:\n",
    "#     position = text.find(keyword)\n",
    "#     if position != -1:\n",
    "#       earliest_position = position if earliest_position == -1 else min(position, earliest_position)\n",
    "#   if earliest_position != -1:\n",
    "#     text = text[:earliest_position]\n",
    "#   return text\n",
    "\n",
    "# def process_text(text):\n",
    "#   \"\"\"Applies a series of cleaning steps to the text.\"\"\"\n",
    "#   keywords = [\"REFERENCES\", \"ACKNOWLEDGEMENTS\", \"References\", \"Acknowledgements\"]\n",
    "\n",
    "#   text = find_and_remove_references(text, keywords)\n",
    "#   text = find_and_remove_references(text, keywords)\n",
    "#   text = remove_text_in_brackets(text)\n",
    "#   text = remove_lines_starting_with_number_or_symbol(text)\n",
    "#   text = remove_lines_with_one_word(text)\n",
    "#   text = remove_empty_lines(text)\n",
    "\n",
    "#   return text\n",
    "\n",
    "# def extract_and_process_text(pdf_path, output_path):\n",
    "#   \"\"\"Extracts text from a PDF file, cleans it, and saves it to a text file.\"\"\"\n",
    "#   pdf_doc = fitz.open(pdf_path)\n",
    "#   text = ''\n",
    "#   for page in pdf_doc:\n",
    "#     text += page.get_text()\n",
    "\n",
    "#   lines = text.split('\\n')\n",
    "#   filtered_lines = [line for line in lines if len(line.strip()) > 1]\n",
    "#   filtered_text = '\\n'.join(filtered_lines)\n",
    "#   cleaned_text = process_text(filtered_text)\n",
    "\n",
    "#   with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "#     output_file.write(cleaned_text)\n",
    "#   return output_path\n",
    "\n",
    "# def process_pdf(pdf_filename):\n",
    "#   \"\"\"Processes a single PDF file, extracts and cleans text, and returns the text filename.\"\"\"\n",
    "#   text_filename = pdf_filename.replace(\".pdf\", \".txt\")\n",
    "#   cleaned_text = extract_and_process_text(pdf_filename, text_filename)\n",
    "#   return cleaned_text\n",
    "\n",
    "\n",
    "# def summarize(ids, mask, **kwargs):\n",
    "#     ids = ids[None, :]\n",
    "#     mask = mask[None, :]\n",
    "\n",
    "#     input_ids = ids.to(_device)\n",
    "#     attention_mask = mask.to(_device)\n",
    "#     global_attention_mask = torch.zeros_like(attention_mask)\n",
    "#     global_attention_mask[:, 0] = 1\n",
    "\n",
    "#     summary_pred_ids = model.generate(\n",
    "#         ids,\n",
    "#         attention_mask=mask,\n",
    "#         global_attention_mask=global_attention_mask,\n",
    "#         return_dict_in_generate=True,\n",
    "#         **kwargs\n",
    "#     )\n",
    "#     summary = tokenizer.batch_decode(\n",
    "#         summary_pred_ids.sequences,\n",
    "#         skip_special_tokens=True,\n",
    "#         remove_invalid_values=True,\n",
    "#     )\n",
    "\n",
    "#     return summary\n",
    "\n",
    "# def summarize_via_tokenbatches(input_text, **kwargs):\n",
    "#     encoded_input = tokenizer(\n",
    "#         input_text,\n",
    "#         padding='max_length',\n",
    "#         truncation=True,\n",
    "#         max_length=token_batch_length,\n",
    "#         stride=batch_stride,\n",
    "#         return_overflowing_tokens=True,\n",
    "#         add_special_tokens=False,\n",
    "#         return_tensors='pt',\n",
    "#     )\n",
    "\n",
    "#     in_id_arr, att_arr = encoded_input.input_ids.to(_device), encoded_input.attention_mask.to(_device)\n",
    "#     gen_summaries = []\n",
    "\n",
    "#     with tqdm(total=len(in_id_arr)) as pbar:\n",
    "#         for _id, _mask in zip(in_id_arr, att_arr):\n",
    "#             result = summarize(ids=_id, mask=_mask, **kwargs)\n",
    "#             _sum = {\"input_tokens\": _id, \"summary\": result}\n",
    "#             gen_summaries.append(_sum)\n",
    "#             pbar.update()\n",
    "\n",
    "#     return gen_summaries\n",
    "\n",
    "# # --- Combine Summaries for Given Author ---\n",
    "\n",
    "# # Assuming you have a DataFrame named 'datf'\n",
    "# def combine_summaries(datf):\n",
    "#   for index, row in datf.iterrows():\n",
    "#     identifiers = row['arxiv-id'].split(',')\n",
    "#     concat_summ = \"\"\n",
    "\n",
    "#     for identifier in identifiers:\n",
    "#       if identifier.strip() == 'None':\n",
    "#         concat_summ = 'None'\n",
    "#         break\n",
    "\n",
    "#       sanitized_identifier = identifier.strip().replace('/', '_')\n",
    "#       filename = f'SUM_{sanitized_identifier}.txt'  # Assuming summaries are saved as SUM_*.txt\n",
    "\n",
    "#       if os.path.exists(filename):\n",
    "#         with open(filename, 'r', encoding='utf-8') as file:\n",
    "#           concat_summ += file.read()\n",
    "#       else:\n",
    "#         concat_summ = 'File not found'\n",
    "\n",
    "#     datf.at[index, 'Summaries'] = concat_summ\n",
    "\n",
    "# # --- N-grams ---\n",
    "\n",
    "# def sum_n_grams(df, directorypath):\n",
    "#     top10Dict = {'Summaries Top 10 Words': [],\n",
    "#                  'Summaries Top 10 Bigrams': [],\n",
    "#                  'Summaries Top 10 Trigrams': []}\n",
    "\n",
    "#     for i in df.values:\n",
    "#         summaries = i[15]  # Assuming 'Summaries' is the column name for your combined summaries\n",
    "\n",
    "#         # Now use functions from TextAnalysis\n",
    "#         top10words = TA.topwords(summaries, directorypath)\n",
    "#         top10bigrams = TA.topbigrams(summaries, directorypath)\n",
    "#         top10trigrams = TA.toptrigrams(summaries, directorypath)\n",
    "\n",
    "#         top10Dict['Summaries Top 10 Words'].append(top10words)\n",
    "#         top10Dict['Summaries Top 10 Bigrams'].append(top10bigrams)\n",
    "#         top10Dict['Summaries Top 10 Trigrams'].append(top10trigrams)\n",
    "\n",
    "#     sum_top10Df = df\n",
    "#     sum_top10Df['Summaries Top 10 Words'] = top10Dict['Summaries Top 10 Words']\n",
    "#     sum_top10Df['Summaries Top 10 Bigrams'] = top10Dict['Summaries Top 10 Bigrams']\n",
    "#     sum_top10Df['Summaries Top 10 Trigrams'] = top10Dict['Summaries Top 10 Trigrams']\n",
    "\n",
    "#     sum_top10Df = sum_top10Df[['Input Author', 'Input Institution', 'First Author', 'Bibcode', 'Title', 'Publication Date',\n",
    "#              'Keywords', 'Affiliations', 'Abstract', 'Top 10 Words', 'Top 10 Bigrams', 'Top 10 Trigrams', 'Data Type',\n",
    "#             'arxiv-id', 'Summaries', 'Summaries Top 10 Words', 'Summaries Top 10 Bigrams', 'Summaries Top 10 Trigrams']]\n",
    "\n",
    "#     return sum_top10Df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- PDF Processing ---\n",
    "\n",
    "# # Specify the directory containing your PDF files\n",
    "# content_directory = \".\\content\"\n",
    "\n",
    "# # Find all PDF files in the directory\n",
    "# pdf_filenames = [os.path.join(content_directory, file) for file in os.listdir(content_directory) if file.endswith('.pdf')]\n",
    "\n",
    "# # Process the PDF files in parallel\n",
    "# text_filenames = []\n",
    "# num_workers = 100\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "#   results = list(executor.map(process_pdf, pdf_filenames))\n",
    "\n",
    "# text_filenames.extend(result for result in results if result is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameteres for Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Summarization Model ---\n",
    "# #Set your User ID\n",
    "# username = \"ielhaime\"\n",
    "\n",
    "# hf_tag = \"pszemraj/led-large-book-summary\"\n",
    "# _device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(f\"./model\").to(_device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(f\"./model\")\n",
    "# token_batch_length = 3072\n",
    "# batch_stride = 50\n",
    "# max_len_ratio = 8\n",
    "\n",
    "# settings = {\n",
    "#     'min_length': 5,\n",
    "#     'max_length': int(token_batch_length // max_len_ratio),\n",
    "#     'no_repeat_ngram_size': 7,\n",
    "#     'encoder_no_repeat_ngram_size': 7,\n",
    "#     'repetition_penalty': 3.7,\n",
    "#     'num_beams': 12,\n",
    "#     'length_penalty': 0.5,\n",
    "#     'early_stopping': True,\n",
    "#     'do_sample': False\n",
    "# }\n",
    "\n",
    "# logging.info(f\"using textgen params:\\n\\n:{settings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Summarization ---\n",
    "\n",
    "# directory = './content'\n",
    "# text_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.txt')]\n",
    "\n",
    "# # Summarize each text file and store in a dictionary\n",
    "# summaries = {} \n",
    "# for file_path in text_files:\n",
    "#   with open(file_path, 'r', errors='ignore') as f:\n",
    "#     raw_text = f.read()\n",
    "\n",
    "#   long_text = clean(raw_text)\n",
    "#   logging.info(f\"removed {len(long_text) - len(raw_text)} chars via cleaning\")\n",
    "\n",
    "#   _summaries = summarize_via_tokenbatches(long_text, **settings)\n",
    "#   summaries[file_path] = _summaries  # Store summaries by filename\n",
    "\n",
    "#   if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Summaries and N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Main Execution ---\n",
    "\n",
    "# summaries = {}\n",
    "\n",
    "# datf = pd.read_csv('output.csv')\n",
    "\n",
    "# stop_dir = \"stopwords.txt\"\n",
    "\n",
    "# sample_text = summaries\n",
    "\n",
    "# sample_ngrams = {\n",
    "#     'top_words': TA.topwords(sample_text, stop_dir),\n",
    "#     'top_bigrams': TA.topbigrams(sample_text, stop_dir),\n",
    "#     'top_trigrams': TA.toptrigrams(sample_text, stop_dir)\n",
    "# }\n",
    "\n",
    "# # Add columns to DataFrame with the same summary and n-grams for all rows\n",
    "# datf['Summary'] = sample_text\n",
    "# datf['Top_Words'] = [sample_ngrams['top_words']] * len(datf)\n",
    "# datf['Top_Bigrams'] = [sample_ngrams['top_bigrams']] * len(datf)\n",
    "# datf['Top_Trigrams'] = [sample_ngrams['top_trigrams']] * len(datf)\n",
    "\n",
    "# # Save the updated DataFrame\n",
    "# datf.to_csv('Results.csv', index=False)\n",
    "\n",
    "\n",
    "# # # summaries = {}\n",
    "\n",
    "# # arVix Broken (Uncomment when its up again. For now, make do with the above code. Its will output the summary and the N-Grams )\n",
    "\n",
    "# # # Load your DataFrame\n",
    "# # datf = pd.read_csv('output.csv')  # Replace 'output.csv' with your actual file\n",
    "\n",
    "# # # Combine summaries\n",
    "# # combine_summaries(datf)\n",
    "\n",
    "# # # Calculate N-grams and update DataFrame\n",
    "# # path_stop = '/content/'\n",
    "# # stop_file = 'stopwords.txt'\n",
    "# # stop_dir = path_stop + stop_file\n",
    "# # final_df = sum_n_grams(datf, stop_dir)\n",
    "\n",
    "# # # Save the updated DataFrame\n",
    "# # final_df.to_csv('Results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
