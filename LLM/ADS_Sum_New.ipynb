{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import fitz\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from cleantext import clean\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import json\n",
    "import TextAnalysis as TA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models to benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Found 1 models to benchmark: ['large-book_summary']\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path(\".\")\n",
    "CONTENT_DIR = BASE_DIR / \"content\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "SUMMARIES_DIR = BASE_DIR / \"summaries\"\n",
    "\n",
    "# Verify directory structure exists\n",
    "if not CONTENT_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Content directory not found at {CONTENT_DIR}. Please create it and add PDF files.\")\n",
    "\n",
    "if not MODELS_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Models directory not found at {MODELS_DIR}. Please create it and add model directories.\")\n",
    "\n",
    "# Find all model directories\n",
    "MODELS_TO_BENCHMARK = [str(d) for d in MODELS_DIR.iterdir() if d.is_dir()]\n",
    "if not MODELS_TO_BENCHMARK:\n",
    "    raise FileNotFoundError(f\"No model directories found in {MODELS_DIR}\")\n",
    "\n",
    "logging.info(f\"Found {len(MODELS_TO_BENCHMARK)} models to benchmark: {[Path(m).name for m in MODELS_TO_BENCHMARK]}\")\n",
    "\n",
    "# Create summaries directory and model-specific subdirectories\n",
    "SUMMARIES_DIR.mkdir(exist_ok=True)\n",
    "for model_path in MODELS_TO_BENCHMARK:\n",
    "    model_name = Path(model_path).name\n",
    "    (SUMMARIES_DIR / model_name).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded 583 stopwords from stopwords.txt\n"
     ]
    }
   ],
   "source": [
    "def load_stopwords(filepath='stopwords.txt'):\n",
    "    \"\"\"\n",
    "    Load stopwords from file or create default if doesn't exist.\n",
    "    Returns set of stopwords.\n",
    "    \"\"\"\n",
    "    default_stopwords = {\n",
    "        'a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'for', 'from',\n",
    "        'has', 'he', 'in', 'is', 'it', 'its', 'of', 'on', 'that', 'the',\n",
    "        'to', 'was', 'were', 'will', 'with', 'the', 'this', 'but', 'they',\n",
    "        'have', 'had', 'what', 'when', 'where', 'who', 'which', 'why', 'how'\n",
    "    }\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        logging.info(f\"Stopwords file not found at {filepath}. Creating default stopwords file.\")\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(sorted(default_stopwords)))\n",
    "        return default_stopwords\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        stopwords = {word.strip() for word in f.readlines() if word.strip()}\n",
    "    \n",
    "    logging.info(f\"Loaded {len(stopwords)} stopwords from {filepath}\")\n",
    "    return stopwords\n",
    "\n",
    "# Create/load stopwords\n",
    "stopwords = load_stopwords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \"\"\"Applies a series of cleaning steps to the text.\"\"\"\n",
    "    keywords = [\"REFERENCES\", \"ACKNOWLEDGEMENTS\", \"References\", \"Acknowledgements\"]\n",
    "    \n",
    "    text = find_and_remove_references(text, keywords)\n",
    "    text = remove_text_in_brackets(text)\n",
    "    text = remove_lines_starting_with_number_or_symbol(text)\n",
    "    text = remove_lines_with_one_word(text)\n",
    "    text = remove_empty_lines(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def find_and_remove_references(text, keywords):\n",
    "    \"\"\"Finds and removes text after the first occurrence of any of the given keywords.\"\"\"\n",
    "    earliest_position = float('inf')\n",
    "    for keyword in keywords:\n",
    "        position = text.find(keyword)\n",
    "        if position != -1:\n",
    "            earliest_position = min(position, earliest_position)\n",
    "    \n",
    "    if earliest_position != float('inf'):\n",
    "        text = text[:earliest_position]\n",
    "    return text\n",
    "\n",
    "def remove_text_in_brackets(text):\n",
    "    \"\"\"Removes text enclosed in parentheses or square brackets.\"\"\"\n",
    "    pattern = r'\\([^)]*\\)|\\[[^\\]]*\\]'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def remove_lines_starting_with_number_or_symbol(text):\n",
    "    \"\"\"Removes lines starting with a number or symbol.\"\"\"\n",
    "    pattern = r'^[\\d\\W].*$'\n",
    "    return re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "\n",
    "def remove_lines_with_one_word(text):\n",
    "    \"\"\"Removes lines containing only one word.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    pattern = r'^\\s*\\w+\\s*$'\n",
    "    filtered_lines = [line for line in lines if not re.match(pattern, line)]\n",
    "    return '\\n'.join(filtered_lines)\n",
    "\n",
    "def remove_empty_lines(text):\n",
    "    \"\"\"Removes empty lines.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    non_empty_lines = [line for line in lines if line.strip() != '']\n",
    "    return '\\n'.join(non_empty_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_process_text(pdf_path, output_path):\n",
    "    \"\"\"Extracts text from a PDF file, cleans it, and saves it to a text file.\"\"\"\n",
    "    pdf_doc = fitz.open(pdf_path)\n",
    "    text = ''\n",
    "    for page in pdf_doc:\n",
    "        text += page.get_text()\n",
    "\n",
    "    cleaned_text = process_text(text)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(cleaned_text)\n",
    "    return output_path\n",
    "\n",
    "def process_pdf(pdf_filename):\n",
    "    \"\"\"Processes a single PDF file, extracts and cleans text.\"\"\"\n",
    "    text_filename = pdf_filename.with_suffix('.txt')  # Use with_suffix to change the extension\n",
    "    cleaned_text = extract_and_process_text(pdf_filename, text_filename)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summaries(summaries, output_dir, filename):\n",
    "    \"\"\"\n",
    "    Save summaries to a JSON file in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        summaries: List of summary dictionaries\n",
    "        output_dir: Path to output directory\n",
    "        filename: Name of the original file\n",
    "    \"\"\"\n",
    "    output_path = output_dir / f\"{Path(filename).stem}_summary.json\"\n",
    "    \n",
    "    # Convert torch tensors to lists for JSON serialization\n",
    "    serializable_summaries = []\n",
    "    for summary in summaries:\n",
    "        serializable_summary = {\n",
    "            \"input_tokens\": summary[\"input_tokens\"].tolist(),\n",
    "            \"summary\": summary[\"summary\"]\n",
    "        }\n",
    "        serializable_summaries.append(serializable_summary)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_summaries, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def summarize_with_model(model_path, text_filenames, device, token_batch_length, batch_stride, settings):\n",
    "    \"\"\"\n",
    "    Generate summaries using a specific model.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the model\n",
    "        text_filenames: List of text files to summarize\n",
    "        device: torch device\n",
    "        token_batch_length: Maximum token length for processing\n",
    "        batch_stride: Stride length for processing\n",
    "        settings: Dictionary of generation settings\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping filenames to summaries\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading model from {model_path}\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    model_name = Path(model_path).name\n",
    "    output_dir = SUMMARIES_DIR / model_name\n",
    "    \n",
    "    summaries = {}\n",
    "    for file_path in text_filenames:\n",
    "        logging.info(f\"Processing {file_path} with model {model_name}\")\n",
    "        \n",
    "        with open(file_path, 'r', errors='ignore') as f:\n",
    "            raw_text = f.read()\n",
    "        \n",
    "        clean_text = clean(raw_text)\n",
    "        _summaries = summarize_via_tokenbatches(\n",
    "            clean_text, model, tokenizer, device, \n",
    "            token_batch_length, batch_stride, **settings\n",
    "        )\n",
    "        \n",
    "        # Save summaries for this file\n",
    "        summary_path = save_summaries(_summaries, output_dir, file_path)\n",
    "        summaries[file_path] = summary_path\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "def summarize(ids, mask, model, tokenizer, device, **kwargs):\n",
    "    \"\"\"Generate summary using the model.\"\"\"\n",
    "    ids = ids[None, :]\n",
    "    mask = mask[None, :]\n",
    "\n",
    "    input_ids = ids.to(device)\n",
    "    attention_mask = mask.to(device)\n",
    "    global_attention_mask = torch.zeros_like(attention_mask)\n",
    "    global_attention_mask[:, 0] = 1\n",
    "\n",
    "    summary_pred_ids = model.generate(\n",
    "        ids,\n",
    "        attention_mask=mask,\n",
    "        global_attention_mask=global_attention_mask,\n",
    "        return_dict_in_generate=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    summary = tokenizer.batch_decode(\n",
    "        summary_pred_ids.sequences,\n",
    "        skip_special_tokens=True,\n",
    "        remove_invalid_values=True,\n",
    "    )\n",
    "\n",
    "    return summary\n",
    "\n",
    "def summarize_via_tokenbatches(input_text, model, tokenizer, device, token_batch_length, batch_stride, **kwargs):\n",
    "    \"\"\"Process text in batches for summarization.\"\"\"\n",
    "    encoded_input = tokenizer(\n",
    "        input_text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=token_batch_length,\n",
    "        stride=batch_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    in_id_arr = encoded_input.input_ids.to(device)\n",
    "    att_arr = encoded_input.attention_mask.to(device)\n",
    "    gen_summaries = []\n",
    "\n",
    "    with tqdm(total=len(in_id_arr)) as pbar:\n",
    "        for _id, _mask in zip(in_id_arr, att_arr):\n",
    "            result = summarize(ids=_id, mask=_mask, model=model, tokenizer=tokenizer, device=device, **kwargs)\n",
    "            _sum = {\"input_tokens\": _id, \"summary\": result}\n",
    "            gen_summaries.append(_sum)\n",
    "            pbar.update()\n",
    "\n",
    "    return gen_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_summaries(df, stopwords_path):\n",
    "    \"\"\"Process summaries and calculate n-grams using TextAnalysis module.\"\"\"\n",
    "    # Load stopwords at the start of processing\n",
    "    stopwords = load_stopwords(stopwords_path)\n",
    "    \n",
    "    results = {\n",
    "        'Summaries Top 10 Words': [],\n",
    "        'Summaries Top 10 Bigrams': [],\n",
    "        'Summaries Top 10 Trigrams': []\n",
    "    }\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        summaries = row.get('Summaries', '')\n",
    "        if pd.isna(summaries) or summaries == '':\n",
    "            empty_result = [('', 0)] * 10\n",
    "            for key in results:\n",
    "                results[key].append(empty_result)\n",
    "            continue\n",
    "            \n",
    "        results['Summaries Top 10 Words'].append(TA.topwords(summaries, stopwords))\n",
    "        results['Summaries Top 10 Bigrams'].append(TA.topbigrams(summaries, stopwords))\n",
    "        results['Summaries Top 10 Trigrams'].append(TA.toptrigrams(summaries, stopwords))\n",
    "    \n",
    "    # Add results to DataFrame\n",
    "    for key in results:\n",
    "        df[key] = results[key]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Path.replace() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 56\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 26\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m pdf_filenames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(CONTENT_DIR\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m---> 26\u001b[0m     text_filenames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_pdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf_filenames\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Process with each model\u001b[39;00m\n\u001b[0;32m     29\u001b[0m all_model_summaries \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[1;32mIn[42], line 16\u001b[0m, in \u001b[0;36mprocess_pdf\u001b[1;34m(pdf_filename)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_pdf\u001b[39m(pdf_filename):\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Processes a single PDF file, extracts and cleans text.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     text_filename \u001b[38;5;241m=\u001b[39m \u001b[43mpdf_filename\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     cleaned_text \u001b[38;5;241m=\u001b[39m extract_and_process_text(pdf_filename, text_filename)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_text\n",
      "\u001b[1;31mTypeError\u001b[0m: Path.replace() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Set parameters\n",
    "    token_batch_length = 3072\n",
    "    batch_stride = 50\n",
    "    max_len_ratio = 8\n",
    "    \n",
    "    settings = {\n",
    "        'min_length': 5,\n",
    "        'max_length': int(token_batch_length // max_len_ratio),\n",
    "        'no_repeat_ngram_size': 7,\n",
    "        'encoder_no_repeat_ngram_size': 7,\n",
    "        'repetition_penalty': 3.7,\n",
    "        'num_beams': 12,\n",
    "        'length_penalty': 0.5,\n",
    "        'early_stopping': True,\n",
    "        'do_sample': False\n",
    "    }\n",
    "    \n",
    "    # Process PDFs\n",
    "    pdf_filenames = list(CONTENT_DIR.glob('*.pdf'))\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        text_filenames = list(executor.map(process_pdf, pdf_filenames))\n",
    "    \n",
    "    # Process with each model\n",
    "    all_model_summaries = {}\n",
    "    for model_path in MODELS_TO_BENCHMARK:\n",
    "        logging.info(f\"Starting summarization with model: {model_path}\")\n",
    "        model_summaries = summarize_with_model(\n",
    "            model_path,\n",
    "            text_filenames,\n",
    "            device,\n",
    "            token_batch_length,\n",
    "            batch_stride,\n",
    "            settings\n",
    "        )\n",
    "        all_model_summaries[Path(model_path).name] = model_summaries\n",
    "    \n",
    "    # Create a summary of the processing\n",
    "    summary_report = {\n",
    "        \"models_processed\": list(all_model_summaries.keys()),\n",
    "        \"files_processed\": len(text_filenames),\n",
    "        \"timestamp\": pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(SUMMARIES_DIR / \"processing_summary.json\", 'w') as f:\n",
    "        json.dump(summary_report, f, indent=2)\n",
    "    \n",
    "    logging.info(f\"Completed processing {len(text_filenames)} files with {len(MODELS_TO_BENCHMARK)} models\")\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depracted Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import concurrent.futures\n",
    "# import fitz\n",
    "# import logging\n",
    "# from pathlib import Path\n",
    "\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from cleantext import clean\n",
    "# # from datasets import load_dataset\n",
    "# # from nltk.corpus import stopwords\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# from tqdm.auto import tqdm\n",
    "# import re\n",
    "\n",
    "# # Assuming you have a TextAnalysis module\n",
    "# import TextAnalysis as TA\n",
    "\n",
    "# # Configure logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# # --- Helper Functions ---\n",
    "\n",
    "# def load_stopwords(filepath='stopwords.txt'):\n",
    "#     \"\"\"Load stopwords from local file.\"\"\"\n",
    "#     try:\n",
    "#         with open(filepath, 'r', encoding='utf-8') as f:\n",
    "#             return {line.strip() for line in f}\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"Warning: {filepath} not found. Using empty stopwords set.\")\n",
    "#         return set()\n",
    "\n",
    "\n",
    "# def remove_text_in_brackets(text):\n",
    "#   \"\"\"Removes text enclosed in parentheses or square brackets.\"\"\"\n",
    "#   pattern = r'\\([^)]*\\)|\\[[^\\]]*\\]'\n",
    "#   return re.sub(pattern, '', text)\n",
    "\n",
    "# def remove_lines_starting_with_number_or_symbol(text):\n",
    "#   \"\"\"Removes lines starting with a number or symbol.\"\"\"\n",
    "#   pattern = r'^[\\d\\W].*$'\n",
    "#   return re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "\n",
    "# def remove_lines_with_one_word(text):\n",
    "#   \"\"\"Removes lines containing only one word.\"\"\"\n",
    "#   lines = text.split('\\n')\n",
    "#   pattern = r'^\\s*\\w+\\s*$'\n",
    "#   filtered_lines = [line for line in lines if not re.match(pattern, line)]\n",
    "#   return '\\n'.join(filtered_lines)\n",
    "\n",
    "# def remove_empty_lines(text):\n",
    "#   \"\"\"Removes empty lines.\"\"\"\n",
    "#   lines = text.split('\\n')\n",
    "#   non_empty_lines = [line for line in lines if line.strip() != '']\n",
    "#   return '\\n'.join(non_empty_lines)\n",
    "\n",
    "# def find_and_remove_references(text, keywords):\n",
    "#   \"\"\"Finds and removes text after the first occurrence of any of the given keywords.\"\"\"\n",
    "#   earliest_position = -1\n",
    "#   for keyword in keywords:\n",
    "#     position = text.find(keyword)\n",
    "#     if position != -1:\n",
    "#       earliest_position = position if earliest_position == -1 else min(position, earliest_position)\n",
    "#   if earliest_position != -1:\n",
    "#     text = text[:earliest_position]\n",
    "#   return text\n",
    "\n",
    "# def process_text(text):\n",
    "#   \"\"\"Applies a series of cleaning steps to the text.\"\"\"\n",
    "#   keywords = [\"REFERENCES\", \"ACKNOWLEDGEMENTS\", \"References\", \"Acknowledgements\"]\n",
    "\n",
    "#   text = find_and_remove_references(text, keywords)\n",
    "#   text = find_and_remove_references(text, keywords)\n",
    "#   text = remove_text_in_brackets(text)\n",
    "#   text = remove_lines_starting_with_number_or_symbol(text)\n",
    "#   text = remove_lines_with_one_word(text)\n",
    "#   text = remove_empty_lines(text)\n",
    "\n",
    "#   return text\n",
    "\n",
    "# def extract_and_process_text(pdf_path, output_path):\n",
    "#   \"\"\"Extracts text from a PDF file, cleans it, and saves it to a text file.\"\"\"\n",
    "#   pdf_doc = fitz.open(pdf_path)\n",
    "#   text = ''\n",
    "#   for page in pdf_doc:\n",
    "#     text += page.get_text()\n",
    "\n",
    "#   lines = text.split('\\n')\n",
    "#   filtered_lines = [line for line in lines if len(line.strip()) > 1]\n",
    "#   filtered_text = '\\n'.join(filtered_lines)\n",
    "#   cleaned_text = process_text(filtered_text)\n",
    "\n",
    "#   with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "#     output_file.write(cleaned_text)\n",
    "#   return output_path\n",
    "\n",
    "# def process_pdf(pdf_filename):\n",
    "#   \"\"\"Processes a single PDF file, extracts and cleans text, and returns the text filename.\"\"\"\n",
    "#   text_filename = pdf_filename.replace(\".pdf\", \".txt\")\n",
    "#   cleaned_text = extract_and_process_text(pdf_filename, text_filename)\n",
    "#   return cleaned_text\n",
    "\n",
    "\n",
    "# def summarize(ids, mask, **kwargs):\n",
    "#     ids = ids[None, :]\n",
    "#     mask = mask[None, :]\n",
    "\n",
    "#     input_ids = ids.to(_device)\n",
    "#     attention_mask = mask.to(_device)\n",
    "#     global_attention_mask = torch.zeros_like(attention_mask)\n",
    "#     global_attention_mask[:, 0] = 1\n",
    "\n",
    "#     summary_pred_ids = model.generate(\n",
    "#         ids,\n",
    "#         attention_mask=mask,\n",
    "#         global_attention_mask=global_attention_mask,\n",
    "#         return_dict_in_generate=True,\n",
    "#         **kwargs\n",
    "#     )\n",
    "#     summary = tokenizer.batch_decode(\n",
    "#         summary_pred_ids.sequences,\n",
    "#         skip_special_tokens=True,\n",
    "#         remove_invalid_values=True,\n",
    "#     )\n",
    "\n",
    "#     return summary\n",
    "\n",
    "# def summarize_via_tokenbatches(input_text, **kwargs):\n",
    "#     encoded_input = tokenizer(\n",
    "#         input_text,\n",
    "#         padding='max_length',\n",
    "#         truncation=True,\n",
    "#         max_length=token_batch_length,\n",
    "#         stride=batch_stride,\n",
    "#         return_overflowing_tokens=True,\n",
    "#         add_special_tokens=False,\n",
    "#         return_tensors='pt',\n",
    "#     )\n",
    "\n",
    "#     in_id_arr, att_arr = encoded_input.input_ids.to(_device), encoded_input.attention_mask.to(_device)\n",
    "#     gen_summaries = []\n",
    "\n",
    "#     with tqdm(total=len(in_id_arr)) as pbar:\n",
    "#         for _id, _mask in zip(in_id_arr, att_arr):\n",
    "#             result = summarize(ids=_id, mask=_mask, **kwargs)\n",
    "#             _sum = {\"input_tokens\": _id, \"summary\": result}\n",
    "#             gen_summaries.append(_sum)\n",
    "#             pbar.update()\n",
    "\n",
    "#     return gen_summaries\n",
    "\n",
    "# # --- Combine Summaries for Given Author ---\n",
    "\n",
    "# # Assuming you have a DataFrame named 'datf'\n",
    "# def combine_summaries(datf):\n",
    "#   for index, row in datf.iterrows():\n",
    "#     identifiers = row['arxiv-id'].split(',')\n",
    "#     concat_summ = \"\"\n",
    "\n",
    "#     for identifier in identifiers:\n",
    "#       if identifier.strip() == 'None':\n",
    "#         concat_summ = 'None'\n",
    "#         break\n",
    "\n",
    "#       sanitized_identifier = identifier.strip().replace('/', '_')\n",
    "#       filename = f'SUM_{sanitized_identifier}.txt'  # Assuming summaries are saved as SUM_*.txt\n",
    "\n",
    "#       if os.path.exists(filename):\n",
    "#         with open(filename, 'r', encoding='utf-8') as file:\n",
    "#           concat_summ += file.read()\n",
    "#       else:\n",
    "#         concat_summ = 'File not found'\n",
    "\n",
    "#     datf.at[index, 'Summaries'] = concat_summ\n",
    "\n",
    "# # --- N-grams ---\n",
    "\n",
    "# def sum_n_grams(df, directorypath):\n",
    "#     top10Dict = {'Summaries Top 10 Words': [],\n",
    "#                  'Summaries Top 10 Bigrams': [],\n",
    "#                  'Summaries Top 10 Trigrams': []}\n",
    "\n",
    "#     for i in df.values:\n",
    "#         summaries = i[15]  # Assuming 'Summaries' is the column name for your combined summaries\n",
    "\n",
    "#         # Now use functions from TextAnalysis\n",
    "#         top10words = TA.topwords(summaries, directorypath)\n",
    "#         top10bigrams = TA.topbigrams(summaries, directorypath)\n",
    "#         top10trigrams = TA.toptrigrams(summaries, directorypath)\n",
    "\n",
    "#         top10Dict['Summaries Top 10 Words'].append(top10words)\n",
    "#         top10Dict['Summaries Top 10 Bigrams'].append(top10bigrams)\n",
    "#         top10Dict['Summaries Top 10 Trigrams'].append(top10trigrams)\n",
    "\n",
    "#     sum_top10Df = df\n",
    "#     sum_top10Df['Summaries Top 10 Words'] = top10Dict['Summaries Top 10 Words']\n",
    "#     sum_top10Df['Summaries Top 10 Bigrams'] = top10Dict['Summaries Top 10 Bigrams']\n",
    "#     sum_top10Df['Summaries Top 10 Trigrams'] = top10Dict['Summaries Top 10 Trigrams']\n",
    "\n",
    "#     sum_top10Df = sum_top10Df[['Input Author', 'Input Institution', 'First Author', 'Bibcode', 'Title', 'Publication Date',\n",
    "#              'Keywords', 'Affiliations', 'Abstract', 'Top 10 Words', 'Top 10 Bigrams', 'Top 10 Trigrams', 'Data Type',\n",
    "#             'arxiv-id', 'Summaries', 'Summaries Top 10 Words', 'Summaries Top 10 Bigrams', 'Summaries Top 10 Trigrams']]\n",
    "\n",
    "#     return sum_top10Df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- PDF Processing ---\n",
    "\n",
    "# # Specify the directory containing your PDF files\n",
    "# content_directory = \".\\content\"\n",
    "\n",
    "# # Find all PDF files in the directory\n",
    "# pdf_filenames = [os.path.join(content_directory, file) for file in os.listdir(content_directory) if file.endswith('.pdf')]\n",
    "\n",
    "# # Process the PDF files in parallel\n",
    "# text_filenames = []\n",
    "# num_workers = 100\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "#   results = list(executor.map(process_pdf, pdf_filenames))\n",
    "\n",
    "# text_filenames.extend(result for result in results if result is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameteres for Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Summarization Model ---\n",
    "# #Set your User ID\n",
    "# username = \"ielhaime\"\n",
    "\n",
    "# hf_tag = \"pszemraj/led-large-book-summary\"\n",
    "# _device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(f\"./model\").to(_device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(f\"./model\")\n",
    "# token_batch_length = 3072\n",
    "# batch_stride = 50\n",
    "# max_len_ratio = 8\n",
    "\n",
    "# settings = {\n",
    "#     'min_length': 5,\n",
    "#     'max_length': int(token_batch_length // max_len_ratio),\n",
    "#     'no_repeat_ngram_size': 7,\n",
    "#     'encoder_no_repeat_ngram_size': 7,\n",
    "#     'repetition_penalty': 3.7,\n",
    "#     'num_beams': 12,\n",
    "#     'length_penalty': 0.5,\n",
    "#     'early_stopping': True,\n",
    "#     'do_sample': False\n",
    "# }\n",
    "\n",
    "# logging.info(f\"using textgen params:\\n\\n:{settings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Summarization ---\n",
    "\n",
    "# directory = './content'\n",
    "# text_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.txt')]\n",
    "\n",
    "# # Summarize each text file and store in a dictionary\n",
    "# summaries = {} \n",
    "# for file_path in text_files:\n",
    "#   with open(file_path, 'r', errors='ignore') as f:\n",
    "#     raw_text = f.read()\n",
    "\n",
    "#   long_text = clean(raw_text)\n",
    "#   logging.info(f\"removed {len(long_text) - len(raw_text)} chars via cleaning\")\n",
    "\n",
    "#   _summaries = summarize_via_tokenbatches(long_text, **settings)\n",
    "#   summaries[file_path] = _summaries  # Store summaries by filename\n",
    "\n",
    "#   if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Summaries and N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Main Execution ---\n",
    "\n",
    "# summaries = {}\n",
    "\n",
    "# datf = pd.read_csv('output.csv')\n",
    "\n",
    "# stop_dir = \"stopwords.txt\"\n",
    "\n",
    "# sample_text = summaries\n",
    "\n",
    "# sample_ngrams = {\n",
    "#     'top_words': TA.topwords(sample_text, stop_dir),\n",
    "#     'top_bigrams': TA.topbigrams(sample_text, stop_dir),\n",
    "#     'top_trigrams': TA.toptrigrams(sample_text, stop_dir)\n",
    "# }\n",
    "\n",
    "# # Add columns to DataFrame with the same summary and n-grams for all rows\n",
    "# datf['Summary'] = sample_text\n",
    "# datf['Top_Words'] = [sample_ngrams['top_words']] * len(datf)\n",
    "# datf['Top_Bigrams'] = [sample_ngrams['top_bigrams']] * len(datf)\n",
    "# datf['Top_Trigrams'] = [sample_ngrams['top_trigrams']] * len(datf)\n",
    "\n",
    "# # Save the updated DataFrame\n",
    "# datf.to_csv('Results.csv', index=False)\n",
    "\n",
    "\n",
    "# # # summaries = {}\n",
    "\n",
    "# # arVix Broken (Uncomment when its up again. For now, make do with the above code. Its will output the summary and the N-Grams )\n",
    "\n",
    "# # # Load your DataFrame\n",
    "# # datf = pd.read_csv('output.csv')  # Replace 'output.csv' with your actual file\n",
    "\n",
    "# # # Combine summaries\n",
    "# # combine_summaries(datf)\n",
    "\n",
    "# # # Calculate N-grams and update DataFrame\n",
    "# # path_stop = '/content/'\n",
    "# # stop_file = 'stopwords.txt'\n",
    "# # stop_dir = path_stop + stop_file\n",
    "# # final_df = sum_n_grams(datf, stop_dir)\n",
    "\n",
    "# # # Save the updated DataFrame\n",
    "# # final_df.to_csv('Results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
